[{"categories":["foundationdb","distributed-systems"],"content":"Learn how to tune FoundationDB for Redwood storage Engine","date":"2024-04-22","objectID":"/posts/redwood-memory-tuning/","tags":null,"title":"Redwood‚Äôs memory tuning in FoundationDB","uri":"/posts/redwood-memory-tuning/"},{"categories":["foundationdb","distributed-systems"],"content":"While FoundationDB allows you to obtain sub-milliseconds transactions‚Äôs latency without any knob-tuning, we had to bump a bit memory usage for Redwood under certain usage and workload. The following configuration has been tested on clusters from 7.1 to 7.3. ","date":"2024-04-22","objectID":"/posts/redwood-memory-tuning/:0:0","tags":null,"title":"Redwood‚Äôs memory tuning in FoundationDB","uri":"/posts/redwood-memory-tuning/"},{"categories":["foundationdb","distributed-systems"],"content":"BTree page cache We discovered the issue when we saw a performance decrease on our cluster storing time-series data. Our cluster was reporting some high disk-business, causing outages: 10.0.3.23:4501 ( 65% cpu; 61% machine; 0.010 Gbps; 93% disk IO; 7.5 GB / 7.4 GB RAM ) 10.0.3.24:4501 ( 61% cpu; 61% machine; 0.010 Gbps; 87% disk IO; 9.7 GB / 7.4 GB RAM ) 10.0.3.25:4501 ( 69% cpu; 61% machine; 0.010 Gbps; 93% disk IO; 5.4 GB / 7.4 GB RAM ) This was our first ¬´we need to dig into this¬ª moment with FDB. We couldn‚Äôt find the root-cause and we asked the community. Turns out we had a classic page-cache issue which was spotted by Markus Pilman and William Dowling. While the trace files are pretty verbose, they are containing a lot of information like this one: \"PagerCacheHit\": \"39852\", \"PagerCacheMiss\": \"25903\", Yep, that‚Äôs a 40% cache-miss ratio over 5s üò± This is why the disk was so busy, spending his time moving pages back and forth. We need to bump the memory, but how much? The general recommandation that worked for us is to target around 1-2% of the kvstore_used_bytes metrics. As we have around 1TiB of data per StorageServer, we can add the following config key: cache_memory = 10GiB Which fixed our cache-miss issue üéâ \"PagerCacheHit\": \"51968\", \"PagerCacheMiss\": \"432\", ","date":"2024-04-22","objectID":"/posts/redwood-memory-tuning/:1:0","tags":null,"title":"Redwood‚Äôs memory tuning in FoundationDB","uri":"/posts/redwood-memory-tuning/"},{"categories":["foundationdb","distributed-systems"],"content":"Byte Sample memory usage But our problems are still unresolved, as we are still seeing some OOM üò≠ Because this cluster is storing time-series data, each StorageServers is holding around 1TiB of data. As we were holding more and more data, we saw more and more OOM errors on our fdbmonitor logs. Something was growing linearly with our usage and needed tuning. This time, we had help from Steve Atherton which pointed us towards the direction of the Byte Sample: There is a data structure that storage servers have called the Byte Sample which stores a deterministic random sample of keys. This data is persisted on disk in the storage engine and is loaded immediately upon storage server startup. Unfortunately, its size is not tracked or reported, but grows linearly with KV size and I suspect yours is somewhere around 4GB-6GB based on the memory usage I‚Äôve seen for smaller storage KV sizes. So, we need to add around 4GB more in the memory, but there is no config for that parameter. It needs to be embedded in the global memory parameter. Let‚Äôs compute the right value! ","date":"2024-04-22","objectID":"/posts/redwood-memory-tuning/:2:0","tags":null,"title":"Redwood‚Äôs memory tuning in FoundationDB","uri":"/posts/redwood-memory-tuning/"},{"categories":["foundationdb","distributed-systems"],"content":"The global memory formula By testing things on our clusters, we ended up with this formula: # Default is 2 cache_memory = (1-2% of kvstore_used_bytes)GiB # Default is 8 memory = (8 + cache_memory + 4-6GB per TB of kvstore_used_bytes)GiB Which fixed all our memory issues with FoundationDB üéâ And to be fair, this is the only things we needed to tune on our clusters, which is quite impressive üëÄ ","date":"2024-04-22","objectID":"/posts/redwood-memory-tuning/:3:0","tags":null,"title":"Redwood‚Äôs memory tuning in FoundationDB","uri":"/posts/redwood-memory-tuning/"},{"categories":["foundationdb","distributed-systems"],"content":"Special thanks I would like to thank Markus, William and Steve from the FoundationDB community for their help ü§ù Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2024-04-22","objectID":"/posts/redwood-memory-tuning/:4:0","tags":null,"title":"Redwood‚Äôs memory tuning in FoundationDB","uri":"/posts/redwood-memory-tuning/"},{"categories":["foundationdb","distributed-systems"],"content":"Learn how to avoid FDB's biggest caveats by using a new feature called automatic idempotency in FoundationDB","date":"2024-03-12","objectID":"/posts/automatic-txn-fdb-730/","tags":null,"title":"True idempotent transactions with FoundationDB 7.3","uri":"/posts/automatic-txn-fdb-730/"},{"categories":["foundationdb","distributed-systems"],"content":"I have been working around FoundationDB for several years now, and the new upcoming version is fixing one of the most evil and painful caveats you can deal with when writing layers: commit_unknown_result. ","date":"2024-03-12","objectID":"/posts/automatic-txn-fdb-730/:0:0","tags":null,"title":"True idempotent transactions with FoundationDB 7.3","uri":"/posts/automatic-txn-fdb-730/"},{"categories":["foundationdb","distributed-systems"],"content":"Transactions with unknown results When you start writing code with FDB, you may be under the assertions that given the database‚Äôs robustness, you will not experience some strange behavior under certain failure scenarios. Turns out, there is one scenario that is possible to reach, and quickly explained in the official documentation: As with other client/server databases, in some failure scenarios a client may be unable to determine whether a transaction succeeded. In these cases, commit() will raise a commit_unknown_result exception. The on_error() function treats this exception as retriable, so retry loops that don‚Äôt check for commit_unknown_result could execute the transaction twice. In these cases, you must consider the idempotency of the transaction. While having idempotent retry loops is possible, sometimes it is not possible, for example when using atomic operations to keep track of statistics. Is this problem worth fixing? Seems a really edgy case ü§î It truly depends whether it is acceptable for your transaction to be committed twice. For most of the case, it is not, but sometimes developers are not aware of this behavior, leading to errors. This is one of the reasons why we worked and open-sourced a way to embed rust-code within FoundationDB‚Äôs simulation framework. Using the simulation crate, your layer can be tested like FDB, and I can assure you: you will see those transactions in simulation üôà. This behavior has given headache to my colleagues, as we tried to bypass correctness and validation code in simulation when transactions state are unknown, and who could blame us? Validate the correctness of your code is hard when certains transactions (for example, one that could clean everything) are ‚Äúmaybe committed‚Äù. Fortunately, the community has released a workaround for this: automatic idempotency. ","date":"2024-03-12","objectID":"/posts/automatic-txn-fdb-730/:1:0","tags":null,"title":"True idempotent transactions with FoundationDB 7.3","uri":"/posts/automatic-txn-fdb-730/"},{"categories":["foundationdb","distributed-systems"],"content":"Automatic idempotency The documentation is fairly explicit: Use the automatic_idempotency transaction option to prevent commits from failing with commit_unknown_result at a small performance cost. The option appeared in FoundationDB 7.3, and could solve our issue. I decided to give it a try and modify the foundationdb-simulation crate example. The example is trying to use a atomic increment under simulation. Before 7.1, during validation, we had to write some code that looks like this: // We don't know how much maybe_committed transactions has succeeded, // so we are checking the possible range if self.success_count \u003c= count \u0026\u0026 count \u003c= self.expected_count + self.maybe_committed_count { // ... As I was adding 7.3 support in the crate, I decided to update the example and try the new option: // Enable idempotent txn trx.set_option(TransactionOption::AutomaticIdempotency)?; If the behavior is correct, I can simplify my consistency checks: if self.success_count == count { self.context.trace( Severity::Info, \"Atomic count match\", details![], ); } // ... I‚Äôve been running hundreds of seeds on my machine and everything works great: no more maybe-committed transactions! Now that 7.3 support is merged in the rust bindings, we will be able to expands our testing thanks to our simulation farm. I‚Äôm also looking to see the performance impact of the feature, even if I‚Äôm pretty sure that it will outperform any layer-work. This is truly a very useful feature and I hope this option will be turned on by default on the next major release. The initial PR can be found here. Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2024-03-12","objectID":"/posts/automatic-txn-fdb-730/:2:0","tags":null,"title":"True idempotent transactions with FoundationDB 7.3","uri":"/posts/automatic-txn-fdb-730/"},{"categories":["learning","distributed-systems"],"content":"Are you not using academic papers in your favorite RSS app? You should!","date":"2024-01-22","objectID":"/posts/academic-conferences/","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":" I really like using RSS feeds. My Feedly account has more than 190 feeds, all neatly organized by categories. They help me keep up with new ideas and interesting blog posts about engineering. But there‚Äôs another source of information I‚Äôve been using for a long time that not many people know about: academic papers. You can discover details about infrastructure that you might not find in regular blog posts. Academic papers, unlike typical blog content, often dive deeper into specific aspects of infrastructure. They provide more in-depth information, uncovering details that are not commonly discussed. So, if you‚Äôre interested in gaining a more comprehensive understanding of infrastructure-related topics, exploring academic papers can be really worthwile. Sounds a bit too academic, doesn‚Äôt it? ü§î I don‚Äôt think so! It‚Äôs true that academic research can sometimes seem distant from everyday industry needs, but following both academic and industry tracks is beneficial. R\u0026D from academia often lead to new ideas and technologies that eventually find their way into practical use. Moreover, numerous academic conferences feature a ‚Äúindustry track‚Äù that is essential to monitor. Aren‚Äôt they too complex to read? ü§î If you don‚Äôt get everything right away, that‚Äôs okay. Reading these smart papers might be a bit hard, but it‚Äôs a skill that gets better with practice. And who knows, maybe you‚Äôll be inspired to write your own paper someday! üòâ I‚Äôm intrigued! Where should I start? Here‚Äôs a short list of my go-to academic papers and conferences that you can follow for infrastructure engineering. Please note that many conferences exists on other subjects, like security and so. ","date":"2024-01-22","objectID":"/posts/academic-conferences/:0:0","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"The USENIX community ","date":"2024-01-22","objectID":"/posts/academic-conferences/:1:0","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"OSDI As part of the USENIX Association, the Operating Systems Design and Implementation is an annual computer science conference that you shouldn‚Äôt miss. You can catch most of the sessions online along with some useful slides. ","date":"2024-01-22","objectID":"/posts/academic-conferences/:1:1","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"Usenix ATC Similar to OSDI, the Usenix Annual Technical Conference is another classic to follow. has been awarded Best Paper in 2023, and it‚Äôs a gem! ","date":"2024-01-22","objectID":"/posts/academic-conferences/:1:2","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"The ACM family ","date":"2024-01-22","objectID":"/posts/academic-conferences/:2:0","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"SIGMOD SIGMOD, or the Special Interest Group on Management of Data, is an essential conference under the ACM umbrella, focusing on the management and organization of data. ","date":"2024-01-22","objectID":"/posts/academic-conferences/:2:1","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"SoCC The Symposium on Cloud Computing or SoCC for short belongs to ACM. It has a bit less content, as videos are not published, but you should keep it in your watchlist. ","date":"2024-01-22","objectID":"/posts/academic-conferences/:2:2","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"SOSP The Symposium on Operating Systems Principles is another noteworthy conference in the ACM family. It‚Äôs a top-tier venue for discussing operating systems research. Stay tuned for updates on the latest breakthroughs and innovative ideas. ","date":"2024-01-22","objectID":"/posts/academic-conferences/:2:3","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"Others ","date":"2024-01-22","objectID":"/posts/academic-conferences/:3:0","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"VLDB Not belonging to USENIX or ACM, the Very Large Data Bases (VLDB) conference is a key event in the database community. It provides a platform for researchers and professionals to exchange ideas on managing and analyzing large-scale datasets. ","date":"2024-01-22","objectID":"/posts/academic-conferences/:3:1","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"CIDR The Conference on Innovative Data Systems Research (CIDR) is a systems-oriented conference, complementary in its mission to the mainstream database conferences like SIGMOD and VLDB, emphasizing the systems architecture perspective. ","date":"2024-01-22","objectID":"/posts/academic-conferences/:3:2","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":["learning","distributed-systems"],"content":"Cool papers examples This is a nice list, but how about some paper examples that you like?ü§î Sure! Here‚Äôs a quick list with some infrastructure-related informations: On-demand Container Loading in AWS Lambda Scalable OLTP in the Cloud: What‚Äôs the BIG DEAL? Kora: A Cloud-Native Event Streaming Platform For Kafka Using Lightweight Formal Methods to Validate a Key-Value Storage Node in Amazon S3 FoundationDB: A Distributed, Unbundled, Transactional Key Value Store Virtual consensus with Delos I‚Äôm also trying to organize them into my Zotero‚Äôs library. Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2024-01-22","objectID":"/posts/academic-conferences/:4:0","tags":null,"title":"The unseen treasures of Infrastructure Engineering: Academic Papers","uri":"/posts/academic-conferences/"},{"categories":null,"content":"Tester la fiabilit√© d‚Äôun logiciel est toujours une chose assez difficile. Malgr√© tous nos efforts, nous n‚Äôarrivons pas √† √©crire des programmes sans bugs. La raison est assez simple: l‚Äô√™tre humain est √©tonnamment mauvais pour pouvoir imaginer toutes les erreurs possibles qu‚Äôun programme peut avoir. Ce constat est encore + vrai quand l‚Äôon travaille dans les entrailles des bases de donn√©es, o√π la moindre erreur peut g√©n√©rer de la corruption de donn√©es clientes. Existe-t-il de l‚Äôoutillage permettant de palier √† ce probl√®me ? Une des solutions consiste √† venir tout contr√¥ler de fa√ßon d√©terministe: du temps que va prendre l‚ÄôI/O, au scheduling des threads, en passant par quelle erreur a √©t√© d√©clench√©. C‚Äôest ce qu‚Äôon appelle le Deterministic Simulation Testing. C‚Äôest la technique que nous avons choisi afin de pouvoir valider l‚Äôimpl√©mentation de nos propres bases de donn√©es serverless. Durant ce talk, vous d√©couvrirez les enjeux et les impacts de la simulation dans le cycle de d√©veloppement d‚Äôun logiciel fortement distribu√©. Vous apprendrez √† utiliser notre simulateur open-source. Vous d√©couvrirez √©galement comment Clever Cloud utilise la simulation pour venir acc√©l√©rer la R\u0026D des futures produits data de l‚Äôentreprise. ","date":"2023-06-01","objectID":"/talks/simulation-bdd/:0:0","tags":null,"title":"D√©velopper des bases de donn√©es fiables gr√¢ce √† la simulation","uri":"/talks/simulation-bdd/"},{"categories":null,"content":"Resources Slides ","date":"2023-06-01","objectID":"/talks/simulation-bdd/:1:0","tags":null,"title":"D√©velopper des bases de donn√©es fiables gr√¢ce √† la simulation","uri":"/talks/simulation-bdd/"},{"categories":null,"content":"Occurences Paris Open Source Data Infrastructure Meetup - June 2023 ","date":"2023-06-01","objectID":"/talks/simulation-bdd/:2:0","tags":null,"title":"D√©velopper des bases de donn√©es fiables gr√¢ce √† la simulation","uri":"/talks/simulation-bdd/"},{"categories":null,"content":"Savez-vous quel est le point commun entre Apple, Snowflake, VMWare et Datadog ? Ces entreprises partagent le m√™me ingr√©dient : FoundationDB FoundationDB est une base de donn√©es distribu√©e, open-source, souvent cach√©e dans l‚Äôinfrastructure bas-niveau permettant de g√©rer des transactions distribu√©es ACID. Durant ce talk, nous vous proposons une d√©couverte de cette base de donn√©es si peu connue. Vous d√©couvrirez l‚Äôhistoire du projet, ses garanties transactionnelles aussi fortes que Spanner, ainsi que sa robustesse op√©rationnelle. Nous ferons aussi un retour d‚Äôexp√©rience sur le d√©veloppement d‚Äôapplications par-dessus FoundationDB. ","date":"2023-04-10","objectID":"/talks/fdb-best-secret/:0:0","tags":null,"title":"FoundationDB : le secret le mieux gard√© des nouvelles architectures distribu√©es !","uri":"/talks/fdb-best-secret/"},{"categories":null,"content":"Resources Slides ","date":"2023-04-10","objectID":"/talks/fdb-best-secret/:1:0","tags":null,"title":"FoundationDB : le secret le mieux gard√© des nouvelles architectures distribu√©es !","uri":"/talks/fdb-best-secret/"},{"categories":null,"content":"Occurences Devoxx France 2023 ","date":"2023-04-10","objectID":"/talks/fdb-best-secret/:2:0","tags":null,"title":"FoundationDB : le secret le mieux gard√© des nouvelles architectures distribu√©es !","uri":"/talks/fdb-best-secret/"},{"categories":null,"content":"Clever Cloud est une entreprise europ√©enne bas√©e en France, experte depuis plus de 10 ans dans la cr√©ation de solutions d‚Äôh√©bergement en mode PaaS (Platform-as-a-Service) performantes et faciles √† utiliser, avec un grand souci de souverainet√© et s√©curit√© des donn√©es. Dans cette optique, nous op√©rons et industrialisons des milliers de bases de donn√©es pour nos clients. Cette volum√©trie nous oblige √† venir imaginer et concevoir les bases de donn√©es de demain, qui devront √™tre √† la fois r√©silientes et efficientes. Une des pistes d‚Äôarchitectures est de venir s√©parer la partie calcul du stockage des donn√©es. Durant ce talk, vous d√©couvrirez les diff√©rentes architectures possibles et existantes. Nous parlerons √©galement des avantages et inconv√©nients de telles architectures. ","date":"2023-02-01","objectID":"/talks/split-compute-storage/:0:0","tags":null,"title":"D√©coupler le stockage du calcul, une fausse bonne id√©e ?","uri":"/talks/split-compute-storage/"},{"categories":null,"content":"Resources Slides ","date":"2023-02-01","objectID":"/talks/split-compute-storage/:1:0","tags":null,"title":"D√©coupler le stockage du calcul, une fausse bonne id√©e ?","uri":"/talks/split-compute-storage/"},{"categories":null,"content":"Occurences VeryTechTrip 2023 ","date":"2023-02-01","objectID":"/talks/split-compute-storage/:2:0","tags":null,"title":"D√©coupler le stockage du calcul, une fausse bonne id√©e ?","uri":"/talks/split-compute-storage/"},{"categories":null,"content":"Tester la fiabilit√© d‚Äôun logiciel est toujours une chose assez difficile. Malgr√© tous nos efforts, nous n‚Äôarrivons pas √† √©crire des programmes sans bugs. La raison est assez simple : l‚Äô√™tre humain est √©tonnamment mauvais pour pouvoir imaginer toutes les erreurs possibles qu‚Äôun programme peut avoir. Existe-t-il un moyen de g√©n√©rer toutes les erreurs possibles afin d‚Äôaider le d√©veloppeur ? Une des solutions consiste √† venir tout contr√¥ler de fa√ßon d√©terministe, du temps que va prendre l‚ÄôI/O, au scheduling des threads, en passant par quelle erreur a √©t√© d√©clench√©e. C‚Äôest ce qu‚Äôon appelle le Deterministic Simulation Testing. Durant ce talk, vous d√©couvrirez les enjeux et les impacts de la simulation dans le cycle de d√©veloppement d‚Äôun logiciel fortement distribu√©. Vous d√©couvrirez √©galement comment Clever Cloud utilise la simulation pour venir acc√©l√©rer la R\u0026D de nouveaux produits. ","date":"2023-02-01","objectID":"/talks/simulation-vtt/:0:0","tags":null,"title":"D√©velopper des logiciels fiables gr√¢ce √† la simulation","uri":"/talks/simulation-vtt/"},{"categories":null,"content":"Resources Slides Replay ","date":"2023-02-01","objectID":"/talks/simulation-vtt/:1:0","tags":null,"title":"D√©velopper des logiciels fiables gr√¢ce √† la simulation","uri":"/talks/simulation-vtt/"},{"categories":null,"content":"Occurences VeryTechTrip 2023 ","date":"2023-02-01","objectID":"/talks/simulation-vtt/:2:0","tags":null,"title":"D√©velopper des logiciels fiables gr√¢ce √† la simulation","uri":"/talks/simulation-vtt/"},{"categories":null,"content":"Abstract Nous avons l‚Äôhabitude en tant que d√©veloppeur de traiter les bases de donn√©es comme des services capables de ranger, organiser et stocker durablement nos informations. On oublie assez facilement que ce sont des logiciels √† part enti√®re. Depuis plus de 50 ans, industriels et chercheurs d√©veloppent de nouvelles architectures et techniques d‚Äôing√©nierie pour les rendre plus scalables et plus performantes. Je vous propose d‚Äôouvrir le capot et de d√©couvrir comment fonctionne une base de donn√©es. Dans cette universit√© de 3 heures, vous allez pouvoir d√©couvrir dans un premier temps comment fonctionne une base de donn√©es: de l‚Äôorganisation des fichiers aux indexes, en passant par le traitement de requ√™tes et la gestion de transactions. Puis nous aborderons les bases de donn√©es distribu√©es, avec leurs lots d‚Äôenjeux comme la r√©plication, le consensus et la coordination. ","date":"2022-04-18","objectID":"/talks/db-101/:1:0","tags":null,"title":"√Ä la d√©couverte des bases de donn√©es","uri":"/talks/db-101/"},{"categories":null,"content":"Resources Slides Apache Calcite demo ","date":"2022-04-18","objectID":"/talks/db-101/:2:0","tags":null,"title":"√Ä la d√©couverte des bases de donn√©es","uri":"/talks/db-101/"},{"categories":null,"content":"Occurences Devoxx France 2022 ","date":"2022-04-18","objectID":"/talks/db-101/:3:0","tags":null,"title":"√Ä la d√©couverte des bases de donn√©es","uri":"/talks/db-101/"},{"categories":null,"content":"Assez peu de d√©veloppeurs ont l‚Äôenvie ou l‚Äôopportunit√© de faire de l‚Äôastreinte. Pourtant, avoir la possibilit√© de voir son propre logiciel (ou celui de coll√®gues) partir en vrille sans comprendre pourquoi est une v√©ritable opportunit√©. En effet, c‚Äôest une chance de comprendre les v√©ritables enjeux derri√®re l‚Äôanalyse des performances et le besoin de d√©bugger en production. Pour pouvoir comprendre ce qui se passe, nous avons besoin de faits tangibles, de donn√©es. C‚Äôest ce qu‚Äôon appelle aujourd‚Äôhui l‚Äôobservabilit√©. Durant ce talk, vous d√©couvrirez comment faciliter le debug et l‚Äôanalyse de la performance de vos applications √† travers l‚Äôinstrumentation du code. Nous parlerons des trois piliers de l‚Äôobservabilit√©, de leurs forces et de leurs faiblesses. Vous d√©couvrirez o√π placer vos points d‚Äôobservations, les visualisations prioritaires et les diff√©rents outils que l‚Äôon peut utiliser. ","date":"2022-04-18","objectID":"/talks/observability-from-on-call/:0:0","tags":null,"title":"D√©velopper des applications observables pour la production ","uri":"/talks/observability-from-on-call/"},{"categories":null,"content":"Resources Slides ","date":"2022-04-18","objectID":"/talks/observability-from-on-call/:1:0","tags":null,"title":"D√©velopper des applications observables pour la production ","uri":"/talks/observability-from-on-call/"},{"categories":null,"content":"Occurences Devoxx France 2022 ","date":"2022-04-18","objectID":"/talks/observability-from-on-call/:2:0","tags":null,"title":"D√©velopper des applications observables pour la production ","uri":"/talks/observability-from-on-call/"},{"categories":["podcast","maci"],"content":"Dans ce formidable √©pisode, il est question d‚Äôautomation et de t√©l√©travail, de Rust, d‚Äôexp√©rimenbtation de Fetch dans Node.js, de HTML Dialog Element, du nouveau chip Intel, de grpcurl‚Ä¶ et comme toujours on finira en chanson ! üëã Laissez un commentaire ou venez discuter avec nous sur @clever_cloudFR pour nous dire ce que vous avez pens√© de ce nouvel √©pisode. ","date":"2022-03-03","objectID":"/podcasts/maci-64/:0:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 64: Des sir√®nes, des crabes coreutils mais pas d'enclave de blu ray ?","uri":"/podcasts/maci-64/"},{"categories":["podcast","maci"],"content":"Dans cet incroyable √©pisode, il est question : de la CNIL et de Google Analytics ; de la faille #log4shell ; d‚Äôauthentification, de la version 100 de Chrome et Firefox ; d‚Äôun nouveau container pour les requ√®tes ; d‚Äôun incident poilu sur PostgreSQL ; de migration de m√©tadata ; d‚Äô√©x√©cution de code arbitraire et d‚Äôextension de cockpit JSON‚Ä¶ pour finir, comme il se doit en musique. ","date":"2022-02-21","objectID":"/podcasts/maci-63/:0:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 63: L'oAuth v100 met en demeure la foundation des fusions de RFC","uri":"/podcasts/maci-63/"},{"categories":["podcast","maci"],"content":"nous parlons impl√©mentation d‚ÄôApache Aiflow sur Clever Cloud, d‚Äôint√©gration de Rust dans GCC, d‚Äôacronymes p√©nibles, d‚ÄôIPtable VIS et surtout et dans le d√©tails des kernels 5.14 et 5.15 de Linux. ","date":"2022-01-19","objectID":"/podcasts/maci-62/:0:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 62: btrfs est le seul filesystem avec des vrais cristaux de sel √† l'int√©rieur","uri":"/podcasts/maci-62/"},{"categories":["learning","distributed-systems"],"content":"Learning distributed systems is tough. You need to go through a lot of academic papers, concepts, code review, before being able to have a global pictures. Thankfully, there is a lot of resources out there that can help you. Here's the best resources I used to learn distributed systems","date":"2022-01-17","objectID":"/posts/distsys-resources/","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Learning distributed systems is tough. You need to go through a lot of academic papers, concepts, code review, before being able to have a global pictures. Thankfully, there is a lot of resources out there that can help you to get started. Here‚Äôs a list of resources I used to learn distributed systems. I will keep this blogpost up-to-date with books, conferences, and so on. A distributed system is one in which the failure of a computer you didn‚Äôt even know existed can render your own computer unusable. -Lamport, 1987 ","date":"2022-01-17","objectID":"/posts/distsys-resources/:0:0","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Updates ‚ö°Ô∏è 2022-01-31: Aphyr distsys classnotes Dan Creswell‚Äôs reading List The Internals of PostgreSQL Distributed systems Understanding Distributed Systems Patterns of Distributed Systems ","date":"2022-01-17","objectID":"/posts/distsys-resources/:1:0","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Reading üìö ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:0","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Designing Data-Intensive Applications Let‚Äôs start by one of my favorite book, Designing Data-Intensive Applications, written by Martin Kleppmann. This is by far the most practical book you will ever find about distributed systems. It covers: Data models, query languages and encoding, Replication, partitioning, the associated troubles, consistency, consensus, batch and stream processing. NoSQL‚Ä¶ Big Data‚Ä¶ Scalability‚Ä¶ CAP Theorem‚Ä¶ Eventual Consistency‚Ä¶ Sharding‚Ä¶ Nice buzzwords, but how does the stuff actually work? As software engineers, we need to build applications that are reliable, scalable and maintainable in the long run. We need to understand the range of available tools and their trade-offs. For that, we have to dig deeper than buzzwords. This book will help you navigate the diverse and fast-changing landscape of technologies for storing and processing data. We compare a broad variety of tools and approaches, so that you can see the strengths and weaknesses of each, and decide what‚Äôs best for your application. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:1","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Database Internals Database Internals, written by Alex Petrov, is a fantastic book for anyone wondering how a database works. I recommend reading it after Designing Data-Intensive Applications, as the author dives in more details compared to Martin‚Äôs book. Have you ever wanted to learn more about Databases but did not know where to start? This is a book just for you. We can treat databases and other infrastructure components as black boxes, but it doesn‚Äôt have to be that way. Sometimes we have to take a closer look at what‚Äôs going on because of performance issues. Sometimes databases misbehave, and we need to find out what exactly is going on. Some of us want to work in infrastructure and develop databases. This book‚Äôs main intention is to introduce you to the cornerstone concepts and help you understand how databases work. The book consists of two parts: Storage Engines and Distributed Systems since that‚Äôs where most of the differences between the vast majority of databases is coming from. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:2","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Distributed Systems Maarten van Steen wrote a book called Distributed Systems 3rd edition. It is a nice book which you can get a digital copy of this book for free. Distributed systems are like 3D brain teasers: easy to disassemble; hard to put together. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:3","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Understanding Distributed Systems If you are not a backend engineer but still curious about distributed systems, I highly recommend Understanding Distributed Systems. Roberto Vitillo is doing an insane job to vulgarize the subject. Want to learn how to build scalable and fault-tolerant cloud applications? This book will teach you the core principles of distributed systems so that you don‚Äôt have to spend countless hours trying to understand how everything fits together. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:4","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"The Internals of PostgreSQL PostgreSQL is getting a lot of love and traction these years, and Hironobu Suzuki wrote a terrific book the about the The Internals of PostgreSQL. PostgreSQL is a well-designed open-source multi-purpose relational database system which is widely used throughout the world. It is one huge system with the integrated subsystems, each of which has a particular complex feature and works with each other cooperatively. Although understanding of the internal mechanism is crucial for both administration and integration using PostgreSQL, its hugeness and complexity prevent it. The main purposes of this document are to explain how each subsystem works, and to provide the whole picture of PostgreSQL. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:5","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Jepsen blog We are often using databases as a source of truth, but they are also pieces of software with bugs in it. Kyle Kingsbury is the most famous database-breaker with Jepsen: Jepsen is an effort to improve the safety of distributed databases, queues, consensus systems, etc. We maintain an open source software library for systems testing, as well as blog posts and conference talks exploring particular systems‚Äô failure modes. In each analysis we explore whether the system lives up to its documentation‚Äôs claims, file new bugs, and suggest recommendations for operators. You will find analysis on many databases, such as CockroachDB, etcd, Kafka, MongoDB, and so on. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:6","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Aphyr distsys class notes Following Jepsen, here‚Äôs a great bonus: Kyle is also teaching distributed systems, and his notes are available. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:7","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Distributed systems for fun and profit Despite being free, Distributed systems for fun and profit is an awesome book. The author, Mikito Takada has done a terrific work to vulgarize distributed systems. I wanted a text that would bring together the ideas behind many of the more recent distributed systems - systems such as Amazon‚Äôs Dynamo, Google‚Äôs BigTable and MapReduce, Apache‚Äôs Hadoop and so on. In this text I‚Äôve tried to provide a more accessible introduction to distributed systems. To me, that means two things: introducing the key concepts that you will need in order to have a good time reading more serious texts, and providing a narrative that covers things in enough detail that you get a gist of what‚Äôs going on without getting stuck on details. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:8","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Translucent Databases I really like the pitch of the book: Do you have personal information in your database? Do you keep files on your customers, your employees, or anyone else? Do you need to worry about European laws restricting the information you keep? Do you keep copies of credit card numbers, social security numbers, or other information that might be useful to identity thieves or insurance fraudsters? Do you deal with medical records or personal secrets? Most database administrators have some of these worries. Some have all of them. That‚Äôs why database security is so important. This new book, Translucent Databases, describes a different attitude toward protecting the information. Translucent Databases is a short book, focus on how to store sensitive data. You will find several dozen examples of interesting case studies on how to efficiently and privately store sensitive data. A must-have. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:9","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"The Art of PostgreSQL The Art of PostgreSQL is all about showing the power of both SQL and PostgreSQL. It explains the how‚Äôs and why‚Äôs of using Postgres‚Äôs many feature, and how you, as a developers, can take advantages of it. A brilliant book that should be read by every developer. This book is for developers, covering advanced SQL techniques for data processing. Learn how to get exactly the result set you need in your application‚Äôs code! Learn advanced SQL with practical examples and datasets that help you get the most of the book! Every query solves a practical use case and is given in context. The book covers (de-)normalisation with simple practical examples to dive into this seemingly complex topic, including Caching and Indexing Strategy. Writing efficient SQL is easier than it looks, and begins with database modeling and writing clear code. The book teaches you how to write fast queries! ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:10","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Readings in Database Systems Another free book, Readings in Database Systems is a great read if you are looking for an opinionated and short review on subject like architecture, engines, analytics and so on. Readings in Database Systems (commonly known as the ‚ÄúRed Book‚Äù) has offered readers an opinionated take on both classic and cutting-edge research in the field of data management since 1988. Here, we present the Fifth Edition of the Red Book ‚Äî the first in over ten years. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:2:11","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Watching üì∫ ","date":"2022-01-17","objectID":"/posts/distsys-resources/:3:0","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"CMU Database Group The Database Group at Carnegie Mellon University have been publishing a lot of contents, including: Intro to Database Systems lecture Advanced Database Systems lecture which are the best lectures about database in my opinion. I also recommend their Quarantine database talks playlists: the ‚ÄúQuarantine Database Tech Talks‚Äù is a on-line seminar series at Carnegie Mellon University with leading developers and researchers of database systems. Each speaker will present the implementation details of their respective systems and examples of the technical challenges that they faced when working with real-world customers. Vaccination Database Tech Talks First Dose Vaccination Database Tech Talks Second Dose ","date":"2022-01-17","objectID":"/posts/distsys-resources/:3:1","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Distributed Systems lecture series Martin Kleppmann(Designing Data Intensive applications‚Äôs author) published an 8-lecture series on distributed systems: This video is part of an 8-lecture series on distributed systems, given as part of the undergraduate computer science course at the University of Cambridge. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:3:2","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Academic conferences Keeping track of the academic world is not easy, but thankfully, we can keep track of several academic conferences which are data-related, including: CIDR SIGMOD/PODS VLDB PaPoC ","date":"2022-01-17","objectID":"/posts/distsys-resources/:3:3","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Industrial conference There is not much database-focused conferences, but you will be interested to see talks from: HydraConf HYTRADBOI ","date":"2022-01-17","objectID":"/posts/distsys-resources/:3:4","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"DistSys Reading Group sessions If you are looking for explanations about a distributed systems paper, you may be interested in the DistSys Reading Group: Every week we present and discuss one distributed systems paper. We try to focus on relatively new papers, although we occasionally break this rule for some important older publications. The main objective of this group is to share knowledge through the discussion. Our participants come from academia and industry and often carry a unique perspective and expertise on the subject matter. Every session can be found on their YouTube channel. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:3:5","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Coding üßë‚Äçüíª ","date":"2022-01-17","objectID":"/posts/distsys-resources/:4:0","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Maelstrom Ever wonder to develop your own toy distributed systems? Fear no more, you can use Maelstrom for that! Maelstrom is a workbench for learning distributed systems by writing your own. It uses the Jepsen testing library to test toy implementations of distributed systems. Maelstrom provides standardized tests for things like ‚Äúa commutative set‚Äù or ‚Äúa transactional key-value store‚Äù, and lets you learn by writing implementations which those test suites can exercise. It‚Äôs used as a part of a distributed systems workshop by Jepsen. Maelstrom provides a range of tests for different kinds of distributed systems, built on top of a simple JSON protocol via STDIN and STDOUT. Users write servers in any language. Maelstrom runs those servers, sends them requests, routes messages via a simulated network, and checks that clients observe expected behavior. You want to write Plumtree in Bash? Byzantine Paxos in Intercal? Maelstrom is for you. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:4:1","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"PingCAP‚Äôs Talent Plan PingCAP is the company behind the tidb/tikv stack, a new distributed systems. They developed their own open source training program: Talent Plan is an open source training program initiated by PingCAP. It aims to create or combine some open source learning materials for people interested in open source, distributed systems, Rust, Golang, and other infrastructure knowledge. As such, it provides a series of courses focused on open source collaboration, rust programming, distributed database and systems. I went through the Raft project in Rust and I learned a lot! ","date":"2022-01-17","objectID":"/posts/distsys-resources/:4:2","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Patterns of Distributed Systems Unmesh Joshi is writing an on-going serie called Patterns of Distributed Systems: Distributed systems provide a particular challenge to program. They often require us to have multiple copies of data, which need to keep synchronized. Yet we cannot rely on processing nodes working reliably, and network delays can easily lead to inconsistencies. Despite this, many organizations rely on a range of core distributed software handling data storage, messaging, system management, and compute capability. These systems face common problems which they solve with similar solutions. This article recognizes and develops these solutions as patterns, with which we can build up an understanding of how to better understand, communicate and teach distributed system design. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:4:3","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Reading lists üëÄ ","date":"2022-01-17","objectID":"/posts/distsys-resources/:5:0","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["learning","distributed-systems"],"content":"Dan Creswell‚Äôs reading List If you want more contents, Dan Creswell has a nice Distributed Systems Reading List üöÄ Thank you for reading my post! Feel free to react to this article, you can find me on Twitter. ","date":"2022-01-17","objectID":"/posts/distsys-resources/:5:1","tags":null,"title":"Best resources to learn about data and distributed systems","uri":"/posts/distsys-resources/"},{"categories":["podcast","maci"],"content":"Dans ce fabuleux √©pisode r√©alis√© en famille avec Quentin Adam, Marc Antoine Perennou, Julien Durillon et Pierre Zemb nous parlons de class action sur une entente Google / Apple sur les moteur de recherche, des processeurs Mobile eye by Intel et des annonces AMD et Intel pour de nouveaux procs, de GNUPG qui devient √©conomiquement viable, du bug Exchange de l‚Äôan 2022. On vous pr√©sente les nouvelles recrues de Clever Cloud et enfin, on vous parle Kernel ! ","date":"2022-01-12","objectID":"/podcasts/maci-61/:0:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 61: overclock le soc, on est en retard sur le kernel","uri":"/podcasts/maci-61/"},{"categories":null,"content":"Abstract This is a one-day course I am giving at my former engineering school ISEN Brest. ","date":"2021-12-13","objectID":"/talks/cloud-computing-101/:1:0","tags":null,"title":"Cloud computing 101","uri":"/talks/cloud-computing-101/"},{"categories":null,"content":"Schedule Cloud Computing IaaS/PaaS/Kubernetes demo Microservices architecture Kafka practice ","date":"2021-12-13","objectID":"/talks/cloud-computing-101/:2:0","tags":null,"title":"Cloud computing 101","uri":"/talks/cloud-computing-101/"},{"categories":null,"content":"Ressources Slides ","date":"2021-12-13","objectID":"/talks/cloud-computing-101/:3:0","tags":null,"title":"Cloud computing 101","uri":"/talks/cloud-computing-101/"},{"categories":["podcast","maci"],"content":"Dans cet √©pisode de r√©f√©rence, bien que difficile √† num√©roter, nous recevons Mathieu Ancelin et nous parlons : de la lev√©e de fonds de PlanetScale, de la guerre entre Databricks et Snowflakes, des 20 ans de HAProxy, des ressources query dans SQL, des meilleurs performances de nos vieux claviers PS/2, d‚Äôun outil Apple Open Source pour l‚Äôanalyse de logs de Garbage Collection, avant de finir en musique‚Ä¶ indice : c‚Äôest pas du Mozart. ","date":"2021-12-01","objectID":"/podcasts/maci-60/:0:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 60: Databricks et Snowflake aboient, haproxy passe‚ÄØet graphe les ramassages des miettes","uri":"/podcasts/maci-60/"},{"categories":["podcast","maci"],"content":"Dans ce fabuleux √©pisode, nous recevons le non moins fabuleux Mathieu Ancelin pour parler : Open Source autour de la MAIF, de Scaleway qui quitte le projet Ga√ØaX, de Nu.age la nouvelle offre de cloud par La Poste, de l‚Äôapparition du verbe ‚Äúquery‚Äù en HTTP, de l‚Äô√©pique mise √† jour par la IETF des RFC par une RFC, d‚ÄôAsync cancellation en RUST, d‚ÄôInnernet un outil pour configurer ses VPN, avant de finir en musique (attention le son est addictif !). ","date":"2021-12-01","objectID":"/podcasts/maci-59/:0:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 59: Scaleway fait une rfc pour cancel la future de gaiax","uri":"/podcasts/maci-59/"},{"categories":null,"content":" Abstract Apache Pulsar est un syst√®me de messagerie pub-sub distribu√© et open source. Il offre de nombreux avantages par rapport √† Kafka, tels que le multi-tenant, la g√©o-r√©plication, le stockage d√©coupl√© ou encore le SQL et FaaS directement int√©gr√©es. La seule chose qui manque pour une large adoption est le support du standard de-facto pour le streaming: Kafka. Et c‚Äôest ainsi que notre histoire commence. Dans ce talk, nous vous raconterons notre parcours pour construire Kafka On Pulsar. Pour construire notre plateforme de topic manag√©, nous avions besoin de ce support. On s‚Äôest d‚Äôabord lanc√© dans l‚Äô√©criture d‚Äôun proxy en Rust capable de transformer le protocole Kafka vers celui de Pulsar √† la vol√©e. Mais lorsque nous avons appris que l‚Äô√©quipe en charge de Pulsar travaillait sur le m√™me sujet, nous avons d√©cid√© de les rejoindre ü§ù A la fin de ce talk, vous saurez plus de choses sur le fonctionnement interne de Kafka et de Pulsar. Vous aurez √©galement un retour d‚Äôexp√©rience sur l‚Äô√©criture d‚Äôun proxy maison de streaming Rust. Mais surtout sur comment passer d‚Äôun d√©veloppement interne √† travailler avec les mainteneurs d‚Äôun projet open-source et int√©grer la communaut√©. Occurences Devoxx 2021 Ressources ","date":"2021-09-24","objectID":"/talks/story-kop/:0:0","tags":null,"title":" Il √©tait une fois Kafka sur Pulsar ","uri":"/talks/story-kop/"},{"categories":null,"content":"Slides The slides are also available here ","date":"2021-09-24","objectID":"/talks/story-kop/:1:0","tags":null,"title":" Il √©tait une fois Kafka sur Pulsar ","uri":"/talks/story-kop/"},{"categories":["podcast","maci"],"content":"Dans ce cinquante-cinqui√®me √©pisode nous parlons de Grafana, patent trolls, MDN, AVIF dans Safari, paradoxe de Simpson, Cosmos, DDoS, Cloudflare, Google, Linux, Razer et PAM ","date":"2021-08-26","objectID":"/podcasts/maci-54/:0:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 54 : Une Attaque De Souris Cosmique Dans MDN Met AVIF Les Nerfs De Google","uri":"/podcasts/maci-54/"},{"categories":["sql","maci"],"content":"Dans ce quarante-septi√®me √©pisode nous parlons de Google, Amazon, QUIC, The Art of PostgreSQL, PolarDB, PG Auto Failover, VLDB, ToroDB, Citus, gh-ost. ","date":"2021-06-04","objectID":"/podcasts/maci-46/:0:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 46 : Faire Le Tri Dans Ses Relations","uri":"/podcasts/maci-46/"},{"categories":["sql","maci"],"content":"Notes Toutes les notes sont disponibles sur https://www.clever-cloud.com/fr/podcast/episode46 Avec par ordre d‚Äôapparition : @urcadox @PierreZ @zepag @tapoueh 00:00:00 - Introduction 00:01:54 - Google et les infos de localisation https://www.businessinsider.fr/us/unredacted-google-lawsuit-docs-detail-efforts-to-collect-user-location-2021-5 00:06:57 - Amazon attaqu√© par le gouvernement US pour pratiques d√©loyales envers les vendeurs de sa marketplace https://www.businessinsider.in/tech/news/amazon-just-got-sued-by-the-us-government-for-the-first-time-alleging-the-worlds-largest-online-marketplace-is-engaging-in-unfair-practices-with-third-party-sellers/articleshow/82975643.cms 00:14:10 - Article sur QUIC apr√®s publication de la RFC 9000 (et des RFC li√©es 8999, 9001,9002). https://www.bortzmeyer.org/quic.html 00:20:52 - The Art of PostgreSQL https://theartofpostgresql.com/ Code de r√©duction : MESSAGE30 00:45:20 - PolarDB https://github.com/alibaba/PolarDB-for-PostgreSQL Manga qui explique PG : https://nostarch.com/mg_databases.htm 00:52:20 - PG Auto Failover https://github.com/citusdata/pg_auto_failover 01:01:20 - Un SSD qui cause SQL http://vldb.org/pvldb/vol14/p1481-lee.pdf http://vldb.org/pvldb/vol14-volume-info/ 01:11:34 - ToroDB https://www.torodb.com/ 01:13:43 - Citus https://www.citusdata.com/blog/2021/03/05/citus-10-release-open-source-rebalancer-and-columnar-for-postgres/ 01:22:12 - Un tool de migration online de schema MySQL https://github.com/github/gh-ost Recommandation musicale de Dimitri : https://www.youtube.com/watch?v=YQGqs3uZkoY Code de r√©duction pour The Art of PostgreSQL : MESSAGE30 ","date":"2021-06-04","objectID":"/podcasts/maci-46/:1:0","tags":null,"title":"üá´üá∑ Message √† caract√®re informatique 46 : Faire Le Tri Dans Ses Relations","uri":"/podcasts/maci-46/"},{"categories":null,"content":"Abstract OVHcloud is the biggest European cloud provider. From dedicated servers to Managed Kubernetes, from VMware¬Æ based Hosted Private Cloud to OpenStack-based Public Cloud, we have over 1.4 million customers worldwide. Because of our Kubinception design(using Kubernetes to run Kubernetes), we are putting hundreds of customers in an ETCD cluster. This design is great to easily spawn control-planes for customers, but it is also putting a lot of pressure on ETCD. To keep it healthy while growing up constantly, we had to learn many things about how ETCD works under the hood and how we can operate it efficiently. In this talk, you will have the insights of how we are operating our ETCD clusters. We will tell you our journey to use ETCD, from our observability to deployments and management, what did work and what did not. Occurences KubeCon Europe 2021 Ressources ","date":"2021-05-02","objectID":"/talks/lessons-learned-from-operating-etcd/:0:0","tags":null,"title":"Lessons Learned from Operating ETCD","uri":"/talks/lessons-learned-from-operating-etcd/"},{"categories":null,"content":"Slides The slides are also available here ","date":"2021-05-02","objectID":"/talks/lessons-learned-from-operating-etcd/:1:0","tags":null,"title":"Lessons Learned from Operating ETCD","uri":"/talks/lessons-learned-from-operating-etcd/"},{"categories":null,"content":"Videos Photos and tweets TOMORROW: For anyone attending #KubeCon EU Virtual, check out @PierreZ's talk about lessons learned with #etcd while he was at @OVHcloud | May 5 at 14:15 CEST https://t.co/BcPnAOx5rm ‚Äî etcd (@etcdio) May 4, 2021 ","date":"2021-05-02","objectID":"/talks/lessons-learned-from-operating-etcd/:2:0","tags":null,"title":"Lessons Learned from Operating ETCD","uri":"/talks/lessons-learned-from-operating-etcd/"},{"categories":["FoundationDB"],"content":"An overview of all the helpers to generate row keys with FoundationDB","date":"2021-02-21","objectID":"/posts/crafting-keys-in-fdb/","tags":null,"title":"Crafting row keys in FoundationDB","uri":"/posts/crafting-keys-in-fdb/"},{"categories":["FoundationDB"],"content":" As I‚Äôm working on my latest contribution around FoundationDB and Rust, I had the chance to dig a bit into how FoundationDB‚Äôs bindings are offering helpers to generate keys. Their approach is interesting enough to deserve a blogpost üòé ","date":"2021-02-21","objectID":"/posts/crafting-keys-in-fdb/:0:0","tags":null,"title":"Crafting row keys in FoundationDB","uri":"/posts/crafting-keys-in-fdb/"},{"categories":["FoundationDB"],"content":"Row key? When you are using a key/value store, the design of the row key is extremely important, as this will define how well: your scans will be optimized, your puts will be spread, you will avoid hot-spotting a shard/region. If you need more information on row keys, I recommend going through these links before moving on: ‚ÄúDesigning your schema‚Äù BigTable documentation ‚ÄúRowkey Design‚Äù HBase documentation ","date":"2021-02-21","objectID":"/posts/crafting-keys-in-fdb/:1:0","tags":null,"title":"Crafting row keys in FoundationDB","uri":"/posts/crafting-keys-in-fdb/"},{"categories":["FoundationDB"],"content":"Hand-crafting row keys Most of the time, you will need to craft the row key ‚Äúby hand‚Äù, like this for an HBase‚Äôs app: // Prefix + classId + labelsId + timestamp // 128 bits byte[] rowkey = new byte[Constants.HBASE_RAW_DATA_KEY_PREFIX.length + 8 + 8 + 8]; System.arraycopy(Constants.HBASE_RAW_DATA_KEY_PREFIX, 0, rowkey, 0, Constants.HBASE_RAW_DATA_KEY_PREFIX.length); // Copy classId/labelsId System.arraycopy(Longs.toByteArray(msg.getClassId()), 0, rowkey, Constants.HBASE_RAW_DATA_KEY_PREFIX.length, 8); System.arraycopy(Longs.toByteArray(msg.getLabelsId()), 0, rowkey, Constants.HBASE_RAW_DATA_KEY_PREFIX.length + 8, 8); Or maybe you will wrap things in a function like this in Go: // EncodeRowKey encodes the table id and record handle into a kv.Key func EncodeRowKey(tableID int64, encodedHandle []byte) kv.Key { buf := make([]byte, 0, prefixLen+len(encodedHandle)) buf = appendTableRecordPrefix(buf, tableID) buf = append(buf, encodedHandle...) return buf } Each time, you need to wrap the complexity of converting your objects to a row-key, by creating a buffer and write stuff in it. In our Java example, there is an interesting comment: // Prefix + classId + labelsId + timestamp If we are replacing some characters, we are not really far from: // (Prefix, classId, labelsId, timestamp) Which looks like a Tuple(a collection of values of different types) and this is what FoundationDB is using as an abstraction to create keys üòç ","date":"2021-02-21","objectID":"/posts/crafting-keys-in-fdb/:2:0","tags":null,"title":"Crafting row keys in FoundationDB","uri":"/posts/crafting-keys-in-fdb/"},{"categories":["FoundationDB"],"content":"FDB‚Äôs abstractions and helpers ","date":"2021-02-21","objectID":"/posts/crafting-keys-in-fdb/:3:0","tags":null,"title":"Crafting row keys in FoundationDB","uri":"/posts/crafting-keys-in-fdb/"},{"categories":["FoundationDB"],"content":"Tuple Instead of crafting bytes by hand, we are packing a Tuple: // create a Tuple\u003cString, i64\u003e with (\"tenant-42\", 1) let tuple = (String::from(\"tenant-42\"), 1); // and compute a row-key from the Tuple let row_key = foundationdb::tuple::pack::\u003c(String, i64)\u003e(\u0026tuple); The generated row-key will be readable from any bindings, as it‚Äôs construction is standardized. Let‚Äôs print it: // and print-it in hexa println!(\"{:#04X?}\", row_key); // can be verified with https://www.utf8-chartable.de/unicode-utf8-table.pl [ 0x02, 0x74, // t 0x65, // e 0x6E, // n 0x61, // a 0x6E, // n 0x74, // t 0x2D, // - 0x31, // 1 0x00, 0x15, 0x2A, // 42 ] As you can see, pack added some extra-characters. There are used to recognized the next type, a bit like when you are encoding/decoding some wire protocols. You can find the relevant documentation here. Having this kind of standard means that we can easily decompose/unpack it: // retrieve the user and the magic number In a Tuple (String, i64) let from_row_key = foundationdb::tuple::unpack::\u003c(String, i64)\u003e(\u0026row_key)?; println!(\"user='{}', magic_number={}\", from_row_key.0, from_row_key.1); // user='tenant-42', magic_number=42 Now that we saw Tuples, let‚Äôs dig in the next abstraction: subspaces ","date":"2021-02-21","objectID":"/posts/crafting-keys-in-fdb/:3:1","tags":null,"title":"Crafting row keys in FoundationDB","uri":"/posts/crafting-keys-in-fdb/"},{"categories":["FoundationDB"],"content":"Subspace When you are working with key-values store, we are often playing with what we call keyspaces, by dedicating a portion of the key to an usage, like this for example: /users/tenant-1/... /users/tenant-2/... /users/tenant-3/... Here, /users/tenant-1/ can be view like a prefix where we will put all the relevant keys. Instead of passing a simple prefix, FoundationDB is offering a dedicated structure called a Subspace: A Subspace represents a well-defined region of keyspace in a FoundationDB database It provides a convenient way to use FoundationDB tuples to define namespaces for different categories of data. The namespace is specified by a prefix tuple which is prepended to all tuples packed by the subspace. When unpacking a key with the subspace, the prefix tuple will be removed from the result. As you can see, the Subspace is heavily relying on FoundationDB‚Äôs tuples, as we can pack and unpack it. As a best practice, API clients should use at least one subspace for application data. Well, as we have now the tools to handle keyspaces easily, it is now futile to craft keys by hand üôÉ Let‚Äôs create a subspace! // create a subspace from the Tuple (\"tenant-1\", 42) let subspace = Subspace::from((String::from(\"tenant-1\"), 42)); // let's print the range println!(\"start: {:#04X?}\\n end: {:#04X?}\", subspace.range().0, subspace.range().1); We can see observe this: // can be verified with https://www.utf8-chartable.de/unicode-utf8-table.pl start: [ 0x02, 0x74, // t 0x65, // e 0x6E, // n 0x61, // a 0x6E, // n 0x74, // t 0x2D, // - 0x31, // 1 0x00, 0x15, 0x2A, // 42 0x00, 0x00, // smallest possible byte ] end: [ 0x02, 0x74, // t 0x65, // e 0x6E, // n 0x61, // a 0x6E, // n 0x74, // t 0x2D, // - 0x31, // 1 0x00, 0x15, 0x2A, // 42 0x00, 0xFF, // biggest possible byte ] Which make sens, if we take (\"tenant-1\", 42) as a prefix, then the range for this subspace will be between (\"tenant-1\", 42, 0x00) and (\"tenant-1\", 42, 0xFF) ","date":"2021-02-21","objectID":"/posts/crafting-keys-in-fdb/:3:2","tags":null,"title":"Crafting row keys in FoundationDB","uri":"/posts/crafting-keys-in-fdb/"},{"categories":["FoundationDB"],"content":"Directory Now that we know our way around Tuples and Subspaces, we can now talk about what I‚Äôm working on, which is the Directory. Let‚Äôs have a look at the relevant documentation: FoundationDB provides directories (available in each language binding) as a tool for managing related subspaces. Directories are a recommended approach for administering applications. Each application should create or open at least one directory to manage its subspaces. Okay, let‚Äôs see the API(in Go, as I‚Äôm working on the Rust API): subspace, err := directory.CreateOrOpen(db, []string{\"application\", \"my-app\", \"tenant\", \"tenant-42\"}, nil) if err != nil { log.Fatal(err) } fmt.Printf(\"%+v\\n\", subspace.Bytes()) // [21 18] We can see that we have a shorter subspace! The directory allows you to generate some integer that will be bind to a path, like here \"application\", \"my-app\", \"tenant\", \"tenant-42\". There are two advantages to this: shorter keys, cheap metadata operations like List or Move: // list all tenant in \"application\", \"my-app\": tenants, err := directory.List(db, []string{\"application\", \"my-app\", \"tenant\"}) if err != nil { log.Fatal(err) } fmt.Printf(\"%+v\\n\", tenants) // [tenant-42] // renaming 'tenant-42' in 'tenant-142' // This will NOT move the data, only the metadata is modified directorySubspace, err = directory.Move(db, []string{\"application\", \"my-app\", \"tenant\", \"tenant-42\"}, // old path []string{\"application\", \"my-app\", \"tenant\", \"tenant-142\"}) // new path if err != nil { log.Fatal(err) } fmt.Printf(\"%+v\\n\", directorySubspace.Bytes()) // still [21 18] The returned object is actually a DirectorySubspace, which implements both Directory and Subspace, which means that you can use it to recreate many directories and subspaces at will üëå If you are wondering about how this integer is generated, I recommend going through this awesome blogpost on how high contention allocator works in FoundationDB. Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2021-02-21","objectID":"/posts/crafting-keys-in-fdb/:3:3","tags":null,"title":"Crafting row keys in FoundationDB","uri":"/posts/crafting-keys-in-fdb/"},{"categories":null,"content":"Occurences CESI DevDay Epitech Insight Ressources ","date":"2021-02-19","objectID":"/talks/to-the-cloud-and-beyond/:0:0","tags":null,"title":"To the ‚òÅÔ∏è and beyond üöÄ","uri":"/talks/to-the-cloud-and-beyond/"},{"categories":null,"content":"Slides The slides are also available here ","date":"2021-02-19","objectID":"/talks/to-the-cloud-and-beyond/:1:0","tags":null,"title":"To the ‚òÅÔ∏è and beyond üöÄ","uri":"/talks/to-the-cloud-and-beyond/"},{"categories":null,"content":"Videos ","date":"2021-02-19","objectID":"/talks/to-the-cloud-and-beyond/:2:0","tags":null,"title":"To the ‚òÅÔ∏è and beyond üöÄ","uri":"/talks/to-the-cloud-and-beyond/"},{"categories":["etcd","raft","electro-monkeys"],"content":"Episode You can listen to the episode here. ","date":"2021-01-20","objectID":"/podcasts/em-62/:1:0","tags":null,"title":"üá´üá∑ Electro Monkeys 62: ETCD avec Pierre Zemb","uri":"/podcasts/em-62/"},{"categories":["etcd","raft","electro-monkeys"],"content":"Sum-up etcd est une base de donn√©es bien connue de toutes les √©quipes op√©rationnelles, puisqu‚Äôelle est au coeur de Kubernetes. Cependant, mis √† part sa documentation en ligne, c‚Äôest une base de donn√©es sur laquelle il n‚Äôexiste aucune litt√©rature. Et c‚Äôest une chose que j‚Äôai peine √† comprendre pour un projet de cette importance qui a un tel impact sur notre quotidien. Par chance, nous ne sommes pas les seuls √† devoir op√©rer etcd. Qui plus est, certains cloud providers l‚Äôutilisent √† bien plus grande √©chelle que nous, ce qui est notamment le cas pour OVHcloud. Leur connaissance du mod√®le op√©rationnel d‚Äôetcd et leurs retours d‚Äôexp√©rience nous sont d‚Äôune aide pr√©cieuse. Dans cet √©pisode, je re√ßois Pierre Zemb. Pierre est leader technique des syst√®mes et du stockage distribu√©s chez OVHcloud, donc la personne id√©ale pour parler d‚Äôetcd. Dans cet √©change, nous √©voquons les bases de donn√©es de nouvelle g√©n√©ration et leur fonctionnement, les raisons pour lesquelles etcd a √©t√© choisie pour Kubernetes, mais aussi ses limites et ses alternatives. ","date":"2021-01-20","objectID":"/podcasts/em-62/:2:0","tags":null,"title":"üá´üá∑ Electro Monkeys 62: ETCD avec Pierre Zemb","uri":"/podcasts/em-62/"},{"categories":["etcd","raft","electro-monkeys"],"content":"Notes Notes about ETCD ","date":"2021-01-20","objectID":"/podcasts/em-62/:3:0","tags":null,"title":"üá´üá∑ Electro Monkeys 62: ETCD avec Pierre Zemb","uri":"/podcasts/em-62/"},{"categories":["etcd","notesabout"],"content":"List of ressources gleaned about ETCD","date":"2021-01-11","objectID":"/posts/notes-about-etcd/","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":" Notes About is a blogpost serie you will find a lot of links, videos, quotes, podcasts to click on about a specific topic. Today we will discover ETCD. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:0:0","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Overview of ETCD As stated in the official documentation: etcd is a strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines. It gracefully handles leader elections during network partitions and can tolerate machine failure, even in the leader node. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:1:0","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"History ETCD was initially developed by CoreOS: CoreOS built etcd to solve the problem of shared configuration and service discovery. July 23, 2013 - announcement December 27, 2013 - etcd 0.2.0 - new API, new modules and tons of improvements February 07, 2014 - etcd 0.3.0 - Improved Cluster Discovery, API Enhancements and Windows Support January 28, 2015 - etcd 2.0 - First Major Stable Release June 30, 2016 - etcd3 - A New Version of etcd from CoreOS June 09, 2017 - etcd 3.2 - etcd 3.2 now with massive watch scaling and easy locks February 01, 2018 - etcd 3.3 - Announcing etcd 3.3, with improvements to stability, performance, and more August 30, 2019 - etcd 3.4 - Better Storage Backend, concurrent Read, Improved Raft Voting Process, Raft Learner Member ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:2:0","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Overall architecture The etcd key-value store is a distributed system intended for use as a coordination primitive. Like Zookeeper and Consul, etcd stores a small volume of infrequently-updated state (by default, up to 8 GB) in a key-value map, and offers strict-serializable reads, writes and micro-transactions across the entire datastore, plus coordination primitives like locks, watches, and leader election. Many distributed systems, such as Kubernetes and OpenStack, use etcd to store cluster metadata, to coordinate consistent views over data, to choose leaders, and so on. ETCD is: using the raft consensus algorithm, a single group raft, using gRPC for communication, using a self-made WAL implementation, storing key-values into bbolt, optimized for consistency over latency in normal situations and consistency over availability in the case of a partition (in terms of the PACELC theorem). ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:0","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Consensus? Raft? Raft is a consensus algorithm for managing a replicated log. consensus involves multiple servers agreeing on values. two common consensus algorithm are Paxos and Raft , Paxos is quite difficult to understand, inspite of numerous attempts to make it more approachable.Furthermore, its architecture requires complex changes to support practical systems. As a result, both system builders and students struggle with Paxos. A common alternative to Paxos/Raft is a non-consensus (aka peer-to-peer) replication protocol. Raft separates the key elements of consensus, such asleader election, log replication, and safety ETCD contains several raft optimizations: Read Index, Follower reads, Transfer leader, Learner role, Client-side load-balancing. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:1","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Exposed API ETCD is exposing several APIs through different gRPC services: Put(key, value), Delete(key, Optional(keyRangeEnd)), Get(key, Optional(keyRangeEnd)), Watch(key, Optional(keyRangeEnd)), Transaction(if/then/else ops), Compact(revision), Lease: Grant, Revoke, KeepAlive Key and values are bytes-oriented but ordered. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:2","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Transactions // From google paxosdb paper: // Our implementation hinges around a powerful primitive which we call MultiOp. All other database // operations except for iteration are implemented as a single call to MultiOp. A MultiOp is applied atomically // and consists of three components: // 1. A list of tests called guard. Each test in guard checks a single entry in the database. It may check // for the absence or presence of a value, or compare with a given value. Two different tests in the guard // may apply to the same or different entries in the database. All tests in the guard are applied and // MultiOp returns the results. If all tests are true, MultiOp executes t op (see item 2 below), otherwise // it executes f op (see item 3 below). // 2. A list of database operations called t op. Each operation in the list is either an insert, delete, or // lookup operation, and applies to a single database entry. Two different operations in the list may apply // to the same or different entries in the database. These operations are executed // if guard evaluates to // true. // 3. A list of database operations called f op. Like t op, but executed if guard evaluates to false. message TxnRequest { // compare is a list of predicates representing a conjunction of terms. // If the comparisons succeed, then the success requests will be processed in order, // and the response will contain their respective responses in order. // If the comparisons fail, then the failure requests will be processed in order, // and the response will contain their respective responses in order. repeated Compare compare = 1; // success is a list of requests which will be applied when compare evaluates to true. repeated RequestOp success = 2; // failure is a list of requests which will be applied when compare evaluates to false. repeated RequestOp failure = 3; } ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:3","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Versioned data Each Key/Value has a revision. When creating a new key, revision starts at 1, and then will be incremented each time the key is updated. In order to avoid having a growing keySpace, one can issue the Compact gRPC service: Compacting the keyspace history drops all information about keys superseded prior to a given keyspace revision ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:4","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Lease // this message represent a Lease message Lease { // TTL is the advisory time-to-live in seconds. Expired lease will return -1. int64 TTL = 1; // ID is the requested ID for the lease. If ID is set to 0, the lessor chooses an ID. int64 ID = 2; int64 insert_timestamp = 3; } ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:5","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Watches message Watch { // key is the key to register for watching. bytes key = 1; // range_end is the end of the range [key, range_end) to watch. If range_end is not given, // only the key argument is watched. If range_end is equal to '\\0', all keys greater than // or equal to the key argument are watched. // If the range_end is one bit larger than the given key, // then all keys with the prefix (the given key) will be watched. bytes range_end = 2; // If watch_id is provided and non-zero, it will be assigned to this watcher. // Since creating a watcher in etcd is not a synchronous operation, // this can be used ensure that ordering is correct when creating multiple // watchers on the same stream. Creating a watcher with an ID already in // use on the stream will cause an error to be returned. int64 watch_id = 7; } ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:6","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Linearizable reads Section 8 of the raft paper explains the issue: Read-only operations can be handled without writing anything into the log. However, with no additional measures, this would run the risk of returning stale data, since the leader responding to the request might have been superseded by a newer leader of which it is unaware. Linearizable reads must not return stale data, and Raft needs two extra precautions to guarantee this without using the log. First, a leader must have the latest information on which entries are committed. The Leader Completeness Property guarantees that a leader has all committed entries, but at the start of its term, it may not know which those are. To find out, it needs to commit an entry from its term. Raft handles this by having each leader commit a blank no-op entry into the log at the start of its term. Second,a leader must check whether it has been deposed before processing a read-only request (its information may be stale if a more recent leader has been elected). Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests. ETCD implements ReadIndex read(more info on Diving into ETCD‚Äôs linearizable reads). ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:7","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"How ETCD is using bbolt bbolt is the underlying kv used in etcd. A bucket called key is used to store data, and the key is the revision. Then, to find keys, a B-Tree is used. Bolt allows only one read-write transaction at a time but allows as many read-only transactions as you want at a time. Each transaction has a consistent view of the data as it existed when the transaction started. Bolt uses a B+tree internally and only a single file. Both approaches have trade-offs. If you require a high random write throughput (\u003e10,000 w/sec) or you need to use spinning disks then LevelDB could be a good choice. If your application is read-heavy or does a lot of range scans then Bolt could be a good choice. Try to avoid long running read transactions. Bolt uses copy-on-write so old pages cannot be reclaimed while an old transaction is using them. Bolt uses a memory-mapped file so the underlying operating system handles the caching of the data. Typically, the OS will cache as much of the file as it can in memory and will release memory as needed to other processes. This means that Bolt can show very high memory usage when working with large databases. Etcd implements multi-version-concurrency-control (MVCC) on top of Boltdb From an Github issue: Note that the underlying bbolt mmap its file in memory. For better performance, usually it is a good idea to ensure the physical memory available to etcd is larger than its data size. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:3:8","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"ETCD in K8S We built Kubernetes upon Etcd due to its similarities to Chubby and to the Omega store. When we exposed Etcd's watch (https://t.co/7rdNCKHjbO) through the K8s API, we let more Etcd details bleed through than originally intended. We need to clean up some of those details soon ‚Äî Brian Grant (@bgrant0607) April 16, 2019 The interface can be found here. Create use TTL and Txn Get use KV.Get Delete use Get and then for with a Txn GuaranteedUpdate uses Txn List uses Get Watch uses Watch with a channel ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:4:0","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Jepsen The Jepsen team tested etcd-3.4.3, here‚Äôs some quotes: In our tests, etcd 3.4.3 lived up to its claims for key-value operations: we observed nothing but strict-serializable consistency for reads, writes, and even multi-key transactions, during process pauses, crashes, clock skew, network partitions, and membership changes. Watches appear correct, at least over single keys. So long as compaction does not destroy historical data while a watch isn‚Äôt running, watches appear to deliver every update to a key in order. However, etcd locks (like all distributed locks) do not provide mutual exclusion. Multiple processes can hold an etcd lock concurrently, even in healthy clusters with perfectly synchronized clocks. If you use etcd locks, consider whether those locks are used to ensure safety, or simply to improve performance by probabilistically limiting concurrency. It‚Äôs fine to use etcd locks for performance, but using them for safety might be risky. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:5:0","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Operation notes ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:6:0","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Deployements tips From the official documentation: Since etcd writes data to disk, SSD is highly recommended. To prevent performance degradation or unintentionally overloading the key-value store, etcd enforces a configurable storage size quota set to 2GB by default. To avoid swapping or running out of memory, the machine should have at least as much RAM to cover the quota. 8GB is a suggested maximum size for normal environments and etcd warns at startup if the configured value exceeds it. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:6:1","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Defrag After compacting the keyspace, the backend database may exhibit internal fragmentation. Defragmentation is issued on a per-member so that cluster-wide latency spikes may be avoided. Defrag is basically dumping the bbolt tree on disk and reopening it. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:6:2","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Snapshot An ETCD snapshot is related to Raft‚Äôs snapshot: Snapshotting is the simplest approach to compaction. In snapshotting, the entire current system state is written to a snapshot on stable storage, then the entire log up to that point is discarded Snapshot can be saved using etcdctl: etcdctl snapshot save backup.db ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:6:3","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"Lease Be careful on Leader‚Äôs change and lease, this can create some issues: The new leader extends timeouts automatically for all leases. This mechanism ensures no lease expires due to server side unavailability. ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:6:4","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":["etcd","notesabout"],"content":"War stories An analysis of the Cloudflare API availability incident on 2020-11-02 How a production outage in Grafana Cloud‚Äôs Hosted Prometheus service was caused by a bad etcd client setup Random performance issue on etcd 3.4 Impact of etcd deployment on Kubernetes, Istio, and application performance ","date":"2021-01-11","objectID":"/posts/notes-about-etcd/:6:5","tags":null,"title":"Notes about ETCD","uri":"/posts/notes-about-etcd/"},{"categories":null,"content":"Abstract The FoundationDB Record Layer is an open source library that provides a record-oriented datastore with semantic similar to a relational database, implemented on top of FoundationDB. It is highly used within Apple by CloudKit, Apple‚Äôs cloud backend service to host billions of independent databases, many with a common schema. In this talk, we will dig into the paper and the library‚Äôs concepts and feature. You will also discover how to use the library from a developer‚Äôs point-of-view, with examples taken from ETCD-Layer and Record-Store. Occurences FDB Paris Meetup #2 Ressources ","date":"2020-10-16","objectID":"/talks/record-layer-101/:0:0","tags":null,"title":"Record-Layer 101","uri":"/talks/record-layer-101/"},{"categories":null,"content":"Slides The slides are also available here ","date":"2020-10-16","objectID":"/talks/record-layer-101/:1:0","tags":null,"title":"Record-Layer 101","uri":"/talks/record-layer-101/"},{"categories":["personal"],"content":"A retrospective of the last 10 years","date":"2020-09-30","objectID":"/posts/ten-years-programming/","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["personal"],"content":"I‚Äôve just realized that I‚Äôve spent the last decade programming ü§Ø While 2020 feels like a strange year, I thought it would be nice to write down a retrospective of the last 10 years üóì ","date":"2020-09-30","objectID":"/posts/ten-years-programming/:0:0","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["personal"],"content":"Learning to program üë®üèª‚Äçüíª I wrote my first Hello, world program somewhere around September 2010, when I started my engineering school to do some electronics, but that C language got me. I spent 6 months struggling to understand pointers and memory. I remember spending nights trying to find a memory leak with valgrind. Of course there were multiples mistakes, but it felt good to dig that far. I also discovered Linux around that time, and spent many nights playing with Linux commands. I started my journey to Linux with Centos and then Ubuntu 11.04. I think this started the loop I‚Äôm (still!) stuck in: for {tryNewDistro()} I‚Äôm pretty sure that if I wanted to go away from distributed systems, I would try to land a job around operating systems. So many things to learn ü§© After learning C, we started to learn web-based technologies like HTML/CSS/JS/PHP. I remember struggling to generate a calendar with PHP üêò I learned about APIs the week after the project üòÖ I remember digging into cookies, and network calls from popular websites to see how they were using it. ","date":"2020-09-30","objectID":"/posts/ten-years-programming/:1:0","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["personal"],"content":"Java and Hadoop üêò I had the chance to land a part-time internship during the third year (out of five) of my engineering school. I joined the Systems team @ Arkea, a french bank. I remember spending a lot of time with my coworkers, learning things from them, from Hadoop to mainframes and Linux. It was my first time grasping the work around ‚Äúsystem programming‚Äù. My first task was around writing an installer for a java app on windows, but my tutor tried to push me further. He saw my interest around some specific layers of their perimeter, such as Hadoop and Kafka. He gave to me a chance to work directly on those. A small API that was could load old monitoring data stored in HDFS and expose them back into the ‚Äúreal-time‚Äù visualization tool. I also used Kafka and even deployed a small HBase cluster for testing. I can‚Äôt thank my tutor enough for giving me this chance, and for allowing me to discover what will become my focus: distributed systems. ","date":"2020-09-30","objectID":"/posts/ten-years-programming/:2:0","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["personal"],"content":"Let‚Äôs meet other people üëã Around the same time, I discovered tech meetups and conferences. At that time, Google I/O was a major event with people jumping from a plane and streaming it through Google Glass. I found out there was a group of people watching the live together. And this is how I discovered my local GDG/JUG ü•≥ I learned so many things by watching local talks, even if it was difficult to grasp everything at first. I remember taking üìù about what I didn‚Äôt understand, to learn about it later. I also met amazing persons, that are now friends and/or mentors. I remember feeling humble to be able to learn from them. I also discovered more global tech conferences. I asked as a birthday üéÅ to go to Devoxx France and DotScale, in 2014. It was awesome üòé By dint of watching talks, I wanted to give some. I started small, giving talks at my engineering school, then moved to the JUG itself. I learned a lot by making a lot of mistakes, but I‚Äôm pretty happy how things turned out, as I‚Äôm now speaking at tech conferences as part of my current work. I also started to be involved in events and organizations such as: The JUG/GDG A coworking place Startup Weekend Devoxx4kids DevFest du bout du monde ","date":"2020-09-30","objectID":"/posts/ten-years-programming/:3:0","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["personal"],"content":"Learning big data üíæ After my graduation and a(nother) part-time internship at OVH, I started working on something called Metrics Data Platform. It is the platform massively used internally to store, query and alert on timeseries data. We avoid the Borgmon approach (deploying Prometheus‚Äôs like database for every team), instead we created a unique platform to ingest all OVHcloud‚Äôs datapoints using a big-data approach. Here‚Äôs the key point of Metrics: multi-tenant: as we said before, a single metrics cluster is handling all telemetry, from servers to applications and smart data centers from OVHcloud. scalable: today we are receiving around 1.8 million datapoints per second/s üôà for about 450 million timeseries üôâ. During European daytime, we are reading around 4.5 millions datapoints per seconds thank to Grafana‚Äôs auto-refresh mode üôä multi-protocol support: we didn‚Äôt want to reflect our infrastructure choice to our users, so we wrote some proxies that can translate known protocols to our query language, so users can query and push data using OpenTSDB, Prometheus, InfluxDB and so on. based on open source we are using Warp10 as the core of our infrastructure with Kafka and HBase. Alerting was built with Apache flink. We open sourced many software, from agent to our proxies. We also gave many talks about what we learnt. I had the chance to built Metrics from the ground. I started working on the management layer and proxies. Then I wanted to learn operations, so I learned it by deploying Hadoop clusters ü§Ø it took me a while to be able to start doing on-calls. I cannot count how many nights I was up, trying to fix some buggy softwares, or yelling at HBase for an inconsistent hbck, or trying to find a way to handle a side effect of a loosing multiple racks. Our work was highly technical, and I loved it: We optimized a lot of things, from HBase to our Go‚Äôs based proxies. optimize HBase's data balancer or fix issues with Go‚Äôs gc was almost a normal task to do We saw Metrics‚Äôs growth, from hundred to millions of datapoints üòé we saw systems breaking at scale, causing us to rewrite software or change architecture. Production became the final test. Every software we developed had a keep it simple, yet scalable policy, and doing on-calls was a good way to ensure software quality. We all learned it the hard way I guess ü§£ We were only 4 to 6 to handle ~800 servers, 3 Hadoop clusters, and thousands of lines of Java/Go/Rust/Ansible codes. As always, things were not always magical, and i struggled more time than I can count. I learned that personal struggle is more difficult than technical, as you can always drill-down your tech problems by reading the code. The team was amazing üöÄ, and we were helping each other a lot ü§ù ","date":"2020-09-30","objectID":"/posts/ten-years-programming/:4:0","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["personal"],"content":"Searching for planets üî≠ ü™ê When I started working on Metrics, we did a lot of internal on boarding. At his core, metrics is usine Warp10, which is coming with his own language to analyze timeseries. This provides heavy query-capabilities, but as it is stack-based, getting started was difficult. I needed a project to dive into timeseries analysis. I love astronomy üî≠, but there‚Äôs too much ‚òÅÔ∏è (not the servers) in my city. I decided to look for astronomical timeseries. Turns out there is a lot, but one use case triggered my interest: exoplanet‚Äôs search. Almost everything from NASA is Opendata, so we decided to create HelloExoWorld. We imported the 25TB dataset into a Warp10 instance and start writing some WarpScript to search for transits. We wrote a hands-on about it. We also did several labs in french conferences like Devoxx and many others. ","date":"2020-09-30","objectID":"/posts/ten-years-programming/:5:0","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["personal"],"content":"IO timeout üöß Around 2018, OVHcloud started Managed Kubernetes, a free K8S control-plane. With this product we saw more developers coming to OVHcloud. We started thinking about how we could help them. Running stateful systems is hard, so maybe we could offer them some databases or queues in a As-a-Service fashion. We started to design such products from our Metrics experience. We started the IO Vision to offer popular Storage APIs in front of a scalable storage. Does it sound familiar? üòá I had a lot of fun working on that vision as a Technical Leader. We started with queuing with ioStream. We wanted something that was: Multi-tenant Multi-protocol Geo-replicated natively Less operation burden at scale than Kafka We built ioStream around Apache Pulsar, and opened the beta around September 2019. As the same time we were working on Kafka‚Äôs support as a proxy in Rust. Writing such a software capable of translating Kafka‚Äôs TCP frames to Pulsar with a state-machine was a fun and challenging work. Rust is really a nice language to write such software. Then we worked with Apache Pulsar‚Äôs PMC to introduce a Kafka protocol handler on Pulsar brokers. I had the chance to work closely to two PMCs, it was an amazing experience for me üöÄ You can read about our collaboration here. Unfortunately as stated by the official communication, the project has been shut down: However, the limited success of the beta service and other strategic focuses, have resulted in us taking the very difficult decision to close it. I learned a lot of things, both technically and on the product-side, especially considering the fact that it was shutdown. ","date":"2020-09-30","objectID":"/posts/ten-years-programming/:6:0","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["personal"],"content":"Today After ioStream‚Äôs shutdown, most of the team moved to create a new LBaaS. I helped them wrote an operator to schedule HAProxy‚Äôs containers on a Kubernetes cluster. It was a nice introduction to operators. Then I decided to join the Managed Kubernetes ‚ò∏Ô∏è team. This is my current team now, where I‚Äôm having a lot of fun working around ETCD. I really hope the next 10 years will be as fun as the last 10 years üòá Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2020-09-30","objectID":"/posts/ten-years-programming/:7:0","tags":null,"title":"10 years of programming and counting üöÄ","uri":"/posts/ten-years-programming/"},{"categories":["FoundationDB","oss"],"content":"Opensourcing Record-Store","date":"2020-09-23","objectID":"/posts/announcing-record-store/","tags":null,"title":"Announcing Record-Store, a new (experimental) place for your data","uri":"/posts/announcing-record-store/"},{"categories":["FoundationDB","oss"],"content":"TL;DR: I‚Äôm really happy to announce my latest open-source project called Record-Store üöÄ Please check it out on https://pierrez.github.io/record-store. ","date":"2020-09-23","objectID":"/posts/announcing-record-store/:0:0","tags":null,"title":"Announcing Record-Store, a new (experimental) place for your data","uri":"/posts/announcing-record-store/"},{"categories":["FoundationDB","oss"],"content":"What? Record-Store is a layer running on top of FoundationDB. It provides abstractions to create, load and deletes customer-defined data called records, which are hold into a RecordSpace. We would like to have this kind of flow for developers: Opening RecordSpace, for example prod/users Create a protobuf definition which will be used as schema Upsert schema Push records Query records delete records You need another KeySpace to store another type of data, or maybe a KeySpace dedicated to production env? Juste create it and you are good to go! ","date":"2020-09-23","objectID":"/posts/announcing-record-store/:1:0","tags":null,"title":"Announcing Record-Store, a new (experimental) place for your data","uri":"/posts/announcing-record-store/"},{"categories":["FoundationDB","oss"],"content":"Features It is currently an experiment, but it already has some strong features: Multi-tenant A tenant can create as many RecordSpace as we want, and we can have many tenants. Standard API We are exposing the record-store with standard technologies: gRPC very experimental GraphQL Scalable We are based on the same tech behind CloudKit called the Record Layer, Transactional We are running on top of FoundationDB. FoundationDB gives you the power of ACID transactions in a distributed database. Encrypted Data are encrypted by default. Multi-model For each RecordSpace, you can define a schema, which is in-fact only a Protobuf definition. You need to store some users, or a more complicated structure? If you can represent it as Protobuf, you are good to go! Index-defined queries Your queries‚Äôs capabilities are defined by the indexes you put on your schema. Secured We are using Biscuit, a mix of JWT and Macaroons to ensure auth{entication, orization}. ","date":"2020-09-23","objectID":"/posts/announcing-record-store/:2:0","tags":null,"title":"Announcing Record-Store, a new (experimental) place for your data","uri":"/posts/announcing-record-store/"},{"categories":["FoundationDB","oss"],"content":"Why? Lately, I have been playing a lot with my ETCD-Layer that is using the Record-Layer. Thanks to it, I was able to bootstrap my ETCD-layer very quickly, but I was not using a tenth of the capacities of this library. So I decided to go deeper. What would a gRPC abstraction of the Record-Layer look like? The name of this project itself is a tribute to the Record Layer as we are exposing the layer within a gRPC interface. ","date":"2020-09-23","objectID":"/posts/announcing-record-store/:3:0","tags":null,"title":"Announcing Record-Store, a new (experimental) place for your data","uri":"/posts/announcing-record-store/"},{"categories":["FoundationDB","oss"],"content":"Try it out Record-Store is open sourced under Apache License V2 in https://github.com/PierreZ/record-store and the documentation can be found https://pierrez.github.io/record-store. Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2020-09-23","objectID":"/posts/announcing-record-store/:4:0","tags":null,"title":"Announcing Record-Store, a new (experimental) place for your data","uri":"/posts/announcing-record-store/"},{"categories":["etcd","diving into"],"content":" Diving Into is a blogpost serie where we are digging a specific part of the project‚Äôs basecode. In this episode, we will digg into the implementation behind ETCD‚Äôs Linearizable reads. ","date":"2020-09-18","objectID":"/posts/diving-into-etcd-linearizable/:0:0","tags":null,"title":"Diving into ETCD's linearizable reads","uri":"/posts/diving-into-etcd-linearizable/"},{"categories":["etcd","diving into"],"content":"What is ETCD? From the official website: etcd is a strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines. It gracefully handles leader elections during network partitions and can tolerate machine failure, even in the leader node. ETCD is well-known to be Kubernetes‚Äôs datastore, and a CNCF incubating project. ","date":"2020-09-18","objectID":"/posts/diving-into-etcd-linearizable/:1:0","tags":null,"title":"Diving into ETCD's linearizable reads","uri":"/posts/diving-into-etcd-linearizable/"},{"categories":["etcd","diving into"],"content":"Linea-what? Let‚Äôs quote Kyle Kingsbury, a.k.a ‚ÄúAphyr‚Äù, for this one: Linearizability is one of the strongest single-object consistency models, and implies that every operation appears to take place atomically, in some order, consistent with the real-time ordering of those operations: e.g., if operation A completes before operation B begins, then B should logically take effect after A. ","date":"2020-09-18","objectID":"/posts/diving-into-etcd-linearizable/:2:0","tags":null,"title":"Diving into ETCD's linearizable reads","uri":"/posts/diving-into-etcd-linearizable/"},{"categories":["etcd","diving into"],"content":"Why? ETCD is using Raft, a consensus algorithm at his core. As always, the devil is hidden in the details, or when things are going wrong. Here‚Äôs an example: node1 is leader and heartbeating properly to node2 and node3, network partition is happening, and node1 is isolated from the others. At this moment, all the actions are depending on timeouts and settings. In a (close) future, all nodes will go into election mode and node 2 and 3 will be able to create a quorum. This can lead to this situation: node1 thinks he is a leader as heartbeat timeouts and retry are not yet reached, so he can serve reads üò± node2 and node3 have elected a new leader and are working again, accepting writes. This situation is violating Linearizable reads, as reads going through node1 will not see the last updates from the current leader. How can we solve this? One way is to use ReadIndex! ","date":"2020-09-18","objectID":"/posts/diving-into-etcd-linearizable/:3:0","tags":null,"title":"Diving into ETCD's linearizable reads","uri":"/posts/diving-into-etcd-linearizable/"},{"categories":["etcd","diving into"],"content":"ReadIndex The basic idea behind this is to confirm that the leader is true leader or not by sending a message to the followers. If a majority of responses are healthy, then the leader can safely serve the reads. Let‚Äôs dive into the implementation! All codes are from the current latest release v3.4.13. Let‚Äôs take a Range operation: if !r.Serializable { err = s.linearizableReadNotify(ctx) trace.Step(\"agreement among raft nodes before linearized reading\") if err != nil { return nil, err } } func (s *EtcdServer) linearizableReadNotify(ctx context.Context) error { s.readMu.RLock() nc := s.readNotifier s.readMu.RUnlock() // signal linearizable loop for current notify if it hasn't been already select { case s.readwaitc \u003c- struct{}{}: default: } // wait for read state notification select { case \u003c-nc.c: return nc.err case \u003c-ctx.Done(): return ctx.Err() case \u003c-s.done: return ErrStopped } } So in linearizableReadNotify, we are waiting for a signal. readwaitc is used in another goroutine called linearizableReadLoop. This goroutines will call this: func (n *node) ReadIndex(ctx context.Context, rctx []byte) error { return n.step(ctx, pb.Message{Type: pb.MsgReadIndex, Entries: []pb.Entry{{Data: rctx}}}) } that will create a MsgReadIndex message that will be handled in stepLeader, who will send the message to the followers, like this: case pb.MsgReadIndex: // If more than the local vote is needed, go through a full broadcast, // otherwise optimize. if !r.prs.IsSingleton() { // PZ: omitting some code here switch r.readOnly.option { case ReadOnlySafe: r.readOnly.addRequest(r.raftLog.committed, m) // The local node automatically acks the request. r.readOnly.recvAck(r.id, m.Entries[0].Data) r.bcastHeartbeatWithCtx(m.Entries[0].Data) case ReadOnlyLeaseBased: ri := r.raftLog.committed if m.From == None || m.From == r.id { // from local member r.readStates = append(r.readStates, ReadState{Index: ri, RequestCtx: m.Entries[0].Data}) } else { r.send(pb.Message{To: m.From, Type: pb.MsgReadIndexResp, Index: ri, Entries: m.Entries}) } } So, the leader is sending a heartbeat in ReadOnlySafe mode. Turns out there is two modes: const ( // ReadOnlySafe guarantees the linearizability of the read only request by // communicating with the quorum. It is the default and suggested option. ReadOnlySafe ReadOnlyOption = iota // ReadOnlyLeaseBased ensures linearizability of the read only request by // relying on the leader lease. It can be affected by clock drift. // If the clock drift is unbounded, leader might keep the lease longer than it // should (clock can move backward/pause without any bound). ReadIndex is not safe // in that case. ReadOnlyLeaseBased ) Responses from the followers will be handled here: case pb.MsgHeartbeatResp: // PZ: omitting some code here rss := r.readOnly.advance(m) for _, rs := range rss { req := rs.req if req.From == None || req.From == r.id { // from local member r.readStates = append(r.readStates, ReadState{Index: rs.index, RequestCtx: req.Entries[0].Data}) } else { r.send(pb.Message{To: req.From, Type: pb.MsgReadIndexResp, Index: rs.index, Entries: req.Entries}) } } We are storing things into a ReadState: // ReadState provides state for read only query. // It's caller's responsibility to call ReadIndex first before getting // this state from ready, it's also caller's duty to differentiate if this // state is what it requests through RequestCtx, eg. given a unique id as // RequestCtx type ReadState struct { Index uint64 RequestCtx []byte } Now that the state has been updated, we need to unblock our linearizableReadLoop: for !timeout \u0026\u0026 !done { select { case rs = \u003c-s.r.readStateC: Cool, another channel! Turns out, readStateC is updated in one of the main goroutine: // start prepares and starts raftNode in a new goroutine. It is no longer safe // to modify the fields after it has been started. func (r *raftNode) start(rh *raftReadyHandler) { internalTimeout := time.Second go func() { defer r.onStop() islead := false for { select { case \u003c","date":"2020-09-18","objectID":"/posts/diving-into-etcd-linearizable/:4:0","tags":null,"title":"Diving into ETCD's linearizable reads","uri":"/posts/diving-into-etcd-linearizable/"},{"categories":["etcd","diving into"],"content":"One more thing: Follower read We went through stepLeader a lot, be there is something interesting in stepFollower: case pb.MsgReadIndex: if r.lead == None { r.logger.Infof(\"%x no leader at term %d; dropping index reading msg\", r.id, r.Term) return nil } m.To = r.lead r.send(m) This means that a follower can send a MsgReadIndex message to perform the same kind of checks than a leader. This small features is in fact enabling follower-reads on ETCD ü§© That is why you can see Range requests from a follower. ","date":"2020-09-18","objectID":"/posts/diving-into-etcd-linearizable/:5:0","tags":null,"title":"Diving into ETCD's linearizable reads","uri":"/posts/diving-into-etcd-linearizable/"},{"categories":["etcd","diving into"],"content":"operational tips If you are running etcd \u003c= 3.4, make sure logger=zap is set. Like this, you will be able to see some tracing logs, and I trully hope you will not witness this one: { \"level\": \"info\", \"ts\": \"2020-08-12T08:24:56.181Z\", \"caller\": \"traceutil/trace.go:145\", \"msg\": \"trace[677217921] range\", \"detail\": \"{range_begin:/...redacted...; range_end:; response_count:1; response_revision:2725080604; }\", \"duration\": \"1.553047811s\", \"start\": \"2020-08-12T08:24:54.628Z\", \"end\": \"2020-08-12T08:24:56.181Z\", \"steps\": [ \"trace[677217921] 'agreement among raft nodes before linearized reading' (duration: 1.534322015s)\" ] } there is a random performance issue on etcd 3.4 there is some metrics than you can watch for ReadIndex issues: etcd_server_read_indexes_failed_total etcd_server_slow_read_indexes_total Thank you for reading my post! feel free to react to this article, I‚Äôm also available on Twitter if needed. ","date":"2020-09-18","objectID":"/posts/diving-into-etcd-linearizable/:6:0","tags":null,"title":"Diving into ETCD's linearizable reads","uri":"/posts/diving-into-etcd-linearizable/"},{"categories":["raft","notesabout"],"content":"List of ressources gleaned about Raft","date":"2020-07-30","objectID":"/posts/notes-about-raft/","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":" Notes About is a blogpost serie you will find a lot of links, videos, quotes, podcasts to click on about a specific topic. Today we will discover Raft‚Äôs paper called ‚ÄòIn Search of an Understandable Consensus Algorithm‚Äô. As I‚Äôm digging into ETCD, I needed to refresh my memory about Raft. I started by reading the paper located here and I‚Äôm also playing with the amazing Raft labs made by PingCAP. These labs are derived from the lab2:raft and lab3:kvraft from the famous MIT 6.824 course but rewritten in Rust. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:0:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Abstract Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, andit is as efficient as Paxos, but its structure is differentfrom Paxos; this makes Raft more understandable thanPaxos and also provides a better foundation for build-ing practical systems. Raft separates the key elements of consensus, such asleader election, log replication, and safety, and it enforcesa stronger degree of coherency to reduce the number ofstates that must be considered. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:1:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Introduction Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members. Paxos has dominated the discussion of consensus algorithms over the last decade. Unfortunately, Paxos is quite difficult to understand, inspite of numerous attempts to make it more approachable.Furthermore, its architecture requires complex changes to support practical systems. As a result, both systembuilders and students struggle with Paxos. Our approach was unusual in that our primary goal was understandability. We believe that Raft is superior to Paxos and other consensus algorithms, both for educational purposes and as a foundation for implementation. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:2:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Replicated state machines The main idea is to compute identical copies of the same state (i.e x:3, y:9) in case of machines‚Äôs failure. Most of the time, an ordered wal (write-ahead log) is used in the implementation, to hold the mutation (x:4). Keeping the replicated log consistent is the job of the consensus algorithm, here Raft. Raft creates a true split between: the consensus module, the wal, the state machine. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:3:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"What‚Äôs wrong with Paxos? The paper is listing the drawbacks of Paxos: difficult to understand, and I can‚Äôt blame them many details are missing from the paper to implement Multi-Paxos as the paper is mainly describing single-decree Paxos It is simpler and more efficient to design a system around a log, where new entries are appended sequentially in a constrained order. As a result, practical systems bear little resemblance to Paxos. Each implementation begins with Paxos, discovers the difficulties in implementing it, and then develops a significantly different architecture. This is time-consuming and error-prone, and the difficulties of understanding Paxos exacerbate the problem. The following com-ment from the Chubby implementers is typical: There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system the final system will be based on an un-proven protocol [4]. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:4:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Designing for understandability Beside all the others goals of Raft: a complete and practical foundation for system building, must be safe under all conditions and available under typical operating conditions, must be efficient for common operations, understandability was the most difficult challenge: It must be possible for a large audience to understand the algorithm comfortably. In addition, it must be possible to develop intuitions about the algorithm, so that system builders can make the extensions that are inevitable in real-world implementations. we divided problems into separate pieces that could be solved, explained, and understood relatively independently. For example, in Raft we separated leader election, log replication, safety, and membership changes. Our second approach was to simplify the state spaceby reducing the number of states to consider, making thesystem more coherent and eliminating nondeterminism where possible. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:5:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"The Raft consensus algorithm Raft is heavily relying on the leader pattern: Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. Thanks to this pattern, Raft is splitting the consensus problem into 3: Leader election Log replication Safety ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:6:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Raft basics Each server can be in one of the three states: Leader handle all requests, Follower passive member, they issue no requests on their own but simply respond to requests from leaders and candidates, Candidate is used to elect a new leader. Leader is elected through election: Each term (interval of time of arbitrary length packed with an number) begins with an election, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term. In the case of a split vote, the term will end with no leader; a new term (with a new election) will begin. Terms act as a logical clock [14] in Raft. Each server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate; if one server‚Äôs current term is smaller than the other‚Äôs, then it updates its current term to the larger value. If a candidate or leader discovers that its term is out of date, it immediately reverts to fol-lower state. If a server receives a request with a stale term number, it rejects the request. RPC is used for communications: RequestVote RPCs are initiated by candidates during elections, Append-Entries RPCs are initiated by leaders to replicate log en-tries and to provide a form of heartbeat. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:6:1","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Leader election A good vizualization is available here. The key-point of the election are the fact that: nodes vote for themselves, the term number is used to recover from failure, election timeouts are randomized. To begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate continues in this state until one of three things happens: (a) it wins the election, (b) another server establishes itself as leader, (c) a period of time goes by with no winner. Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly. To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150‚Äì300ms). ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:6:2","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Log replication A good vizualization is available here. Once a leader has been elected, it begins servicing client requests. Each client request contains a command to be executed by the replicated state machines. The leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry. When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client. The term numbers in log entries are used to detect inconsistencies between logs The leader decides when it is safe to apply a log entry to the state machines; such an entry is called committed. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. Raft is implementing a lot of safety inside the log: When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, then it refuses the new entries This is really interesting to be leader-failure proof. And for follower‚Äôs failure: In Raft, the leader handles inconsistencies by forcing the followers‚Äô logs to duplicate its own. To bring a follower‚Äôs log into consistency with its own,the leader must find the latest log entry where the two logs agree, delete any entries in the follower‚Äôs log after that point, and send the follower all of the leader‚Äôs entries after that point. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:6:3","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Safety ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:7:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Leader election As Raft guarantees that all the committed entries are available on all followers, log entries only flow in one di-rection, from leaders to followers, and leaders never over-write existing entries in their logs. Raft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries. A candidate must contact a majority of the cluster in order to be elected, which means that every committed entry must be present in at least one of those servers. Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the log send with the same term, then whichever log is longer is more up-to-date. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:7:1","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Committing entries from previous terms Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader‚Äôs current term are committed by counting replicas. This behavior avoids future leaders to attempt to finish replicating an entry where the leader crashes before committing an entry. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:7:2","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Follower and candidate crashes If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:7:3","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Cluster membership changes This section presents how to do cluster configuration(the set of servers participating in the consensus algorithm). Raft implements a two-phase approach: In Raft the cluster first switches to a transitional configuration we call joint consensus; once the joint consensus has been committed,the system then transitions to the new configuration. The joint consensus combines both the old and new configurations: Log entries are replicated to all servers in both con-figurations, Any server from either configuration may serve asleader, Agreement (for elections and entry commitment) requires separate majorities from both the old and new configurations. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:8:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Log compaction As the WAL holds the commands, we need to compact it. Raft is using snapshots as describe here: the leader must occasionally send snapshots to followers that lag behind. This is useful for slow follower or a new server joining the cluster. The leader uses a new RPC called InstallSnapshot to send snapshots to followers that are too far behind. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:9:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":["raft","notesabout"],"content":"Client interaction Clients of Raft send all of their requests to the leader. When a client first starts up, it connects to a randomly-chosen server. If the client‚Äôs first choice is not the leader,that server will reject the client‚Äôs request and supply information about the most recent leader it has heard from. Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2020-07-30","objectID":"/posts/notes-about-raft/:10:0","tags":null,"title":"Notes about Raft's paper","uri":"/posts/notes-about-raft/"},{"categories":null,"content":"Abstract OVHcloud is the biggest European cloud provider. From dedicated servers to Managed Kubernetes, from VMware¬Æ based Hosted Private Cloud to OpenStack-based Public Cloud, we have over 1.4 million customers worldwide. Internally, we have been running Apache Kafka for years, and despite all the skills obtained operating multiples clusters with millions of messages per second, we decided to shift and build the foundation of our ‚Äôtopic-as-a-service‚Äô product called ioStream on Apache Pulsar. In this talk, you will have the insights of why we decided to use Apache Pulsar instead of Apache Kafka as the core of ioStream. We will tell you our journey to use Apache Pulsar, from our deployments to the management, what did work and what did not. Occurences Pulsar-Summit 2020 Ressources ","date":"2020-06-16","objectID":"/talks/messaging-solution-pulsar/:0:0","tags":null,"title":"Building a Messaging Solutions for OVHcloud with Apache Pulsar","uri":"/talks/messaging-solution-pulsar/"},{"categories":null,"content":"Slides The slides are also available here ","date":"2020-06-16","objectID":"/talks/messaging-solution-pulsar/:1:0","tags":null,"title":"Building a Messaging Solutions for OVHcloud with Apache Pulsar","uri":"/talks/messaging-solution-pulsar/"},{"categories":null,"content":"Videos Photos and tweets I'm very excited to talk at the first @PulsarSummit Virtual Conference! I will be speaking about: - Bringing native Kafka protocol support to Pulsar - Building a Messaging Solutions for OVHcloud with #ApachePulsar Register now at https://t.co/JSOwQM9I1Z#PulsarSummit2020 pic.twitter.com/MZJBVaaqMY ‚Äî Pierre Zemb (@PierreZ) June 2, 2020 ","date":"2020-06-16","objectID":"/talks/messaging-solution-pulsar/:2:0","tags":null,"title":"Building a Messaging Solutions for OVHcloud with Apache Pulsar","uri":"/talks/messaging-solution-pulsar/"},{"categories":null,"content":" Abstract Kafka-on-Pulsar has been one of the most anticipated features in the Pulsar ecosystem. The Kafka-on-Pulsar project was initiated by StreamNative and the OVHCloud team quickly joined the project to collaborate on its development. Kafka-on-Pulsar enables Kafka applications to leverage Pulsar‚Äôs powerful features, such as streamlined operations with enterprise-grade multi-tenancy, without modifying code. In this webinar, Sijie Guo, from StreamNative, and Pierre Zemb, from OVHCloud, will introduce KoP and discuss the following: What are the key benefits? What is the protocol handler and how does it work? How KoP is implemented? What are the new use cases it unlocks? Watch a Live Demo! Occurences Kafka-on-Pulsar Webinar Pulsar-Summit Ressources ","date":"2020-03-31","objectID":"/talks/announcing-kop/:0:0","tags":null,"title":"Introducing Kafka-on-Pulsar: Bring native Kafka protocol support to Apache Pulsar","uri":"/talks/announcing-kop/"},{"categories":null,"content":"Slides The slides are also available here ","date":"2020-03-31","objectID":"/talks/announcing-kop/:1:0","tags":null,"title":"Introducing Kafka-on-Pulsar: Bring native Kafka protocol support to Apache Pulsar","uri":"/talks/announcing-kop/"},{"categories":null,"content":"Videos ","date":"2020-03-31","objectID":"/talks/announcing-kop/:2:0","tags":null,"title":"Introducing Kafka-on-Pulsar: Bring native Kafka protocol support to Apache Pulsar","uri":"/talks/announcing-kop/"},{"categories":null,"content":"June 2020 ","date":"2020-03-31","objectID":"/talks/announcing-kop/:2:1","tags":null,"title":"Introducing Kafka-on-Pulsar: Bring native Kafka protocol support to Apache Pulsar","uri":"/talks/announcing-kop/"},{"categories":null,"content":"March 2020 Photos and tweets .@PierreZ from @OVHcloud and @sijieg from @streamnativeio will co-host a webinar about KoP at March 31. They will introduce KoP and demonstrate how it unlocks new use cases by integrating two popular messaging and streaming ecosystems. Save your spot!https://t.co/2u0SDq15xZ pic.twitter.com/PvMvURvAEg ‚Äî StreamNative (@streamnativeio) March 30, 2020 ","date":"2020-03-31","objectID":"/talks/announcing-kop/:2:2","tags":null,"title":"Introducing Kafka-on-Pulsar: Bring native Kafka protocol support to Apache Pulsar","uri":"/talks/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"We are excited to announce that StreamNative and OVHcloud are open-sourcing ‚ÄúKafka on Pulsar‚Äù (KoP).  KoP brings the native Apache Kafka protocol support to Apache Pulsar by introducing a Kafka protocol handler on Pulsar brokers","date":"2020-03-24","objectID":"/posts/announcing-kop/","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":" This is a repost from OVHcloud‚Äôs official blogpost., please read it there to support my company. Thanks Horacio Gonzalez for the awesome drawings! This post has been published on both the StreamNative and OVHcloud blogs and was co-authored by Sijie Guo, Jia Zhai and Pierre Zemb. Thanks Horacio Gonzalez for the illustrations! We are excited to announce that StreamNative and OVHcloud are open-sourcing ‚ÄúKafka on Pulsar‚Äù (KoP). KoP brings the native Apache Kafka protocol support to Apache Pulsar by introducing a Kafka protocol handler on Pulsar brokers. By adding the KoP protocol handler to your existing Pulsar cluster, you can now migrate your existing Kafka applications and services to Pulsar without modifying the code. This enables Kafka applications to leverage Pulsar‚Äôs powerful features, such as: Streamlined operations with enterprise-grade multi-tenancy Simplified operations with a rebalance-free architecture Infinite event stream retention with Apache BookKeeper and tiered storage Serverless event processing with Pulsar Functions ","date":"2020-03-24","objectID":"/posts/announcing-kop/:0:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"What is Apache Pulsar? Apache Pulsar is an event streaming platform designed from the ground up to be cloud-native- deploying a multi-layer and segment-centric architecture. The architecture separates serving and storage into different layers, making the system container-friendly. The cloud-native architecture provides scalability, availability and resiliency and enables companies to expand their offerings with real-time data-enabled solutions. Pulsar has gained wide adoption since it was open-sourced in 2016 and was designated an Apache Top-Level project in 2018. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:1:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"The need behind KoP Pulsar provides a unified messaging model for both queueing and streaming workloads. Pulsar implemented its own protobuf-based binary protocol to provide high performance and low latency. This choice of protobuf makes it convenient to implement Pulsar clients and the project already supports Java, Go, Python and C++ languages alongside thirdparty clients provided by the community. However, existing applications written using other messaging protocols had to be rewritten to adopt Pulsar‚Äôs new unified messaging protocol. To address this, the Pulsar community developed applications to facilitate the migration to Pulsar from other messaging systems. For example, Pulsar provides a Kafka wrapper on Kafka Java API, which allows existing applications that already use Kafka Java client switching from Kafka to Pulsar without code change. Pulsar also has a rich connector ecosystem, connecting Pulsar with other data systems. Yet, there was still a strong demand from those looking to switch from other Kafka applications to Pulsar. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:2:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"StreamNative and OVHcloud‚Äôs collaboration StreamNative was receiving a lot of inbound requests for help migrating from other messaging systems to Pulsar and recognized the need to support other messaging protocols (such as AMQP and Kafka) natively on Pulsar. StreamNative began working on introducing a general protocol handler framework in Pulsar that would allow developers using other messaging protocols to use Pulsar. Internally, OVHcloud had been running Apache Kafka for years, but despite their experience operating multiple clusters with millions of messages per second on Kafka, there were painful operational challenges. For example, putting thousands of topics from thousands of users into a single cluster was difficult without multi-tenancy. As a result, OVHcloud decided to shift and build the foundation of their topic-as-a-service product, called ioStream, on Pulsar instead of Kafka. Pulsar‚Äôs multi-tenancy and the overall architecture with Apache Bookkeeper simplified operations compared to Kafka. After spawning the first region, OVHcloud decided to implement it as a proof-of-concept proxy capable of transforming the Kafka protocol to Pulsar on the fly. During this process, OVHcloud discovered that StreamNative was working on bringing the Kafka protocol natively to Pulsar, and they joined forces to develop KoP. KoP was developed to provide a streamlined and comprehensive solution leveraging Pulsar and BookKeeper‚Äôs event stream storage infrastructure and Pulsar‚Äôs pluggable protocol handler framework. KoP is implemented as a protocol handler plugin with protocol name ‚Äúkafka‚Äù. It can be installed and configured to run as part of Pulsar brokers. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:3:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"The distributed log Both Pulsar and Kafka share a very similar data model around log for both pub/sub messaging and event streaming. For example, both are built on top of a distributed log. A key difference between these two systems is how they implement the distributed log. Kafka implements the distributed log in a partition-basis architecture, where a distributed log (a partition in Kafka) is designated to store in a set of brokers, while Pulsar deploys a segment-based architecture to implement its distributed log by leveraging Apache BookKeeper as its scale-out segment storage layer. Pulsar‚Äôs segment based architecture provides benefits such as rebalance-free, instant scalability, and infinite event stream storage. You can learn more about the key differences between Pulsar and Kafka in this Splunk blog and in this blog from the Bookkeeper project. Since both of the systems are built on a similar data model, a distributed log, it is very simple to implement a Kafka-compatible protocol handler by leveraging Pulsar‚Äôs distributed log storage and its pluggable protocol handler framework (introduced in the 2.5.0 release). ","date":"2020-03-24","objectID":"/posts/announcing-kop/:4:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Implementations The implementation is done by comparing the protocols between Pulsar and Kafka. We found that there are a lot of similarities between these two protocols. Both protocols are comprised of the following operations: Topic Lookup: All the clients connect to any broker to lookup the metadata (i.e. the owner broker) of the topics. After fetching the metadata, the clients establish persistent TCP connections to the owner brokers. Produce: The clients talk to the owner broker of a topic partition to append the messages to a distributed log. Consume: The clients talk to the owner broker of a topic partition to read the messages from a distributed log. Offset: The messages produced to a topic partition are assigned with an offset. The offset in Pulsar is called MessageId. Consumers can use offsets to seek to a given position within the log to read messages. Consumption State: Both systems maintain the consumption state for consumers within a subscription (or a consumer group in Kafka). The consumption state is stored in __offsets topic in Kafka, while the consumption state is stored as cursors in Pulsar. As you can see, these are all the primitive operations provided by a scale-out distributed log storage such as Apache BookKeeper. The core capabilities of Pulsar are implemented on top of Apache BookKeeper. Thus it is pretty easy and straightforward to implement the Kafka concepts by using the existing components that Pulsar has developed on BookKeeper. The following figure illustrates how we add the Kafka protocol support within Pulsar. We are introducing a new Protocol Handlerwhich implements the Kafka wire protocol by leveraging the existing components (such as topic discovery, the distributed log library ‚Äì ManagedLedger, cursors and etc) that Pulsar already has. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:5:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Topics In Kafka, all the topics are stored in one flat namespace. But in Pulsar, topics are organized in hierarchical multi-tenant namespaces. We introduce a setting kafkaNamespace in broker configuration to allow the administrator configuring to map Kafka topics to Pulsar topics. In order to let Kafka users leverage the multi-tenancy feature of Apache Pulsar, a Kafka user can specify a Pulsar tenant and namespace as its SASL username when it uses SASL authentication mechanism to authenticate a Kafka client. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:5:1","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Message ID and offset In Kafka, each message is assigned with an offset once it is successfully produced to a topic partition. In Pulsar, each message is assigned with a MessageID. The message id consists of 3 components, ledger-id, entry-id, and batch-index. We are using the same approach in Pulsar-Kafka wrapper to convert a Pulsar MessageID to an offset and vice versa. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:5:2","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Messages Both a Kafka message and a Pulsar message have key, value, timestamp, and headers (note: this is called ‚Äòproperties‚Äô in Pulsar). We convert these fields automatically between Kafka messages and Pulsar messages. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:5:3","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Topic lookup We use the same topic lookup approach for the Kafka request handler as the Pulsar request handler. The request handler does topic discovery to lookup all the ownerships for the requested topic partitions and responds with the ownership information as part of Kafka TopicMetadata back to Kafka clients. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:5:4","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Produce Messages When the Kafka request handler receives produced messages from a Kafka client, it converts Kafka messages to Pulsar messages by mapping the fields (i.e. key, value, timestamp and headers) one by one, and uses the ManagedLedger append API to append those converted Pulsar messages to BookKeeper. Converting Kafka messages to Pulsar messages allows existing Pulsar applications to consume messages produced by Kafka clients. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:5:5","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Consume Messages When the Kafka request handler receives a consumer request from a Kafka client, it opens a non-durable cursor to read the entries starting from the requested offset. The Kafka request handler converts the Pulsar messages back to Kafka messages to allow existing Kafka applications to consume the messages produced by Pulsar clients. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:5:6","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Group coordinator \u0026 offsets management The most challenging part is to implement the group coordinator and offsets management. Because Pulsar doesn‚Äôt have a centralized group coordinator for assigning partitions to consumers of a consumer group and managing offsets for each consumer group. In Pulsar, the partition assignment is managed by broker on a per-partition basis, and the offset management is done by storing the acknowledgements in cursors by the owner broker of that partition. It is difficult to align the Pulsar model with the Kafka model. Hence, for the sake of providing full compatibility with Kafka clients, we implemented the Kafka group coordinator by storing the coordinator group changes and offsets in a system topic called *public/kafka/*offsets in Pulsar. This allows us to bridge the gap between Pulsar and Kafka and allows people to use existing Pulsar tools and policies to manage subscriptions and monitor Kafka consumers. We add a background thread in the implemented group coordinator to periodically sync offset updates from the system topic to Pulsar cursors. Hence a Kafka consumer group is effectively treated as a Pulsar subscription. All the existing Pulsar toolings can be used for managing Kafka consumer groups as well. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:5:7","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Bridge two popular messaging ecosystems At both companies, we value customer success. We believe that providing a native Kafka protocol on Apache Pulsar will reduce the barriers for people adopting Pulsar to achieve their business success. By integrating two popular event streaming ecosystems, KoP unlocks new use cases. Customers can leverage advantages from each ecosystem and build a truly unified event streaming platform with Apache Pulsar to accelerate the development of real-time applications and services. With KoP, a log collector can continue collecting log data from its sources and producing messages to Apache Pulsar using existing Kafka integrations. The downstream applications can use Pulsar Functions to process the events arriving in the system to do serverless event streaming. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:6:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Try it out KoP is open sourced under Apache License V2 in https://github.com/streamnative/kop. We are looking forward to your issues, and PRs. You can also join #kop channel in Pulsar Slack to discuss all things about Kafka-on-Pulsar. StreamNative and OVHcloud are also hosting a webinar about KoP on March 31. If you are interested in learning more details about KoP,please sign up. Looking forward to meeting you online. ","date":"2020-03-24","objectID":"/posts/announcing-kop/:7:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":["pulsar","contribution"],"content":"Thanks The KoP project was originally initiated by StreamNative. The OVHcloud team joined the project to collaborate on the development of the KoP project. Many thanks to Pierre Zemb and Steven Le Roux from OVHcloud for their contributions to this project! ","date":"2020-03-24","objectID":"/posts/announcing-kop/:8:0","tags":null,"title":"Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar","uri":"/posts/announcing-kop/"},{"categories":null,"content":"Notes Notes about FoundationDB ","date":"2020-02-17","objectID":"/podcasts/bdh-93/:1:0","tags":["FoundationDB","big-data-hebdo"],"title":"üá´üá∑ BigData Hebdo 93 : FoundationDB","uri":"/podcasts/bdh-93/"},{"categories":["hbase","contribution"],"content":"In today‚Äôs blogpost, we‚Äôre going to take a look at our upstream contribution to Apache HBase‚Äôs stochastic load balancer, based on our experience of running HBase clusters to support OVHcloud‚Äôs monitoring.","date":"2020-02-14","objectID":"/posts/hbase-custom-data-balancing/","tags":null,"title":"Contributing to Apache HBase: custom data balancing","uri":"/posts/hbase-custom-data-balancing/"},{"categories":["hbase","contribution"],"content":" This is a repost from OVHcloud‚Äôs official blogpost., please read it there to support my company. Thanks Horacio Gonzalez for the awesome drawings! In today‚Äôs blogpost, we‚Äôre going to take a look at our upstream contribution to Apache HBase‚Äôs stochastic load balancer, based on our experience of running HBase clusters to support OVHcloud‚Äôs monitoring. ","date":"2020-02-14","objectID":"/posts/hbase-custom-data-balancing/:0:0","tags":null,"title":"Contributing to Apache HBase: custom data balancing","uri":"/posts/hbase-custom-data-balancing/"},{"categories":["hbase","contribution"],"content":"The context Have you ever wondered how: we generate the graphs for your OVHcloud server or web hosting package? our internal teams monitor their own servers and applications? All internal teams are constantly gathering telemetry and monitoring data and sending them to a dedicated team, who are responsible for handling all the metrics and logs generated by OVHcloud‚Äôs infrastructure: the Observability team. We tried a lot of different Time Series databases, and eventually chose Warp10 to handle our workloads. Warp10 can be integrated with the various big-data solutions provided by the Apache Foundation. In our case, we use Apache HBase as the long-term storage datastore for our metrics. Apache HBase, a datastore built on top of Apache Hadoop, provides an elastic, distributed, key-ordered map. As such, one of the key features of Apache HBase for us is the ability to scan, i.e. retrieve a range of keys. Thanks to this feature, we can fetch thousands of datapoints in an optimised way. We have our own dedicated clusters, the biggest of which has more than 270 nodes to spread our workloads: between 1.6 and 2 million writes per second, 24/7 between 4 and 6 million reads per second around 300TB of telemetry, stored within Apache HBase As you can probably imagine, storing 300TB of data in 270 nodes comes with some challenges regarding repartition, as every bit is hot data, and should be accessible at any time. Let‚Äôs dive in! ","date":"2020-02-14","objectID":"/posts/hbase-custom-data-balancing/:1:0","tags":null,"title":"Contributing to Apache HBase: custom data balancing","uri":"/posts/hbase-custom-data-balancing/"},{"categories":["hbase","contribution"],"content":"How does balancing work in Apache HBase? Before diving into the balancer, let‚Äôs take a look at how it works. In Apache HBase, data is split into shards called Regions, and distributed through RegionServers. The number of regions will increase as the data is coming in, and regions will be split as a result. This is where the Balancer comes in. It will move regions to avoid hotspotting a single RegionServer and effectively distribute the load. The actual implementation, called StochasticBalancer, uses a cost-based approach: It first computes the overall cost of the cluster, by looping through cost functions. Every cost function returns a number between 0 and 1 inclusive, where 0 is the lowest cost-best solution, and 1 is the highest possible cost and worst solution. Apache Hbase is coming with several cost functions, which are measuring things like region load, table load, data locality, number of regions per RegionServers‚Ä¶ The computed costs are scaled by their respective coefficients, defined in the configuration. Now that the initial cost is computed, we can try to Mutate our cluster. For this, the Balancer creates a random nextAction, which could be something like swapping two regions, or moving one region to another RegionServer. The action is applied virtually , and then the new cost is calculated. If the new cost is lower than our previous one, the action is stored. If not, it is skipped. This operation is repeated thousands of times, hence the Stochastic. At the end, the list of valid actions is applied to the actual cluster. ","date":"2020-02-14","objectID":"/posts/hbase-custom-data-balancing/:2:0","tags":null,"title":"Contributing to Apache HBase: custom data balancing","uri":"/posts/hbase-custom-data-balancing/"},{"categories":["hbase","contribution"],"content":"What was not working for us? We found out that for our specific use case, which involved: Single table Dedicated Apache HBase and Apache Hadoop, tailored for our requirements Good key distribution the number of regions per RegionServer was the real limit for us. Even if the balancing strategy seems simple, we do think that being able to run an Apache HBase cluster on heterogeneous hardware is vital, especially in cloud environments, because you may not be able to buy the same server specs again in the future. In our earlier example, our cluster grew from 80 to ~250 machines in four years. Throughout that time, we bought new dedicated server references, and even tested some special internal references. We ended-up with differents groups of hardware: some servers can handle only 180 regions, whereas the biggest can handle more than 900. Because of this disparity, we had to disable the Load Balancer to avoid the RegionCountSkewCostFunction, which would try to bring all RegionServers to the same number of regions. Two years ago we developed some internal tools, which are responsible for load balancing regions across RegionServers. The tooling worked really good for our use case, simplifying the day-to-day operation of our cluster. Open source is at the DNA of OVHcloud, and that means that we build our tools on open source software, but also that we contribute and give it back to the community. When we talked around, we saw that we weren‚Äôt the only one concerned by the heterogenous cluster problem. We decided to rewrite our tooling to make it more general, and to contribute it directly upstream to the HBase project . ","date":"2020-02-14","objectID":"/posts/hbase-custom-data-balancing/:3:0","tags":null,"title":"Contributing to Apache HBase: custom data balancing","uri":"/posts/hbase-custom-data-balancing/"},{"categories":["hbase","contribution"],"content":"Our contributions The first contribution was pretty simple, the cost function list was a constant. We added the possibility to load custom cost functions. The second contribution was about adding an optional costFunction to balance regions according to a capacity rule. ","date":"2020-02-14","objectID":"/posts/hbase-custom-data-balancing/:4:0","tags":null,"title":"Contributing to Apache HBase: custom data balancing","uri":"/posts/hbase-custom-data-balancing/"},{"categories":["hbase","contribution"],"content":"How does it works? The balancer will load a file containing lines of rules. A rule is composed of a regexp for hostname, and a limit. For example, we could have: rs[0-9] 200 rs1[0-9] 50 RegionServers with hostnames matching the first rules will have a limit of 200, and the others 50. If there‚Äôs no match, a default is set. Thanks to these rule, we have two key pieces of information: the max number of regions for this cluster the *rules for each servers The HeterogeneousRegionCountCostFunction will try to balance regions, according to their capacity. Let‚Äôs take an example‚Ä¶ Imagine that we have 20 RS: 10 RS, named rs0 to rs9, loaded with 60 regions each, which can each handle 200 regions. 10 RS, named rs10 to rs19, loaded with 60 regions each, which can each handle 50 regions. So, based on the following rules: rs[0-9] 200 rs1[0-9] 50 ‚Ä¶ we can see that the second group is overloaded, whereas the first group has plenty of space. We know that we can handle a maximum of 2,500 regions (200√ó10 + 50√ó10), and we have currently 1,200 regions (60√ó20). As such, the HeterogeneousRegionCountCostFunction will understand that the cluster is full at 48.0% (1200/2500). Based on this information, we will then try to put all the RegionServers at ~48% of the load, according to the rules. ","date":"2020-02-14","objectID":"/posts/hbase-custom-data-balancing/:5:0","tags":null,"title":"Contributing to Apache HBase: custom data balancing","uri":"/posts/hbase-custom-data-balancing/"},{"categories":["hbase","contribution"],"content":"Where to next? Thanks to Apache HBase‚Äôs contributors, our patches are now merged into the master branch. As soon as Apache HBase maintainers publish a new release, we will deploy and use it at scale. This will allow more automation on our side, and ease operations for the Observability Team. Contributing was an awesome journey. What I love most about open source is the opportunity ability to contribute back, and build stronger software. We had an opinion about how a particular issue should addressed, but the discussions with the community helped us to refine it. We spoke with e ngineers from other companies, who were struggling with Apache HBase‚Äôs cloud deployments, just as we were, and thanks to those exchanges, our contribution became more and more relevant. ","date":"2020-02-14","objectID":"/posts/hbase-custom-data-balancing/:6:0","tags":null,"title":"Contributing to Apache HBase: custom data balancing","uri":"/posts/hbase-custom-data-balancing/"},{"categories":["FoundationDB","notesabout"],"content":"List of ressources gleaned about FoundationDB","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":" Notes About is a blogpost serie you will find a lot of links, videos, quotes, podcasts to click on about a specific topic. Today we will discover FoundationDB. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:0:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Overview of FoundationDB As stated in the official documentation: FoundationDB is a distributed database designed to handle large volumes of structured data across clusters of commodity servers. It organizes data as an ordered key-value store and employs ACID transactions for all operations. It is especially well-suited for read/write workloads but also has excellent performance for write-intensive workloads. It has strong key points: Multi-model data store Easily scalable and fault tolerant Industry-leading performance Open source. From a database dialect, it provides: strict serializability(operations appear to have occurred in some order), external consistency(For any two transactions, T1 and T2, if T2 starts to commit after T1 finishes committing, then the timestamp for T2 is greater than the timestamp for T1). ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:1:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"The story FoundationDB started as a company in 2009, and then has been acquired in 2015 by Apple. It was a bad public publicity for the database as the download were removed. On April 19, 2018, Apple open sourced the software, releasing it under the Apache 2.0 license. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:2:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Tooling before coding ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:3:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Flow From the Engineering page: FoundationDB began with ambitious goals for both high performance per node and scalability. We knew that to achieve these goals we would face serious engineering challenges that would require tool breakthroughs. We‚Äôd need efficient asynchronous communicating processes like in Erlang or the Async in .NET, but we‚Äôd also need the raw speed, I/O efficiency, and control of C++. To meet these challenges, we developed several new tools, the most important of which is Flow, a new programming language that brings actor-based concurrency to C++11. Flow is more of a stateful distributed system framework than an asynchronous library. It takes a number of highly opinionated stances on how the overall distributed system should be written, and isn‚Äôt trying to be a widely reusable building block. Flow adds about 10 keywords to C++11 and is technically a trans-compiler: the Flow compiler reads Flow code and compiles it down to raw C++11, which is then compiled to a native binary with a traditional toolchain. Flow was developed before FDB, as stated in this 2013‚Äôs post: FoundationDB founder here. Flow sounds crazy. What hubris to think that you need a new programming language for your project? Three years later: Best decision we ever made. We knew this was going to be a long project so we invested heavily in tools at the beginning. The first two weeks of FoundationDB were building this new programming language to give us the speed of C++ with high level tools for actor-model concurrency. But, the real magic is how Flow enables us to use our real code to do deterministic simulations of a cluster in a single thread. We have a white paper upcoming on this. We‚Äôve had quite a bit of interest in Flow over the years and I‚Äôve given several talks on it at meetups/conferences. We‚Äôve always thought about open-sourcing it‚Ä¶ It‚Äôs not as elegant as some other actor-model languages like Scala or Erlang (see: C++) but it‚Äôs nice and fast at run-time and really helps productivity vs. writing callbacks, etc. (Fun fact: We‚Äôve only ever found two bugs in Flow. After the first, we decided that we never wanted a bug again in our programming language. So, we built a program in Python that generates random Flow code and independently-executes it to validate Flow‚Äôs behavior. This fuzz tester found one more bug, and we‚Äôve never found another.) A very good overview of Flow is available here and some details here. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:3:1","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Simulation-Driven development One of Flow‚Äôs most important job is enabling Simulation: We wanted FoundationDB to survive failures of machines, networks, disks, clocks, racks, data centers, file systems, etc., so we created a simulation framework closely tied to Flow. By replacing physical interfaces with shims, replacing the main epoll-based run loop with a time-based simulation, and running multiple logical processes as concurrent Flow Actors, Simulation is able to conduct a deterministic simulation of an entire FoundationDB cluster within a single-thread! Even better, we are able to execute this simulation in a deterministic way, enabling us to reproduce problems and add instrumentation ex post facto. This incredible capability enabled us to build FoundationDB exclusively in simulation for the first 18 months and ensure exceptional fault tolerance long before it sent its first real network packet. For a database with as strong a contract as the FoundationDB, testing is crucial, and over the years we have run the equivalent of a trillion CPU-hours of simulated stress testing. A good overview of the simulation can be found here. You can also have a look at this awesome talk! ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:3:2","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Known limitations Limitations are well described in the official documentation. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:3:3","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Recap An awesome recap is available on the Software Engineering Daily podcast: FoundationDB is tested in a very rigorous way using what‚Äôs called a deterministic simulation. The reason they needed a new programming language to do this, is that to get a deterministic simulation, you have to make something that is deterministic. It‚Äôs kind of obvious, but it‚Äôs hard to do. For example, if your process interacts with the network, or disks, or clocks, it‚Äôs not deterministic. If you have multiple threads, not deterministic. So, they needed a way to write a concurrent program that could talk with networks and disks and that type of thing. They needed a way to write a concurrent program that does all of those things that you would think are non-deterministic in a deterministic way. So, all FoundationDB processes, and FoundationDB, it‚Äôs basically all written in Flow except a very small amount of it from the SQLite B-tree. The reason why that was useful is that when you use Flow, you get all of these higher level abstraction that let what you do what feels to you like asynchronous stuff, but under the hood, it‚Äôs all implemented using callbacks in C++, which you can make deterministic by running it in a single thread. So, there‚Äôs a scheduler that just calls these callbacks one after another and it‚Äôs very crazy looking C++ code, like you wouldn‚Äôt want to read it, but it‚Äôs because of Flow they were able to implement that deterministic simulation. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:3:4","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"The Architecture According to the fdbmonitor and fdbserver: The core FoundationDB server process is fdbserver. Each fdbserver process uses up to one full CPU core, so a production FoundationDB cluster will usually run N such processes on an N-core system. To make configuring, starting, stopping, and restarting fdbserver processes easy, FoundationDB also comes with a singleton daemon process, fdbmonitor, which is started automatically on boot. fdbmonitor reads the foundationdb.conf file and starts the configured set of fdbserver processes. It is also responsible for starting backup-agent. The whole architecture is designed to automatically: load-balanced data and traffic, self-healing. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:4:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Microservices A typical FDB cluster is composed of different actors which are describe here. The most important role in FDB is the Coordinator, it uses Paxos to manage membership on a quorum to do writes. The Coordinator is mostly only used to elect some peers and during recovery. You can view it as a Zookeeper-like stack. The Coordinator starts by electing a Cluster Controller. It provides administratives informations about the cluster(I have 4 storage processes). Every process needs to register to the Cluster Controller and then it will assign roles to them. It is the one that will heart-beat all the processes. Then a Master is elected. The Master process is reponsible for the data distribution algorithms. Fun fact, the mapping between keys and storage servers is stored within FDB, which is you can actually move data by running transactions like any other application. He is also the one providing read versions and version number internally. He is also acting as the RateKeeper. The Proxies are responsible for providing read versions, committing transactions, and tracking the storage servers responsible for each range of keys. The Transaction Resolvers are responsible determining conflicts between transactions. A transaction conflicts if it reads a key that has been written between the transaction‚Äôs read version and commit version. The resolver does this by holding the last 5 seconds of committed writes in memory, and comparing a new transaction‚Äôs reads against this set of commits. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:4:1","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Read and Write Path Read Path Retrieve a consistend read version for the transaction Do reads from a consistent MVCC snapshot at that read version on the storage node Write Path client is sending a bundle to the proxy containing: read version for the transaction every readen key every mutation that you want to do The proxy will assign a Commit version to a batch of transactions. Commit version is generated by the Master Proxy is sending to the resolver. This will check if the data that you want to mutate has been changed between your read Version and your Commit version. They are sharded by key-range. Transaction is made durable within the Transaction Logs by fsyncing the data. Before the data is even written to disk it is forwarded to the storage servers responsible for that mutation. Internally, Transactions Logs are creating a stream per Storage Server. Once the storage servers have made the mutation durable, they pop it from the log. This generally happens roughly 6 seconds after the mutation was originally committed to the log. Storage servers are lazily updating data on disk from the Transaction logs. They are keeping new write in-memory. Transaction Logs is responding OK to the Proxy and then the proxy is replying OK to the client. You can find more diagrams about transactions here. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:4:2","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Recovery Recovery processes are detailled at around 25min. During failure of a process (Except storage servers), the systems will try to create a new generation, so new Master, proxies, resolvers and transactions logs. New master will get a read version from transactions logs, and commit with Paxos the fact that starting from Read version, the new generation is the one in charge. Storage servers are replicating data on failures. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:4:3","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"The 5-second transaction limit FoundationDB currently does not support transactions running for over five seconds. More details around 16min but the tl;dr is: Storage servers are caching latest read in-memory, Resolvers are caching the last 5 seconds transactions. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:4:4","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Ratekeeper More details around 31min but the tl;dr is that when system is saturated, retrieving the Read version is slowed down. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:4:5","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Storage A lot of information are available in this talk: memory is optimized for small databases. Data is stored in memory and logged to disk. In this storage engine, all data must be resident in memory at all times, and all reads are satisfied from memory. SSD Storage Engine is based on SQLite B-Tree Redwood will be a new storage engine based on Versioned B+Tree ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:4:6","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Developer experience FoundationDB‚Äôs keys are ordered, making tuples a particularly useful tool for data modeling. FoundationDB provides a tuple layer (available in each language binding) that encodes tuples into keys. This layer lets you store data using a tuple like (state, county) as a key. Later, you can perform reads using a prefix like (state,). The layer works by preserving the natural ordering of the tuples. Everything is wrapped into a transaction in FDB. You can have a nice overview by reading the README of tsdb-layer, an experiment combining Time Series and FoundationDB: Millions of writes/s and 10x compression in under 2,000 lines of Go. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:5:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"FDB One more things: Layers ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:6:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Concept of layers FDB is resolving many distributed problems, but you still need things like security, multi-tenancy, query optimizations, schema, indexing. Layers are designed to develop features above FDB. The record-layer provided by Apple is a good starting point to build things above it, as it provides structured schema, indexes, and (async) query planner. The record-layer provided by Apple is a good starting point to build things above it, as it provides structured schema, indexes, and (async) query planner. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:6:1","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Apple‚Äôs Record Layer The paper is located FoundationDB Record Layer:A Multi-Tenant Structured Datastore Record Layer was designed to solve CloudKit problem. Record allow multi-tenancy with schema above FDB Record Layers is providing stateless compute And streaming queries! ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:6:2","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Kubernetes Operators ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:7:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Overview of the operator Upgrade is done by bumping all processes at once üò± ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:7:1","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Combining chaos-mesh and the operator I played a bit with the operator by combining: FoundationDB/fdb-kubernetes-operator pingcap/go-ycsb pingcap/chaos-mesh PierreZ/fdb-prometheus-exporter The experiment is available here. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:7:2","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["FoundationDB","notesabout"],"content":"Roadmap FoundationDB Release 7.0 Planning Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2020-01-30","objectID":"/posts/notes-about-foundationdb/:8:0","tags":null,"title":"Notes about FoundationDB","uri":"/posts/notes-about-foundationdb/"},{"categories":["kafka","diving into"],"content":" Diving Into is a blogpost serie where we are digging a specific part of of the project‚Äôs basecode. In this episode, we will digg into Kafka‚Äôs protocol. ","date":"2019-12-08","objectID":"/posts/diving-into-kafka-protocol/:0:0","tags":null,"title":"Diving into Kafka's Protocol","uri":"/posts/diving-into-kafka-protocol/"},{"categories":["kafka","diving into"],"content":"The protocol reference For the last few months, I worked a lot around Kafka‚Äôs protocols, first by creating a fully async Kafka to Pulsar Proxy in Rust, and now by contributing directly to KoP (Kafka On Pulsar). The full Kafka Protocol documentation is available here, but it does not offer a global view of what is happening for a classic Producer and Consumer exchange. Let‚Äôs dive in! ","date":"2019-12-08","objectID":"/posts/diving-into-kafka-protocol/:1:0","tags":null,"title":"Diving into Kafka's Protocol","uri":"/posts/diving-into-kafka-protocol/"},{"categories":["kafka","diving into"],"content":"Common handshake After a client established the TCP connection, there is a few common requests and responses that are almost always here. The common handhake can be divided in three parts: Being able to understand each other. For this, we are using API_VERSIONS to know which versions of which TCP frames can be uses, Establish Auth using SASL if needed, thanks to SASL_HANDSHAKE and SASL_AUTHENTICATE, Retrieve the topology of the cluster using METADATA. All exchange are based between a Kafka 2.0 cluster and client. All the following diagrams are generated with MermaidJS. ","date":"2019-12-08","objectID":"/posts/diving-into-kafka-protocol/:1:1","tags":null,"title":"Diving into Kafka's Protocol","uri":"/posts/diving-into-kafka-protocol/"},{"categories":["kafka","diving into"],"content":"Producing The PRODUCE API is used to send message sets to the server. For efficiency it allows sending message sets intended for many topic partitions in a single request. ","date":"2019-12-08","objectID":"/posts/diving-into-kafka-protocol/:1:2","tags":null,"title":"Diving into Kafka's Protocol","uri":"/posts/diving-into-kafka-protocol/"},{"categories":["kafka","diving into"],"content":"Consuming Consuming is more complicated than producing. You can learn more in The Magical Group Coordination Protocol of Apache Kafka By Gwen Shapira, Principal Data Architect @ Confluent and also in the Kafka Client-side Assignment Proposal. Consuming can be divided in three parts: coordinating the consumers to assign them partitions, using: FIND_COORDINATOR, JOIN_GROUP, SYNC_GROUP, then fetch messages using: OFFSET_FETCH, LIST_OFFSETS, FETCH, OFFSET_COMMIT, Send lifeproof to the coordinator using HEARTBEAT. For the sake of the explanation, we have now another Broker1 which is holding the coordinator for topic ‚Äòmy-topic‚Äô. In real-life, it would be the same. Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2019-12-08","objectID":"/posts/diving-into-kafka-protocol/:1:3","tags":null,"title":"Diving into Kafka's Protocol","uri":"/posts/diving-into-kafka-protocol/"},{"categories":["hbase","diving into"],"content":" Diving Into is a blogpost serie where we are digging a specific part of of the project‚Äôs basecode. In this episode, we will digg into the implementation behind Hbase‚Äôs MemStore. tl;dr: Hbase is using the ConcurrentSkipListMap. ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:0:0","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":["hbase","diving into"],"content":"What is the MemStore? The memtable from the official BigTable paper is the equivalent of the MemStore in Hbase. As rows are sorted lexicographically in Hbase, when data comes in, you need to have some kind of a in-memory buffer to order those keys. This is where the MemStore comes in. It absorbs the recent write (or put in Hbase semantics) operations. All the rest are immutable files called HFile stored in HDFS. There is one MemStore per column family. Let‚Äôs dig into how the MemStore internally works in Hbase 1.X. ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:1:0","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":["hbase","diving into"],"content":"Hbase 1 All extract of code for this section are taken from rel/1.4.9 tag. ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:2:0","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":["hbase","diving into"],"content":"in-memory storage The MemStore interface is giving us insight on how it is working internally. /** * Write an update * @param cell * @return approximate size of the passed cell. */ long add(final Cell cell); ‚Äì add function on the MemStore The implementation is hold by DefaultMemStore. add is wrapped by several functions, but in the end, we are arriving here: private boolean addToCellSet(Cell e) { boolean b = this.activeSection.getCellSkipListSet().add(e); ‚Äì addToCellSet on the DefaultMemStore CellSkipListSet class is built on top of ConcurrentSkipListMap, which provide nice features: concurrency sorted elements ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:2:1","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":["hbase","diving into"],"content":"Flush on HDFS As we seen above, the MemStore is supporting all the puts. When asked to flush, the current memstore is moved to snapshot and is cleared. Flushed file are called (HFiles) and they are similar to SSTables introduced by the official BigTable paper. HFiles are flushed on the Hadoop Distributed File System called HDFS. If you want deeper insight about SSTables, I recommend reading Table Format from the awesome RocksDB wiki ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:2:2","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":["hbase","diving into"],"content":"Compaction Compaction are only run on HFiles. It means that if hot data is continuously updated, we are overusing memory due to duplicate entries per row per MemStore. Accordion tends to solve this problem through in-memory compactions. Let‚Äôs have a look to Hbase 2.X! ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:2:3","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":["hbase","diving into"],"content":"Hbase 2 ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:3:0","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":["hbase","diving into"],"content":"storing data All extract of code starting from here are taken from rel/2.1.2 tag. Does MemStore interface changed? /** * Write an update * @param cell * @param memstoreSizing The delta in memstore size will be passed back via this. * This will include both data size and heap overhead delta. */ void add(final Cell cell, MemStoreSizing memstoreSizing); ‚Äì add function in MemStore interface The signature changed a bit, to include passing a object instead of returning a long. Moving on. The new structure implementing MemStore is called AbstractMemStore. Again, we have some layers, where AbstractMemStore is writing to a MutableSegment, which itsef is wrapping Segment. If you dig far enough, you will find that data are stored into the CellSet class which is also things built on top of ConcurrentSkipListMap! ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:3:1","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":["hbase","diving into"],"content":"in-memory Compactions Hbase 2.0 introduces a big change to the original memstore called Accordion which is a codename for in-memory compactions. An awesome blogpost is available here: Accordion: HBase Breathes with In-Memory Compaction and the document design is also available. Thank you for reading my post! feel free to react to this article, I‚Äôm also available on Twitter if needed. ","date":"2019-11-17","objectID":"/posts/diving-into-hbase-memstore/:3:2","tags":null,"title":"Diving into Hbase's MemStore","uri":"/posts/diving-into-hbase-memstore/"},{"categories":null,"content":"Abstract Apache Bookkeeper is the main datastore used by Apache Pulsar. This Top-Level Apache project is defined as ¬´ A scalable, fault-tolerant, and low-latency storage service optimized for real-time workloads ¬ª. BookKeeper is suitable for a wide variety of use cases, from WAL to storage. In this talk, we will dive into Bookkeeper‚Äôs concepts, the public API, deployments and how it is used within Pulsar. Occurences HUG France: Special Apache Pulsar Meetup Ressources slides Photos and tweets Si vous cherchez √† organiser les MeetUP, nos bureaux √† Paris, Nantes, Rennes, Lyon, Bordeaux, Brest, Wroclaw, Madrid, Milan, London, Montreal, Dallas, Reston sont dispo gratos. Faites nous signe. #OVHcloud pic.twitter.com/TCVjHDyw1V ‚Äî Octave Klaba (@olesovhcom) October 11, 2019 It was a great time to attend the first @apache_pulsar meetup in Paris. Nice Pulsar and @asfbookkeeper 101 by @GwinizDu @waxzce @PierreZ and learned from @KannarFr about how they use Pulsar üëçüëç Merci √† tous pic.twitter.com/2yllfGqsnb ‚Äî Sijie (@sijieg) October 11, 2019 C'est maintenant au tour de @PierreZ d'introduire @asfbookkeeper üòä pic.twitter.com/DM7ZHkcFsL ‚Äî Aur√©lien H√©bert (@AurrelH95) October 11, 2019 #hugFr @PierreZ pr√©sente Bookkeper pic.twitter.com/GrDsUTKLfr ‚Äî Charly CLAIRMONT (@egwada) October 11, 2019 ","date":"2019-10-08","objectID":"/talks/bookkeeper-101/:0:0","tags":null,"title":"Bookkeeper 101","uri":"/talks/bookkeeper-101/"},{"categories":["distributed-systems","hadoop","colossus"],"content":" In the last few months, there has been numerous blogposts about the end of the Hadoop-era. It is true that: Health of Hadoop-based companies are publicly bad Hadoop has a bad publicity with headlines like ‚ÄòWhat does the death of Hadoop mean for big data?‚Äô Hadoop, as a distributed-system, is hard to operate, but can be essential for some type of workload. As Hadoop is based on GFS, we can wonder how GFS evolved inside Google. ","date":"2019-08-04","objectID":"/posts/colossus-google/:0:0","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hadoop","colossus"],"content":"Hadoop‚Äôs story Hadoop is based on a Google‚Äôs paper called The Google File System published in 2003. There are some key-elements on this paper: It was designed to be deployed with Borg, to ‚Äúsimplify the overall design problem‚Äù, they: implemented a single master architecture dropped the idea of a full POSIX-compliant file system Metadatas are stored in RAM in the master, Datas are stored within chunkservers, There is no YARN or Map/Reduce or any kind of compute capabilities. ","date":"2019-08-04","objectID":"/posts/colossus-google/:1:0","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hadoop","colossus"],"content":"Is Hadoop still revelant? Google with GFS and the rest of the world with Hadoop hit some issues: One (Metadata) machine is not large enough for large FS, Single bottleneck for metadata operations, Not appropriate for latency sensitive applications, Fault tolerant not HA, Unpredictable performance, Replication‚Äôs cost, HDFS Write-path pipelining, fixed-size of blocks, cost of operations, ‚Ä¶ Despite all the issues, Hadoop is still relevant for some usecases, such as Map/Reduce, or if you need Hbase as a main datastore. There is stories available online about the scalability of Hadoop: Twitter has multiple clusters storing over 500 PB (2017) whereas Google prefered to ‚ÄúScaled to approximately 50M files, 10P‚Äù to avoid ‚Äúadded management overhead‚Äù brought by the scaling. Nowadays, Hadoop is mostly used for Business Intelligence or to create a datalake, but at first, GFS was designed to provide a distributed file-system on top of commodity servers. Google‚Äôs developers were/are deploying applications into ‚Äúcontainers‚Äù, meaning that any process could be spawned somewhere into the cloud. Developers are used to work with the file-system abstraction, which provide a layer of durability and security. To mimic that process, they developed GFS, so that processes don‚Äôt need to worry about replication (like Bigtable/HBase). This is a promise that, I think, was forgotten. In a world where Kubernetes seems to be the standard, the need of a global distributed file-system is now higher than before. By providing a ‚Äúfile-system‚Äù abstraction for applications deployed in Kubernetes, we may be solving many problems Kubernetes-adopters are hitting, such as: How can I retrieve that particular file for my applications deployed on the other side of the Kubernetes cluster? Should I be moving that persistent volume over my slow network? What is happening when Kubernetes killed an alpha pod in the middle of retrieving snapshot? ","date":"2019-08-04","objectID":"/posts/colossus-google/:2:0","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hadoop","colossus"],"content":"Well, let‚Äôs put Hadoop in Kubernetes Putting a distributed systems inside Kubernetes is currently a unpleasant experience because of the current tooling: Helm is not helping me expressing my needs as a distributed-system operator. Even worse, the official Helm chart for Hadoop is limited to YARN and Map/Reduce and ‚ÄúData should be read from cloud based datastores such as Google Cloud Storage, S3 or Swift.‚Äù Kubernetes Operators has no access to key-metrics, so they cannot watch over your applications correctly. It is only providing a ‚Äúday-zero to day-two‚Äù good experience, Google seems to not be using the Operators design internally. CouchDB developers are saying that: ‚ÄúFor certain workloads, the technology isn‚Äôt quite there yet‚Äù ‚ÄúIn certain scenarios that are getting smaller and smaller, both Kubernetes and Docker get in the way of that. At that point, CouchDB gets slow, or you get timeout errors, that you can‚Äôt explain.‚Äù ","date":"2019-08-04","objectID":"/posts/colossus-google/:3:0","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hadoop","colossus"],"content":"How GFS evolved within Google ","date":"2019-08-04","objectID":"/posts/colossus-google/:4:0","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hadoop","colossus"],"content":"Technical overview As GFS‚Äôs paper was published in 2003, we can ask ourselves if GFS has evolved. And it did! The sad part is that there is only a few informations about this project codenamed Colossus. There is no papers, and not a lot informations available, here‚Äôs what can be found online: From Storage Architecture and Challenges(2010): They moved from full-replication to Reed-Salomon. This feature is acually in Hadoop 3, replication is handled by the client, instead of the pipelining, the metadata layer is automatically sharded. We can find more informations about that in the next ressource! From Cluster-Level Storage @ Google(2017): GFS master replaced by Colossus GFS chunkserver replaced by D Colossus rebalances old, cold data distributes newly written data evenly across disks Metadatas are stored into BigTable. each Bigtable row corresponds to a single file. The ‚Äúall in RAM‚Äù GFS master design was a severe single-point-of-failure, so getting rid of it was a priority. They didn‚Äôt had a lof of options for a scalable and rock-solid datastore beside BigTable. When you think about it, a key/value datastore is a great replacement for a distributed file-system master: automatic sharding of regions, scan capabilities for files in the same ‚Äúdirectory‚Äù, lexical ordering, ‚Ä¶ The funny part is that they now need a Colossus for Colossus. The only things saving them is that storing the metametametadata (the metadata of the metadata of the metadata) can be hold in Chubby. From GFS: Evolution on Fast-forward(2009) they moved to chunks of 1MB of files, as the limitations of the master disappeared. This is also allowing Colossus to support latency sensitive applications, From a Github comment on Colossus: File reconstruction from Reed-Salomnon was performed on both client-side and server-side on-the-fly recovery of data is greatly enhanced by this data layout(Reed Salomon) From a Hacker News comment: Colossus and D are two separate things. From Colossus under the hood: a peek into Google‚Äôs scalable storage system: Colossus‚Äôs Control Plane is a scalable metadata service, which consists of many Curators. Clients talk directly to curators for control operations, such as file creation, and can scale horizontally. background storage managers called Custodians, there are handling tasks like disk space balancing and RAID reconstruction. Applications needs to specifies I/O, availability, and durability requirements What is that ‚ÄúD‚Äù? From The Production Environment at Google, from the Viewpoint of an SRE: D stands for Disk, D is a fileserver running on almost all machines in a cluster. From The Production Environment at Google: D is more of a block server than a file server It provides nothing apart from checksums. ","date":"2019-08-04","objectID":"/posts/colossus-google/:4:1","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hadoop","colossus"],"content":"Deployments How everything is deployed? Using Borg! ","date":"2019-08-04","objectID":"/posts/colossus-google/:4:2","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hadoop","colossus"],"content":"Migration The migration process is described in the now free Case Studies in Infrastructure Change Management book. ","date":"2019-08-04","objectID":"/posts/colossus-google/:4:3","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hadoop","colossus"],"content":"Is there an open-source effort to create a Colossus-like DFS? I did not found any point towards a open-source version of Colossus, beside some work made for The Baidu File System in which the Nameserver is implemented as a raft group. There is some work to add colossus‚Äôs features in Hadoop but based on the bad publicity Hadoop has now, I don‚Äôt think there will be a lot of money to power those efforts. I do think that rewriting an distributed file-system based on Colossus would be a huge benefit for the community: Reimplement D may be easy, my current question is how far can we use modern FS such as OpenZFS to facilitate the work? FS capabilities such as OpenZFS checksums seems pretty interesting. To resolve the distributed master issue, we could use Tikv as a building block to provide an ‚ÄúBigTable experience‚Äù without the need of a distributed file-system underneath. But remember: Like crypto, Do not roll your own DFS! Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2019-08-04","objectID":"/posts/colossus-google/:5:0","tags":null,"title":"What can be gleaned about GFS successor codenamed Colossus?","uri":"/posts/colossus-google/"},{"categories":["distributed-systems","hbase"],"content":" Among all features provided by HBase, there is one that is pretty handy to deal with your data‚Äôs lifecyle: the fact that every cell version can have Time to Live or TTL. Let‚Äôs dive into the feature! Time To Live (TTL) Let‚Äôs read the doc first! ColumnFamilies can set a TTL length in seconds, and HBase will automatically delete rows once the expiration time is reached. HBase Book: Time To Live (TTL) Let‚Äôs play with it! You can easily start an standalone HBase by following the HBase Book. Once your standalone cluster is started, we can get started: ./bin/hbase shell hbase(main):001:0\u003e create 'test_table', {'NAME' =\u003e 'cf1','TTL' =\u003e 30} # 30 sec Now that our test_table is created, we can put some data on it: hbase(main):002:0\u003e put 'test_table','row123','cf1:desc', 'TTL Demo' And you can get it with: hbase(main):003:0\u003e get 'test_table','row123','cf1:desc' COLUMN CELL cf1:desc timestamp=1558366581134, value=TTL Demo 1 row(s) in 0.0080 seconds Here‚Äôs our row! But if you wait a bit, it will disappear thanks to the TTL: hbase(main):004:0\u003e get 'test_table','row123','cf1:desc' COLUMN CELL 0 row(s) in 0.0220 seconds It has been filtered from the result, but the data is still here. You can trigger a raw scan to check: hbase(main):002:0\u003e scan 'test_table', {RAW =\u003e true} ROW COLUMN+CELL row123 column=cf1:desc, timestamp=1558366581134, value=TTL Demo 1 row(s) in 0.3280 seconds It will be removed only when a major-compaction will occur. As we are playing, we can: force the memstore to be flushed as HFiles force the compaction: hbase(main):014:0\u003e flush 'test_table' Took 0.4456 seconds hbase(main):015:0\u003e compact 'test_table' Took 0.0468 seconds # wait a bit hbase(main):016:0\u003e scan 'test_table', {RAW =\u003e true} ROW COLUMN+CELL 0 row(s) Took 0.0060 seconds How does it works? As always, the truth is held by the documentation: A {row, column, version} tuple exactly specifies a cell in HBase. It‚Äôs possible to have an unbounded number of cells where the row and column are the same but the cell address differs only in its version dimension. While rows and column keys are expressed as bytes, the version is specified using a long integer. Typically this long contains time instances such as those returned by java.util.Date.getTime() or System.currentTimeMillis(), HBase Book: Versions You may have seen it during our scan earlier, there is a timestamp associated with the version of the cell: hbase(main):003:0\u003e get 'test_table','row123','cf1:desc' COLUMN CELL cf1:desc timestamp=1558366581134, value=TTL Demo # here ^^^^^^^^^^^^^^^^^^^^^^^ Hbase used the System.currentTimeMillis() at ingest time to add it. During scanner and compaction, as time went by, there was more than TTL seconds between the cell version and now, so the row was discarded. Now the real question is: can you set it by yourself and be real Time-Lord (of HBase)? The reponse is yes! There is also a bit of a warning a bit below: Caution: the version timestamp is used internally by HBase for things like time-to-live calculations. It‚Äôs usually best to avoid setting this timestamp yourself. Prefer using a separate timestamp attribute of the row, or have the timestamp as a part of the row key, or both. Let‚Äôs try it: date +%s -d \"+2 min\" 1558472441 # don't forget to add 3 zeroes as the time need to be in millisecond! ./bin/hbase shell hbase(main):001:0\u003e put 'test_table','row1234','cf1:desc', 'timestamp Demo', 1558472441000 hbase(main):044:0\u003e scan 'test_table' ROW COLUMN+CELL row1234 column=cf1:desc, timestamp=1558473315, value=timestamp Demo 1 row(s) Took 0.0031 seconds Notice that we are using a timestamp at the end of the put method? This will add the desired timestamp to the version. Which means that your application can control when your version will be removed, even with a TTL on your column-qualifier. You just need to compute a timestamp like this: ts = now - ttlCF + desiredTTL. Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2019-05-27","objectID":"/posts/ttl-hbase/:0:0","tags":null,"title":"Playing with TTL in HBase","uri":"/posts/ttl-hbase/"},{"categories":null,"content":"Abstract What to do when you must monitor the whole infrastructure of the biggest European hosting and cloud provider? How to choose a tool when the most used ones fail to scale to your needs? How to build an Metrics platform to unify, conciliate and replace years of fragmented legacy partial solutions? In this talk we will relate our experience building and maintaining OVH Metrics, the platform used to monitor all OVH infrastructure. We needed to go to places where most monitoring solutions hadn‚Äôt gone before, it needed to operate at the scale of the biggest European hosting and cloud providers: 27 data centers, more than 300k servers (bare metal!), and hundreds of products to fulfill our mission to host 1.3 million customers. You will hear about time series, about open source solutions pushed to the limit, about HBase clusters operated at the extreme, and how about a small team leveraged the power of a handful of open source solution and lots of coding glue to build one of the most performant monitoring solutions ever. Occurences RivieraDev, 2019 Ressources slides Photos and tweets @LostInBrittany et @PierreZ nous raconte leur ¬´¬†histoire sans fin¬†¬ª pour cr√©er la plateforme metrics üëèüèª @RivieraDEV pic.twitter.com/iVwKjJ2BnR ‚Äî Cecile (@CecileHbh) May 16, 2019 @PierreZ @LostInBrittany speaking about Wap 10 @ @RivieraDEV. Awesome presentation guys! pic.twitter.com/q0bdJ0m2mw ‚Äî Nikita Rousseau (@nirousseau) May 16, 2019 Ceci n'est pas du placement de produit üòú@SenXHQ @warp10io √† l'honneur üòé@PierreZ @LostInBrittany @SylvainLareyre Ok pour le c√¢lin üòä pic.twitter.com/M60jw7Wmyv ‚Äî JobOpportunIT (@JobOpportunIT_) May 16, 2019 Great talk @RivieraDEV from @LostInBrittany and @PierreZ on Monitoring @OVH #SenX pic.twitter.com/Voxwa4KyIf ‚Äî Tiffany Souterre (@TiffanySouterre) May 16, 2019 On part sur Monitoring OVH: 300k serveurs, 27 DCs une plateforme de m√©triques avec @LostInBrittany et @PierreZ pic.twitter.com/GB5CsLoz6m ‚Äî Riviera DEV üå¥ (@RivieraDEV) May 16, 2019 ","date":"2019-05-19","objectID":"/talks/one-monitoring-platform/:0:0","tags":null,"title":"Monitoring OVH: 300k servers, 27 DCs and one metrics platform","uri":"/talks/one-monitoring-platform/"},{"categories":null,"content":"Abstract OVH has 27 data centers, more than 300 000 servers and 1.3 million customers. We operate one of the world‚Äôs largest cloud infrastructures on a daily basis, running millions of applications. Those softwares manage millions of transactions every second. To monitor such an architecture, OVH has chosen to build a unified monitoring platform: OVH Metrics Data Platform. Such a platform uses complex distributed systems to operate, based mainly on open source tools running on the JVM (Apache Kafka, Apache HBase, Warp 10, ‚Ä¶). In order to improve developments on such systems, it is useful to have visualization tools to be able to observe the different behaviors of the JVM. This approach makes it possible, for the purpose of continuous improvement and in the interest of performance, to perfect the applications that we develop. This talk summarizes two years of experiments on the development and administration of applications using the JVM in a distributed world. Occurences Jax.de, 2019 Ressources Slides ","date":"2019-05-06","objectID":"/talks/handling-jvm-scale/:0:0","tags":null,"title":"Operate JVM at Scale","uri":"/talks/handling-jvm-scale/"},{"categories":null,"content":"Abstract OVH relies heavily on metrics to effectively monitor its entire infrastructure. Offering a low-level vision and business, these allow teams to better operate the daily operation of our services. After managing more than 300 TB of telemetry, we started working on an alerting solution over this huge datalake. For that, we decided to use Apache Flink to manage all these large scale alerts. Today, this project manages the alerting of flagship OVH products such as Public Cloud Instances and Kubernetes. This conference is a feedback that will present: What is Apache Flink? How to develop a Flink job from 0 Deploying and operating a Flink cluster Occurences FinistDevs, 2019 Devoxx France, 2019 Ressources slides Github Photos and tweets @ApacheFlink at scale at #ovh by @PierreZ . Really good presentation on high volume throughput üí™ ‚Äî Francois Teychene (@fteychene) April 18, 2019 All the ressources for my #DevoxxFR talk ¬´Handling alerts at OVH-Scale with Apache Flink¬´¬†are available on my website! https://t.co/0E1RxvwsRd pic.twitter.com/HkIkgJv8LT ‚Äî Pierre Zemb (@PierreZ) April 18, 2019 Last slides check! I‚Äôll be speaking at #devoxxFR about how #OVH is handling alerts with @ApacheFlink at 2:30pm ! pic.twitter.com/NLOdxbahCl ‚Äî Pierre Zemb (@PierreZ) April 18, 2019 Apache Flink expliquait par @PierreZ √† @DevoxxFR √ßa va d√©chirer... GRAVE üí™üëè pic.twitter.com/8MBLQerqEK ‚Äî Estelle Landry ‚òÄÔ∏è (@estelandry) April 18, 2019 ","date":"2019-04-18","objectID":"/talks/ovh-alerts-flink/:0:0","tags":null,"title":"Handling alerts at OVH-Scale with Apache Flink","uri":"/talks/ovh-alerts-flink/"},{"categories":null,"content":"Abstract Have you been wondered what is like to be working at the largest European Cloud Provider as a DevOps ? Despite both working on differents teams and products, we have a lot in common, such as good practices, organizations, technologies and even our good mood ! Time to lift the curtain! We will share with you perspectives from two DevOps: one working on OVH managed Kubernetes product and one working on Metrics, OVH distributed monitoring platform. Are you ready to talk with us about this technical and organisational context ? We, more than ever ! Occurences Incontro DevOps Italia, 2019 Ressources slides Photos and tweets It's now @PierreZ and @joellecorre time! They are going to share some feedbacks as @OVH softwares and systems engineers @IncontroDevOps ü§óü§óü§ó #IDI2019 pic.twitter.com/g5MWcIlZn5 ‚Äî Aur√©lien H√©bert (@AurrelH95) March 8, 2019 . @joellecorre \u0026 @PierreZ speaking at #IDI2019 about \"What we learned as DevOps at OVH\" - @OVH @OVHcloud #devops #cloud #ovh pic.twitter.com/I5hbc2bKIB ‚Äî Alessandro Polidori (@ale_polidori) March 8, 2019 ","date":"2019-03-06","objectID":"/talks/devops-ovh/:0:0","tags":null,"title":"What we learned as DevOps @OVH","uri":"/talks/devops-ovh/"},{"categories":["distributed-systems","flink"],"content":"This is a repost from OVH‚Äôs official blogpost.. Thanks Horacio Gonzalez for the awesome drawings! Handling OVH‚Äôs alerts with Apache Flink OVH relies extensively on metrics to effectively monitor its entire stack. Whether they are low-level or business centric, they allow teams to gain insight into how our services are operating on a daily basis. The need to store millions of datapoints per second has produced the need to create a dedicated team to build a operate a product to handle that load: **Metrics Data Platform.By relying on **Apache Hbase, Apache Kafka and Warp 10, we succeeded in creating a fully distributed platform that is handling all our metrics‚Ä¶ and yours! After building the platform to deal with all those metrics, our next challenge was to build one of the most needed feature for Metrics: the Alerting. ","date":"2019-02-03","objectID":"/posts/ovh-alerts-flink/:0:0","tags":null,"title":"Handling OVH's alerts with Apache Flink","uri":"/posts/ovh-alerts-flink/"},{"categories":["distributed-systems","flink"],"content":"Meet OMNI, our alerting layer OMNI is our code name for a fully distributed, as-code, alerting system that we developed on top of Metrics. It is split into components: The management part, taking your alerts definitions defined in a Git repository, and represent them as continuous queries, The query executor, scheduling your queries in a distributed way. The query executor is pushing the query results into Kafka, ready to be handled! We now need to perform all the tasks that an alerting system does: Handling alerts deduplication and grouping, to avoid alert fatigue. Handling escalation steps, acknowledgementor snooze. Notify the end user, through differents channels: SMS, mail, Push notifications, ‚Ä¶ To handle that, we looked at open-source projects, such as Prometheus AlertManager, LinkedIn Iris, we discovered the hidden truth: Handling alerts as streams of data, moving from operators to another. We embraced it, and decided to leverage Apache Flink to create Beacon. In the next section we are going to describe the architecture of Beacon, and how we built and operate it. If you want some more information on Apache Flink, we suggest to read the introduction article on the official website: What is Apache Flink? ","date":"2019-02-03","objectID":"/posts/ovh-alerts-flink/:1:0","tags":null,"title":"Handling OVH's alerts with Apache Flink","uri":"/posts/ovh-alerts-flink/"},{"categories":["distributed-systems","flink"],"content":"Beacon architecture At his core, Beacon is reading events from Kafka. Everything is represented as a message, from alerts to aggregations rules, snooze orders and so on. The pipeline is divided into two branches: One that is running the aggregations, and triggering notifications based on customer‚Äôs rules. One that is handling the escalation steps. Then everything is merged to generate a notification, that is going to be forward to the right person. A notification message is pushed into Kafka, that will be consumed by another component called beacon-notifier. architecture ","date":"2019-02-03","objectID":"/posts/ovh-alerts-flink/:2:0","tags":null,"title":"Handling OVH's alerts with Apache Flink","uri":"/posts/ovh-alerts-flink/"},{"categories":["distributed-systems","flink"],"content":"Handling States If you are new to streaming architecture, I recommend reading Dataflow Programming Model from Flink official documentation. beacon architecture Everything is merged into a dataStream, partitionned (keyed byin Flink API) by users. Here‚Äôs an example: final DataStream\u003e alertStream = // Partitioning Stream per AlertIdentifier cleanedAlertsStream.keyBy(0) // Applying a Map Operation which is setting since when an alert is triggered .map(new SetSinceOnSelector()) .name(\"setting-since-on-selector\").uid(\"setting-since-on-selector\") // Partitioning again Stream per AlertIdentifier .keyBy(0) // Applying another Map Operation which is setting State and Trend .map(new SetStateAndTrend()) .name(\"setting-state\").uid(\"setting-state\"); In the example above, we are chaining two keyed operations: SetSinceOnSelector, which is setting since when the alert is triggered SetStateAndTrend, which is setting the state(ONGOING, RECOVERY or OK) and the trend(do we have more or less metrics in errors). Each of this class is under 120 lines of codes because Flink is handling all the difficulties. Most of the pipeline are only composed of classic transformations such as Map, FlatMap, Reduce, including their Rich and Keyed version. We have a few Process Functions, which are very handy to develop, for example, the escalation timer. ","date":"2019-02-03","objectID":"/posts/ovh-alerts-flink/:3:0","tags":null,"title":"Handling OVH's alerts with Apache Flink","uri":"/posts/ovh-alerts-flink/"},{"categories":["distributed-systems","flink"],"content":"Integration tests As the number of classes was growing, we needed to test our pipeline. Because it is only wired to Kafka, we wrapped consumer and producer to create what we call **scenari:**a series of integration tests running different scenarios. ","date":"2019-02-03","objectID":"/posts/ovh-alerts-flink/:4:0","tags":null,"title":"Handling OVH's alerts with Apache Flink","uri":"/posts/ovh-alerts-flink/"},{"categories":["distributed-systems","flink"],"content":"Queryable state One killer feature of Apache Flink is the capabilities of **querying the internal state of an operator**. Even if it is a beta feature, it allows us the get the current state of the different parts of the job: at which escalation steps are we on is it snoozed or ack-ed Which alert is ongoing and so on. Queryable state overview Thanks to this, we easily developed an API over the queryable state, that is powering our alerting view in Metrics Studio, our codename for the Web UI of the Metrics Data Platform. ","date":"2019-02-03","objectID":"/posts/ovh-alerts-flink/:5:0","tags":null,"title":"Handling OVH's alerts with Apache Flink","uri":"/posts/ovh-alerts-flink/"},{"categories":["distributed-systems","flink"],"content":"Apache Flink deployment We deployed the latest version of Flink (1.7.1 at the time of writing) directly on bare metal servers with a dedicated Zookeeper‚Äôs cluster using Ansible. Operating Flink has been a really nice surprise for us, with clear documentation and configuration, and an impressive resilience. We are capable of rebooting the whole Flink cluster, and the job is restarting at his last saved state, like nothing happened. We are using RockDB as a state backend, backed by OpenStack Swift storageprovided by OVH Public Cloud. For monitoring, we are relying on Prometheus Exporter with Beamium to gain observability over job‚Äôs health. ","date":"2019-02-03","objectID":"/posts/ovh-alerts-flink/:5:1","tags":null,"title":"Handling OVH's alerts with Apache Flink","uri":"/posts/ovh-alerts-flink/"},{"categories":["distributed-systems","flink"],"content":"In short, we love Apache Flink If you are used to work with stream related software, you may have realized that we did not used any rocket science or tricks. We may be relying on basics streaming features offered by Apache Flink, but they allowed us to tackle many business and scalability problems with ease. As such, we highly recommend that any developers should have a look to Apache Flink. I encourage you to go through Apache Flink Training, written by Data Artisans. Furthermore, the community has put a lot of effort to easily deploy Apache Flink to Kubernetes, so you can easily try Flink using our Managed Kubernetes! ","date":"2019-02-03","objectID":"/posts/ovh-alerts-flink/:5:2","tags":null,"title":"Handling OVH's alerts with Apache Flink","uri":"/posts/ovh-alerts-flink/"},{"categories":["transaction","sql"],"content":"Transaction? \"Programming should be about transforming data\" ‚Äî Programming Elixir 1.3 by Dave Thomas As developers, we are interacting oftenly with data, whenever handling it from an API or a messaging consumer. To store it, we started to create softwares called relational database management system or RDBMS. Thanks to them, we, as developers, can develop applications pretty easily, without the need to implement our own storage solution. Interacting with mySQL or PostgreSQL have now become a commodity. Handling a database is not that easy though, because anything can happen, from failures to concurrency isssues: How can we interact with datastores that can fail? What is happening if two users are updating a value at the same time? As a database user, we are using transactions to answer these questions. As a developer, a transaction is a single unit of logic or work, sometimes made up of multiple operations. It is mainly an abstraction that we are using to hide underlying problems, such as concurrency or hardware faults. ACID appears in a paper published in 1983 called ‚ÄúPrinciples of transaction-oriented database recovery‚Äù written by Theo Haerder and Andreas Reuter. This paper introduce a terminology of properties for a transaction: Atomic, Consistency, Isolation, Durability ","date":"2019-02-03","objectID":"/posts/acid-transactions/:0:0","tags":null,"title":"What are ACID transactions?","uri":"/posts/acid-transactions/"},{"categories":["transaction","sql"],"content":"Atomic Atomic, as you may have guessed, atomic represents something that cannot be splitted. In the database transaction world, it means for example that if a transaction with several writes is started and failed at some point, none of the write will be committed. As stated by many, the word atomic could be reword as abortability. ","date":"2019-02-03","objectID":"/posts/acid-transactions/:1:0","tags":null,"title":"What are ACID transactions?","uri":"/posts/acid-transactions/"},{"categories":["transaction","sql"],"content":"Consistency You will hear about consistency a lot of this serie. Unfortunately, this word can be used in a lot of context. In the ACID definition, it refers to the fact that a transaction will bring the database from one valid state to another. ","date":"2019-02-03","objectID":"/posts/acid-transactions/:2:0","tags":null,"title":"What are ACID transactions?","uri":"/posts/acid-transactions/"},{"categories":["transaction","sql"],"content":"Isolation Think back to your database. Were you the only user on it? I don‚Äôt think so. Maybe they were concurrent transactions at the same time, beside yours. Isolation while keeping good performance is the most difficult item on the list. There‚Äôs a lot of litterature and papers about it, and we will only scratch the surface. There is different transaction isolation levels, depending on the number of guarantees provided. ","date":"2019-02-03","objectID":"/posts/acid-transactions/:3:0","tags":null,"title":"What are ACID transactions?","uri":"/posts/acid-transactions/"},{"categories":["transaction","sql"],"content":"Isolation by the theory The SQL standard defines four isolation levels: Serializable, Repeatable Read, Read Commited and Read Uncommited. The strongest isolation is Serializable where transaction are not runned in parallel. As you may have guessed, it is also the slowest. Weaker isolation level are trading speed against anomalies that can be sum-up like this: Isolation level dirty reads Non-repeatable reads Phantom reads Performance Serializable üòé üòé üòé üëç Repeatable Read üòé üòé üò± üëçüëç Read Commited üòé üò± üò± üëçüëçüëç Read uncommited üò± üò± üò± üëçüëçüëçüëç I encourage you to click on all the links within the table to see everything that could go wrong in a weak database! ","date":"2019-02-03","objectID":"/posts/acid-transactions/:3:1","tags":null,"title":"What are ACID transactions?","uri":"/posts/acid-transactions/"},{"categories":["transaction","sql"],"content":"Isolation in Real Databases Now that we saw some theory, let‚Äôs have a look on a particular well-known database: PostgreSQL. What kind of isolation PostgreSQL is offering? PostgreSQL provides a rich set of tools for developers to manage concurrent access to data. Internally, data consistency is maintained by using a multiversion model (Multiversion Concurrency Control, MVCC). ‚Äî Concurrency Control introduction Wait what? What is MVCC? Well, turns out that after the SQL standards came another type of Isolation: Snapshot Isolation. Instead of locking that row for reading when somebody starts working on it, it ensures that any transaction will see a version of the data that is corresponding to the start of the query. As it is providing a good balance between performance and consistency, it became a standard used by the industry. ","date":"2019-02-03","objectID":"/posts/acid-transactions/:3:2","tags":null,"title":"What are ACID transactions?","uri":"/posts/acid-transactions/"},{"categories":["transaction","sql"],"content":"Durability Durability ensure that your database is a safe place where data can be stored without fear of losing it. If a transaction has commited successfully, any written data will not be forgotten. That‚Äôs it? All these properties may seems obvious to you but each of the item is involving a lot of engineering and researchs. And this is only valid for a single machine, the distributed transaction field is even more complicated, but we will get to it in another blogpost! Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2019-02-03","objectID":"/posts/acid-transactions/:4:0","tags":null,"title":"What are ACID transactions?","uri":"/posts/acid-transactions/"},{"categories":["distributed-systems","hbase"],"content":"HBase? Apache HBase‚Ñ¢ is a type of ‚ÄúNoSQL‚Äù database. ‚ÄúNoSQL‚Äù is a general term meaning that the database isn‚Äôt an RDBMS which supports SQL as its primary access language. Technically speaking, HBase is really more a ‚ÄúData Store‚Äù than ‚ÄúData Base‚Äù because it lacks many of the features you find in an RDBMS, such as typed columns, secondary indexes, triggers, and advanced query languages, etc. ‚Äì Hbase architecture overview ","date":"2019-01-27","objectID":"/posts/hbase-data-model/:1:0","tags":null,"title":"Hbase Data Model","uri":"/posts/hbase-data-model/"},{"categories":["distributed-systems","hbase"],"content":"Hbase data model The data model is simple: it‚Äôs like a multi-dimensional map: Elements are stored as rows in a table. Each table has only one index, the row key. There are no secondary indices. Rows are sorted lexicographically by row key. A range of rows is called a region. It is similar to a shard. A row in HBase consists of a row key and one or more columns, which are holding the cells. Values are stored into what we call a cell and are versioned with a timestamp. A column is divided between a Column Family and a Column Qualifier. Long story short, a Column Family is kind of like a column in classic SQL, and a qualifier is a sub-structure inside a Colum family. A column Family is static, you need to create it during table creation, whereas Column Qualifiers can be created on the fly. Not as easy as you thought? Here‚Äôs an example! Let‚Äôs say that we‚Äôre trying to save the whole internet. To do this, we need to store the content of each pages, and versioned it. We can use the page address as the row key and store the contents in a column called ‚ÄúContents‚Äù. Nowadays, website contents can be anything, from a HTML file to a binary such as a PDF. To handle that, we can create as many qualifiers as we want, such as ‚Äúcontent:html‚Äù or ‚Äúcontent:video‚Äù. { \"fr.pierrezemb.www\": { // Row key \"contents\": { // Column family \"content:html\": { // Column qualifier \"2017-01-01\": // A timestamp \"\u003chtml\u003e...\", // The actual value \"2016-01-01\": // Another timestamp \"\u003chtml\u003e...\" // Another cell }, \"content:pdf\": { // Another Column qualifier \"2015-01-01\": \"\u003cpdf\u003e...\" // my website may only contained a pdf in 2015 } } } } ","date":"2019-01-27","objectID":"/posts/hbase-data-model/:2:0","tags":null,"title":"Hbase Data Model","uri":"/posts/hbase-data-model/"},{"categories":["distributed-systems","hbase"],"content":"Key design Hbase is most efficient at queries when we‚Äôre getting a single row key, or during row range, ie. getting a block of contiguous data because keys are sorted lexicographically by row key. For example, my website fr.pierrezemb.www and org.pierrezemb.www would not be ‚Äúnear‚Äù. As such, the key design is really important: If your data are too spread, you will have poor performance. If your data are too much collocate, you will also have poor performance. As stated by the official documentation: Hotspotting occurs when a large amount of client traffic is directed at one node, or only a few nodes, of a cluster. This traffic may represent reads, writes, or other operations. The traffic overwhelms the single machine responsible for hosting that region, causing performance degradation and potentially leading to region unavailability. As you may have guessed, this is why we are using the reverse address name in my example, because www is too generic, we would have hotspot the poor region holding data for www. If you are curious about Hbase schema, you should have a look on Designing Your BigTable Schema, as BigTable is kind of the proprietary version of Hbase. ","date":"2019-01-27","objectID":"/posts/hbase-data-model/:3:0","tags":null,"title":"Hbase Data Model","uri":"/posts/hbase-data-model/"},{"categories":["distributed-systems","hbase"],"content":"Be warned I have been working with Hbase for the past three years, including operation and on-call duty. It is a really nice data store, but it diverges from classical RDBMS. Here‚Äôs some warnings extracted from the well-written documentation: HBase is really more a ‚ÄúData Store‚Äù than ‚ÄúData Base‚Äù because it lacks many of the features you find in an RDBMS, such as typed columns, secondary indexes, triggers, and advanced query languages, etc. However, HBase has many features which supports both linear and modular scaling. ‚Äì NoSQL? If you have hundreds of millions or billions of rows, then HBase is a good candidate. If you only have a few thousand/million rows, then using a traditional RDBMS might be a better choice due to the fact that all of your data might wind up on a single node (or two) and the rest of the cluster may be sitting idle. ‚Äì When Should I Use HBase? Thank you for reading my post! Feel free to react to this article, I am also available on Twitter if needed. ","date":"2019-01-27","objectID":"/posts/hbase-data-model/:4:0","tags":null,"title":"Hbase Data Model","uri":"/posts/hbase-data-model/"},{"categories":null,"content":"Abstract For many years humanity has been exploring the skies, dreaming of interstellar voyages and new planetary colonies. And you, do you want to go 3h with us to discover the universe? It turns out that NASA has a great public dataset, especially one that is used to search for exoplanets, that is, planets outside our solar system. During this Hands-on, we will guide you through the different stages of rediscovering exoplanets using Warp10, an open-source time series processing platform. Occurences BreizhCamp, 2018 Devoxx France, 2018 RivieraDev, 2018 DevFest Lille, 2018 Sunny Tech, 2018 Jug Summer Camp, 2018 DevFest Nantes, 2018 BDX.io, 2018 Ressources The hands-on can be done online, and the slides are available here. Photos and tweets Packed room for our @HelloExoWorld lab at @SunnyTech_MTP pic.twitter.com/piW8Ht22mz ‚Äî Horacio Gonz√°lez üá™üá∫ ü•ë (@LostInBrittany) June 28, 2018 Salle pleine pour notre lab @HelloExoWorld √† @DevoxxFR. Cherchons des exoplan√®tes avec @warp10io et le dataset @NASAKepler √† #devoxxfr ! pic.twitter.com/IXvZDTH8Cr ‚Äî Horacio Gonz√°lez üá™üá∫ ü•ë (@LostInBrittany) April 18, 2018 First hands-on from our team @breizhcamp using @NASAKepler dataset. People loved it! Thank you for coming ‚ù§Ô∏è pic.twitter.com/WXe5K4ahqO ‚Äî HelloExoWorld (@HelloExoWorld) March 28, 2018 #RivieraDEV c'est parti pour l'exploration de nouvelles plan√®tes avec @HelloExoWorld pic.twitter.com/eEKlwiR72f ‚Äî Karim Senhaji (@kimsnj) May 16, 2018 C'est parti pour le lab de @helloExoWorld avec @LostInBrittany, @PierreZ et @AurelienMv au @breizhcamp ! #BreizhCamp pic.twitter.com/3fsFtAip4u ‚Äî Brendan Abolivier (@brendan@abolivier.bzh) (@BrenAbolivier) March 28, 2018 #helloexoworld pr√©sent√© par @LostInBrittany et @0xd33d33 au @web2day #Web2day en #sketchnote pic.twitter.com/NxYhbU6YO7 ‚Äî Pierre TIBULLE (@ptibulle) June 14, 2018 Et c'est le tour de @PierreZ @OvhMetrics de pr√©senter @HelloExoWorld au @montpellierjug. Time Series + @warp10io + @NASAKepler dataset pour la recherche d'exoplan√®tes pic.twitter.com/PUEXYnMiGV ‚Äî Horacio Gonz√°lez üá™üá∫ ü•ë (@LostInBrittany) March 21, 2018 ","date":"2018-12-15","objectID":"/talks/nasa-datasets/:0:0","tags":null,"title":"Rediscover the known data universe with NASA dataset","uri":"/talks/nasa-datasets/"},{"categories":null,"content":"I‚Äôm a Engineering Manager building data infrastructure at Clever Cloud. I‚Äôve built and operated a variety of stateful distributed systems throughout my career. I‚Äôm interested in distributed systems, data stores, understanding how things works under the hood. I enjoy being part of open-source communities, through talks, posts, and contributions to: SenX Warp10, Apache HBase, Flink, Pulsar, arrow-datafusion, PingCAP go-ycsb, Apple FoundationDB and their Kubernetes Operator, StreamNative KoP (Kafka On Pulsar), Kafka protocol handler for Pulsar, CNCF ETCD, I am also maintaining some projects such as: PierreZ/goStatic, A really small static web server for Docker, PierreZ/fdb-etcd, An experiment to provide ETCD layer on top of Record-Layer and FoundationDB, PierreZ/Record-Store, A light, multi-model, user-defined place for your data. FoundationDB-rs, The rust binding for FoundationDB. On my free time, I am giving a hand to local events, such as the local GDG/JUG FinistDevs, Devoxx4Kids and I am also a teaching assistant in my former Engineer School. I co-founded in 2017 HelloExoWorld, an initiative to search for exoplanets using Warp10, a time-series platform. ","date":"2018-12-15","objectID":"/about/:0:0","tags":null,"title":"Hello üëã","uri":"/about/"},{"categories":null,"content":"Work ","date":"2018-12-15","objectID":"/about/:1:0","tags":null,"title":"Hello üëã","uri":"/about/"},{"categories":null,"content":"2023 to now: Engineering Manager @Clever Cloud Managing all data-related teams, from DBaaS to internal metrics Technical leadership around serverless databases Technical advisor around distributed systems and data-related systems On-call duty ","date":"2018-12-15","objectID":"/about/:1:1","tags":null,"title":"Hello üëã","uri":"/about/"},{"categories":null,"content":"2021 to 2023: Senior Software Engineer @Clever Cloud I am working around data infrastructure, to offer new database-oriented products üöÄ ","date":"2018-12-15","objectID":"/about/:1:2","tags":null,"title":"Hello üëã","uri":"/about/"},{"categories":null,"content":"2020 to 2021: Technical Leader @OVHcloud - Managed Kubernetes I was working on the managed Kubernetes product. I was involved in: Improving ETCD‚Äôs scalability and OpenStack CSI, Self-healing customers control-planes, Go mentoring, On-call duty. ","date":"2018-12-15","objectID":"/about/:1:3","tags":null,"title":"Hello üëã","uri":"/about/"},{"categories":null,"content":"2019 to 2020: Technical Leader @OVHcloud - ioStream I was working on the underlying infrastructure of IO oriented products, including ioStream, a geo-replicated, managed topic-as-a-service product built using Apache Pulsar. During that time, I worked around adding Kafka‚Äôs protocol to Pulsar. We launched a beta mid-2019, and the project has been shutdown mid-2020. 2016 to 2019: Infrastructure Engineer @OVH I worked on Metrics Data Platform. We are using Warp10 with friendly Apache softwares such as Hbase, Hadoop, Zookeeper, Kafka and Flink to handle all OVH‚Äôs metrics-based monitoring, which represent around 432 billions of measurements per day. I have taken part of most of Metrics development, from internal management to Ingress/Egress translation part. I also worked on the implementation and deployment of a distributed and scalable alerting system using Apache Flink. I was using Flink, HBase, Hadoop, Kafka, Ansible, Go, Rust, Java, Linux, WarpScript on a daily basis. I was on-call duty on more than 700 servers, including: 3 Warp10 fully distributed clusters, including one that handling 1.5 millions datapoints per second 3 Hadoop clusters, including a 250 nodes Hadoop cluster running a 75k regions Hbase table Various Apache technologies, such as Kafka, Zookeeper and Flink I gave training and support for both external and internal client of Metrics, including WarpScript. During that time, I contributed to Apache Flink and HBase. ","date":"2018-12-15","objectID":"/about/:1:4","tags":null,"title":"Hello üëã","uri":"/about/"},{"categories":null,"content":"Abstract This is a one-day course I am giving at my former engineering school ISEN Brest. Schedule Introduction to Time Series Time Series in real life Code instrumentation Lab: searching for exoplanets Ressources slides ","date":"2018-11-11","objectID":"/talks/one-day-timeseries/:0:0","tags":null,"title":"One day into the Time Series's world","uri":"/talks/one-day-timeseries/"},{"categories":null,"content":"Occurences Paris Open Source Summit, 2017 Ressources The slides are available here. Photos and Tweets Let's discover @OvhMetrics experience with @PierreZ at #OSSPARIS17 pic.twitter.com/Xz1tRTNcFW ‚Äî Collignon R√©mi (@miton1810) December 7, 2017 ","date":"2017-12-16","objectID":"/talks/metrics-experience/:0:0","tags":null,"title":"Metrics Experience beyond protocols","uri":"/talks/metrics-experience/"},{"categories":["timeseries","helloexoworld","analytics","warp10"],"content":"update 2019: this is a repost on my own blog. original article can be read on medium. Artist‚Äôs impression of the super-Earth exoplanet LHS 1140b By ESO/spaceengine.org‚Ää‚Äî‚ÄäCC BY 4.0 My passion for programming was kind of late, I typed my first line of code at my engineering school. It then became a passion, something I‚Äôm willing to do at work, on my free-time, at night or the week-end. But before discovering C and other languages, I had another passion: astronomy. Every summer, I was participating at the Nuit des Etoiles, a global french event organized by numerous clubs of astronomers offering several hundreds (between 300 and 500 depending on the year) of free animation sites for the general public. As you can see below, I was kind of young at the time! But the sad truth is that I didn‚Äôt do any astronomy during my studies. But now, I want to get back to it and look at the sky again. There were two obstacles: The price of equipments The local weather I was looking for something that would unit my two passions: computer and astronomy. So I started googling: I found a lot of amazing projects using Raspberry pis, but I didn‚Äôt find something that would motivate me over the time. So I started typing over keywords, more work-related, such as time series or analytics. I found many papers related to astrophysics, but there was two keywords that were coming back: exoplanet detection. ","date":"2017-10-11","objectID":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/:0:0","tags":null,"title":"Introducing HelloExoWorld: The quest to discover exoplanets with Warp10 and Tensorflow","uri":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/"},{"categories":["timeseries","helloexoworld","analytics","warp10"],"content":"What is an exoplanet and how to detect it? Let‚Äôs quote our good old friend Wikipedia: An exoplanet or extrasolar planet is a planet outside of our solar system that orbits a star. do you know how many exoplanets that have been discovered? 3,529 confirmed planets as of 10/09/2017. I was amazed by the number of them. I started digging into the detection methods. Turns out there is one method heavily used, called the transit method. It‚Äôs like a eclipse: when the exoplanet is passing in front of the star, the photometry is varying during the transit, as shown below: animation illustrating how a dip in the observed brightness of a star may indicate the presence of an exoplanet. Credits: NASA‚Äôs Goddard Space Flight Center To recap, exoplanet detection using the transit method are in reality a time series analysis problem. As I‚Äôm starting to be familiar with that type of analytics thanks to my current work at OVH in Metrics Data Platform, I wanted to give it a try. ","date":"2017-10-11","objectID":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/:0:1","tags":null,"title":"Introducing HelloExoWorld: The quest to discover exoplanets with Warp10 and Tensorflow","uri":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/"},{"categories":["timeseries","helloexoworld","analytics","warp10"],"content":"Kepler/K2 mission Image Credit: NASA Ames/W. Stenzel Kepler is a space observatory launched by NASA in March 2009 to discover Earth-sized planets orbiting other stars. The loss of a second of the four reaction wheels during May 2013 put an end to the original mission. Fortunately, scientists decided to create an entirely community-driven mission called K2, to reuse the Kepler spacecraft and its assets. But furthermore, the community is also encouraged to exploit the mission‚Äôs unique open data archive. Every image taken by the satellite can be downloaded and analyzed by anyone. More information about the telescope itself can be found here. ","date":"2017-10-11","objectID":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/:0:2","tags":null,"title":"Introducing HelloExoWorld: The quest to discover exoplanets with Warp10 and Tensorflow","uri":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/"},{"categories":["timeseries","helloexoworld","analytics","warp10"],"content":"Where I‚Äôm going The goal of my project is to see if I can contribute to the exoplanets search using new tools such as Warp10 and TensorFlow. Using Deep Learning to search for anomalies could be much more effective than writing WarpScript, because it is the neural network's job to learn by itself how to detect the exoplanets. As I‚Äôm currently following Andrew Ng courses about Deep Learning, it is also a great opportunity for me to play with Tensorflow in a personal project. The project can be divided into several steps: Import the data Analyze the data using WarpScript Build a neural network to search for exoplanets Let's see how the import was done! ","date":"2017-10-11","objectID":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/:0:3","tags":null,"title":"Introducing HelloExoWorld: The quest to discover exoplanets with Warp10 and Tensorflow","uri":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/"},{"categories":["timeseries","helloexoworld","analytics","warp10"],"content":"Importing Kepler and K2 dataset Step 0: Find the data As mentioned previously, data are available from The Mikulski Archive for Space Telescopes or MAST. It‚Äôs a NASA funded project to support and provide the astronomical community with a variety of astronomical data archives. Both Kepler and K2 dataset are available through campaigns. Each campaign has a collection of tar files, which are containing the FITS files associated. A FITS file is an open format for images which is also containing scientific data. FITS file representation. Image Credit: KEPLER \u0026 K2 Science Center Step 1: ETL (Extract, Transform and Load) into Warp10 To speed-up acquisition, I developed kepler-lens to automatically download Kepler/K2 datasets and extract the needed time series into a CSV format. Kepler-lens is using two awesome libraries: pyKe to export the data from the FITS files to CSV (#PR69 and #PR76 have been merged). kplr is used to tag the dataset. With it, I can easily find stars with confirmed exoplanets or candidates. Then Kepler2Warp10 is used to push the CSV files generated by kepler-lens to Warp10. To ease importation, an Ansible role has been made, to spread the work across multiples small virtual machines. The import of @NASAKepler dataset has been spread on 16 machines, just because I can üòé pic.twitter.com/qa49tAgdzz ‚Äî Pierre Zemb (@PierreZ) September 15, 2017 550k distincts stars around 50k datapoints per star That's around 27,5 billions of measures (300GB of LevelDB files), imported on a standalone instance. The Warp10 instance is self-hosted on a dedicated Kimsufi server at OVH. Here‚Äôs the full specifications for the curious ones: Now that the data are available, we are ready to dive into the dataset and look for exoplanets! Let's use WarpScript !### Let's see a transit using WarpScript WarpScript logo For those who don‚Äôt know WarpScript, I recommend reading my previous blogpost ‚ÄúEngage maximum warp speed in time series analysis with WarpScript‚Äù. Let‚Äôs first plot the data! We are going to take a well-known star called Kepler-11. It has (at least) 6 confirmed exoplanets. Let's write our first WarpScript: The FETCH function retrieves raw datapoints from Warp10. Let‚Äôs plot the result of our script: Mmmmh, the straight lines are representing empties period with no datapoints; they correspond to different observations. Let's divide the data and generate one time series per observation using TIMESPLIT: To ease the display, 0 GET is used to get only the first observation. Let's see the result: Much better. Do you see the dropouts? Those are transiting exoplanets! Now we‚Äôll need to write a WarpScript to automatically detect transits. But that was enough for today, so we‚Äôll cover this **in the next blogpost!**Thank you for reading! Feel free to comment and to subscribe to the twitter account! Artist‚Äôs impression of the ultracool dwarf star TRAPPIST-1 from close to one of its planets. Image Credit: By ESO/M. Kornmesser‚Ää‚Äî‚ÄäCC BY-SA 4.0 ","date":"2017-10-11","objectID":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/:0:4","tags":null,"title":"Introducing HelloExoWorld: The quest to discover exoplanets with Warp10 and Tensorflow","uri":"/posts/introducing-helloexoworld-the-quest-to-discover-exoplanets-with-warp10-and-tensorflow/"},{"categories":["timeseries","warp10","analytics"],"content":"update 2019: this is a repost on my own blog. original article can be read on medium. We, at Metrics Data Platform, are working everyday with Warp10 Platform, an open source Time Series database. You may not know it because it‚Äôs not as famous as Prometheus or InfluxDB but Warp10 is the most powerful and generic solution to store and analyze sensor data. It‚Äôs the core of Metrics, and many internal teams from OVH are using Metrics Data Platform to monitor their infrastructure. As a result, we are handling a pretty nice traffic 24/7/365, as you can see below: Not only Warp10 allows us to reach an unbelievable scalability but it also comes with his own language called WarpScript, to manipulate and perform heavy time series analysis. Before digging into the need of a new language, let‚Äôs talk a bit about the need of time series analysis.### What is a time serie ? A time serie, or sensor data, is simply a sequence of measurements over time. The definition is quite generic, because many things can be represented as a time serie: the evolution of the stock exchange or a bank account the number of calls on a webserver the fuel consumption of a car the time to insert a value into a database the time a customer is taking to register on your website the heart rate of a person measured through a smartwatch From an historical point of view, time series appeared shortly after the creation of the Web, to help engineers monitor the networks. It quickly expands to also monitors servers. With the right monitoring system, you can have insights and KPIs about your service: Analysis of long-term trend How fast is my database growing? At what speed my number of active user accounts grows? The comparison over time My queries run faster with the new version of my library? Is my site slower than last week? Alerts Trigger alerts based on advanced queries Displaying data through dashboards Dashboards help answer basic questions on the service, and in particular the 4 indispensable metrics: latency, traffic, errors and service saturation The possibility of designing retrospective Our latency is doubling, what‚Äôs going on?### Time series are complicated to handle Storage, retrieval and analysis of time series cannot be done through standard relational databases. Generally, highly scalable databases are used to support volumetry. For example, the 300,000 Airbus A380 sensors on board can generate an average of 16 TB of data per flight. On a smaller scale, a single sensor that measures every second generates 31.5 million values per year. Handling time series at scale is difficult, because you‚Äôre running into advanced distributed systems issues, such as: ingestion scalability, i.e. how to absorb all the datapoints 24‚ÅÑ7 query scalability, i.e. how to query in a raisonnable amount of time delete capability, i.e. how to handle deletes without stopping ingestion and query Frustration with existing open source monitoring tools like Nagios and Ganglia is why the giants created their own tools‚Ää‚Äî‚ÄäGoogle has Borgmon and Facebook has Gorilla, just to name two. They are closed sources but the idea of treating time-series data as a data source for generating alerts is now accessible to everyone, thanks to the former Googlers who decided to rewrite Borgmon outside Google.### Why another time series database? Now the time series ecosystem is bigger than ever, here‚Äôs a short list of what you can find to handle time series data: InfluxDB. Prometheus. Riak TS. OpenTSDB. Then there‚Äôs Warp10. The difference is quite simple, Warp10 is a platform whereas all the time series listed above are stores. This is game changing, for multiples reasons. Security-first design Security is mandatory for data access and sharing job‚Äôs results, but in most of the above databases, security access is not handled by default. With Warp10, security is handled with crypto tokens similar to Macaroons. High level analysis capabilities Using classical time series database, high level analysis must be done e","date":"2017-10-08","objectID":"/posts/engage-maximum-warp-speed-in-time-series-analysis-with-warpscript/:0:0","tags":null,"title":"Engage maximum warp speed in time series analysis with WarpScript","uri":"/posts/engage-maximum-warp-speed-in-time-series-analysis-with-warpscript/"},{"categories":["timeseries","warp10","analytics"],"content":"Geez, give me an example Here‚Äôs an example of a simple but advanced query: // Fetching all values [ $token ‚Äòtemperature‚Äô {} NOW 1 h ] FETCH // Get max value for each minute [ SWAP bucketizer.max 0 1 m 0 ] BUCKETIZE // Round to nearest long [ SWAP mapper.round 0 0 0 ] MAP // reduce the data by keeping the max, grouping by 'buildingID' [ SWAP [ 'buildingID' ] reducer.max ] REDUCE Have you guessed the goal? The result will display the temperature from now to 1 hour of the hottest room per buildingID. ","date":"2017-10-08","objectID":"/posts/engage-maximum-warp-speed-in-time-series-analysis-with-warpscript/:0:1","tags":null,"title":"Engage maximum warp speed in time series analysis with WarpScript","uri":"/posts/engage-maximum-warp-speed-in-time-series-analysis-with-warpscript/"},{"categories":["timeseries","warp10","analytics"],"content":"What about a more complex example? You‚Äôre still here? Good, let‚Äôs have a more complex example. Let‚Äôs say that I want to do some patterns recognition. Let‚Äôs take an example. Here‚Äôs a cosinus with an increasing amplitude: I want to detect the green part of the time series, because I know that my service is crashing when I have that kind of load. With WarpScript, it‚Äôs only a 2 functions calls: PATTERNS is generating a list of motifs. PATTERNDETECTION is running the list of motifs on all the time series you have. Here‚Äôs the code // defining some variables 32 'windowSize' STORE 8 'patternLength' STORE 16 'quantizationScale' STORE // Generate patterns $pattern.to.detect 0 GET $windowSize $patternLength $quantizationScale PATTERNS VALUES 'patterns' STORE // Running the patterns through a list of GTS (Geo Time Series) $list.of.gts $patterns $windowSize $patternLength $quantizationScale PATTERNDETECTION Here‚Äôs the result: As you can see, PATTERNDETECTION is working even with the increasing amplitude! You can discover this example by yourself by using Quantum, the official web-based IDE for WarpScript. You need to switch X-axis scale to Timestamp in order to see the courbe.Thanks for reading, here‚Äôs a nice list of additionnals informations about the time series subject and Warp10: Metrics Data Platform, our product Warp10 official documentation Warp10 tour, similar to ‚ÄúThe Go Tour‚Äù Presentation of the Warp 10 Time Series Platform at the 42 US school in Fremont Warp10 Google Groups ","date":"2017-10-08","objectID":"/posts/engage-maximum-warp-speed-in-time-series-analysis-with-warpscript/:0:2","tags":null,"title":"Engage maximum warp speed in time series analysis with WarpScript","uri":"/posts/engage-maximum-warp-speed-in-time-series-analysis-with-warpscript/"},{"categories":["programming","event"],"content":"update 2019: this is a repost on my own blog. original article can be read on medium. Do your own cover on http://dev.to/rly I‚Äôm still a student, so my point of view could be far from reality, be gentle ;) **tl;dr: Queue messaging are cool. Use them at the core of your architecture.**I‚Äôm currently playing a lot around Kafka and Flink at work. I also discovered Vert.x at my local JUG. All three have a common word: events. Event-driven architecture is not something that I learned at school, and I think that‚Äôs a shame. It‚Äôs really powerful and useful, especially in a world where we speak more and more about ‚Äúserverless‚Äù and ‚Äúmicro services‚Äù stuff. So here‚Äôs my attempt to make a big sum-up. the Unix philosophy I‚Äôm a huge fan of GNU/Linux. I just love my terminal. It‚Äôs been difficult at the beginning, but now, I consider myself fluent with it. My favorite feature ? Pipes or |. For those who don‚Äôt know, it‚Äôs the ability to pass the result of the command to another command. For example, to count how many files you have in a folder, you‚Äôll find yourself doing something like this: list files in a folder From this list, manipulate/filter it. One line must correspond to one file, things like folder are omitted And then count the line! In the UNIX world, it should give you something like ‚Äúls -l | grep ^- | wc -l‚Äù. it might feels like chinese. For me, it‚Äôs just feels logical. 3 operations mapped into 3 commands. You declare a set a commands that, in the end, give you the result. It‚Äôs simple and also very fast (in fact, you can find funny articles like this one: Command-line tools can be 235x faster than your Hadoop cluster). This is only possible thanks to the UNIX philosophy, greatly describe by Doug McIlroy, Elliot Pinson and Berk Tague in 1978: Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new ‚Äúfeatures‚Äù.\u003e Expect the output of every program to become the input to another, as yet unknown, program. Why should I care? It‚Äôs 2016, not 1978! Well‚Ä¶ Back in 2016 Cloud changed everything in terms of software engineering. We can now deploy applications without thinking about the underlying server. How cool is that? Let‚Äôs take some steps back. Now that you can easily deploy a huge application, what can be accomplished? Well, if I can deploy one app with ease, Why should I deploy only one huge app ? why can‚Äôt I deploy multiples applications instead of one? Let‚Äôs call theses applications micro services because we are in 2016. OK, so now I‚Äôm applying the first rule of the UNIX Philosophy, because I have multiples programs that are doing one job each. But about the second rule? How can they communicate? How can we simulate UNIX pipes? Before answering, let‚Äôs answer to another question first: What do we really need to send through our network? Don‚Äôt forget the Fallacies of distributed computing‚Ä¶ Let‚Äôs take an example. We are a new startup, and we are building our plateform. We‚Äôll certainly need to handle our customers. Let‚Äôs say that for each new customer, we need to make two actions: add it to our database, and then to our mailing-list. A simple and classical way would be to just call two functions (whether on the same applications or not), and then say to the customer: ‚ÄúYou‚Äôre successfully registered‚Äù. Like this: Classic approach Is there another approach? Let‚Äôs use an event-based architecture: Let‚Äôs talk events Let‚Äôs ask Google, what‚Äôs an event? a thing that happens, especially one of importance. Well, handling a new customer is a thing that happens (hopefully). For this, we‚Äôll be using a Queue messaging system or Broker. It‚Äôs a middleware that will receive events, and making them available for another application or groups of applications. Queue messaging architecture with 2 producers and 4 consumers So let‚Äôs rethink our architecture. Pay attention to the words: our Register page will produce an event that will contains all the information about our client. This event will be queued","date":"2016-05-13","objectID":"/posts/eventdriven-architecture-101/:0:0","tags":null,"title":"Event-driven architecture 101","uri":"/posts/eventdriven-architecture-101/"},{"categories":["operating","containers"],"content":"update 2019: this is a repost on my own blog. original article can be read on medium. English is not my first language, so the whole story may have some mistakes‚Ä¶ corrections and fixes will be greatly appreciated. I‚Äôm also still a student, so my point of view could be far from ‚Äúproduction ready‚Äù, be gentle ;-) In the last two years, there‚Äôs been a technology that became really hype. It was the graal for easy deployments, easy applications management. Let‚Äôs talk about containers. ","date":"2016-01-04","objectID":"/posts/lets-talk-about-containers/:0:0","tags":null,"title":"Let‚Äôs talk about containers","uri":"/posts/lets-talk-about-containers/"},{"categories":["operating","containers"],"content":"‚ÄúWrite once, run everywhere‚Äù When I first heard about containers, I was working as a part-time internship for a french bank as a developer in a Ops team. I was working around Hadoop and monitoring systems, and I was wondering ‚ÄúHow should I properly deploy my work?‚Äù. It was a java app, running into the official Java version provided by my company. I couldn‚Äôt just give it to my colleagues and leave them do some vaudou stuff because they are the Ops team. I remembered saying to myself ‚Äùfortunately, all the features that I need are in this official java version, I don‚Äôt need the latest JRE. I just need to bundle everything into a jar and done‚Äù. But what if it wasn‚Äôt? What if I had to explain to my colleagues that I need the new JRE for a really small app written by an intern? Or I needed another non-standard library during runtime? The important thing here at the time was that, at any time, I could deploy it on another server that had Java, because everything is bundled into that big fat jar file. After all, ‚Äúwrite once, run everywhere‚Äù was the slogan created by Sun Microsystems to illustrate the cross-platform benefits of the Java language. That is a real commodity, and this is the first thing that strike me with Docker. ","date":"2016-01-04","objectID":"/posts/lets-talk-about-containers/:0:1","tags":null,"title":"Let‚Äôs talk about containers","uri":"/posts/lets-talk-about-containers/"},{"categories":["operating","containers"],"content":"Docker hype I will always remember my chat with my colleagues about it. I was like this: ","date":"2016-01-04","objectID":"/posts/lets-talk-about-containers/:0:2","tags":null,"title":"Let‚Äôs talk about containers","uri":"/posts/lets-talk-about-containers/"},{"categories":["operating","containers"],"content":"And they were more like Ops knew about containers since the dawn of time, so why such hype now? I think that ‚Äúwrite once, run everywhere‚Äù is the true slogan of Docker, because you can run docker containers in any environments that has Docker. You want to try the latest datastore/SaaS app that you found on Hacker News or Reddit? There‚Äôs a Dockerfile for that. And that is super cool. So everyone started to get interested in Docker, myself included. But the real benefit is that many huge companies like Google admits that containers are the way they are deploying apps. They don‚Äôt care what type of applications they are deploying or where it‚Äôs running, it‚Äôs just running somewhere. That‚Äôs all that matters. By unifying the packages, you can automatize and deliver whatever you want somewhere. Do you really care if it‚Äôs on a specific machine? No you don‚Äôt. That‚Äôs a powerful way to think infrastructure more like a bunch of compute or storage power, and not individual machines. ","date":"2016-01-04","objectID":"/posts/lets-talk-about-containers/:1:0","tags":null,"title":"Let‚Äôs talk about containers","uri":"/posts/lets-talk-about-containers/"},{"categories":["operating","containers"],"content":"Let‚Äôs create a container That‚Äôs not a secret: I love Go. It‚Äôs in my opinion a very nice programming language that you should really try. So let‚Äôs say that I‚Äôm creating a go app, and then ship it with Docker. So I‚Äôll use the officiel Docker image right? Then I end up with a 700MB container to ship a 10MB app‚Ä¶ I thought that containers were supposed to be small‚Ä¶ Why? because it‚Äôs based on a full OS, with go compiler and so on. To run a single binary, there‚Äôs no need to have the whole Go compiler stack. That was really bothering me. At this point, if the container is holding everything, why not use a VM? Why do we need to bundle Ubuntu into the container? From a outside point-of-view, running a container in interactive mode is much like a virtual machines right? At the time of writing, Docker‚Äôs official image for Ubuntu was pulled more than 36,000,000 time. That‚Äôs huge! And disturbing. Do you really need for example ‚Äúls, chmod, chown, sudo‚Äù into a container? There is another huge impact on having a full distribution on a container: Security. You now have to watch not only for CVEs (Common Vulnerabilities and Exposures) on the packages in your host distribution, but also in your container! After all, based on this presentation, 66.6% of analyzed images on Quay.io are vulnerable to Ghost, and 80% to Heartbleed. That is quite scary‚Ä¶ So adding this nightmare doesn‚Äôt seems the solution. ","date":"2016-01-04","objectID":"/posts/lets-talk-about-containers/:1:1","tags":null,"title":"Let‚Äôs talk about containers","uri":"/posts/lets-talk-about-containers/"},{"categories":["operating","containers"],"content":"So what should I put into my container? I looked a lot around the internet, I saw things like docker-alpine or [baseimage-docker] (https://github.com/phusion/baseimage-docker)which are cool, but in fact, the answer was on Docker‚Äôs website‚Ä¶ Here‚Äôs the [official sentence] (https://www.docker.com/what-docker)that explains the difference between containers and virtual machines: ‚ÄúContainers include the application and all of its dependencies, but share the kernel with other containers.‚Äù This specific sentence triggers something in my head. When you execute a program on your UNIX system, the system creates a special environment for that program. This environment contains everything needed for the system to run the program as if no other program were running on the system. It‚Äôs exactly the same! So a container should be abstract not as a Virtual machines, but as a UNIX process! application + dependencies represent the image Runtime environment like token/password will be passed through env vars for example ","date":"2016-01-04","objectID":"/posts/lets-talk-about-containers/:1:2","tags":null,"title":"Let‚Äôs talk about containers","uri":"/posts/lets-talk-about-containers/"},{"categories":["operating","containers"],"content":"Static compilation Meet Go Here‚Äôs an interesting fact: Go, the open-source programming language pushed by Google supports statically apps, what a coincidence! That means that this statically app will be directly talking to the kernel. Our Docker image can be empty, except for the binary and needed files like configuration. There‚Äôs a strange image on Docker that you might have seen, which is called ‚Äúscratch‚Äù: You can use Docker‚Äôs reserved, minimal image, scratch, as a starting point for building containers. Using the scratch ‚Äúimage‚Äù signals to the build process that you want the next command in the Dockerfile to be the first filesystem layer in your image. While scratch appears in Docker‚Äôs repository on the hub, you can‚Äôt pull it, run it, or tag any image with the name scratch. Instead, you can refer to it in your Dockerfile. That means that our Dockerfile now looks like this: FROM scratch ADD hello / CMD [/hello] So now, I have finally (I think) the right abstraction for a container! We have a container containing only our app. Can we go even further? The most interesting thing that I learned from (quickly) reading Large-scale cluster management at Google with Borg is this: Borg programs are statically linked to reduce dependencies on their runtime environment, and structured as packages of binaries and data files, whose installation is orchestrated by Borg. Here‚Äôs the (final) answer! By trully coming back to the UNIX process point-of-view, we can abstract containers as Unix processes. Bu we still need to handle them. So the role of Docker would be more like a Operating System builder (nice name found by Quentin ADAM).As a conclusion, I think that Docker true success was to show developers that they can sandbox their apps easily, and now it‚Äôs our work to build better software, and learning new design patterns.Please, Feel free to react to this article, you can reach me on Twitter, Or visite my website. ","date":"2016-01-04","objectID":"/posts/lets-talk-about-containers/:1:3","tags":null,"title":"Let‚Äôs talk about containers","uri":"/posts/lets-talk-about-containers/"}]