<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
      <title>Pierre Zemb&#x27;s Blog - tuning</title>
      <link>https://pierrezemb.fr</link>
      <description>Pierre Zemb personal blog</description>
      <generator>Zola</generator>
      <language>en</language>
      <atom:link href="https://pierrezemb.fr/tags/tuning/rss.xml" rel="self" type="application/rss+xml"/>
      <lastBuildDate>Mon, 22 Apr 2024 00:37:27 +0100</lastBuildDate>
      <item>
          <title>Redwood‚Äôs memory tuning in FoundationDB</title>
          <pubDate>Mon, 22 Apr 2024 00:37:27 +0100</pubDate>
          <author>Pierre Zemb</author>
          <link>https://pierrezemb.fr/posts/redwood-memory-tuning/</link>
          <guid>https://pierrezemb.fr/posts/redwood-memory-tuning/</guid>
          <description xml:base="https://pierrezemb.fr/posts/redwood-memory-tuning/">&lt;p&gt;While FoundationDB allows you to obtain sub-milliseconds transactions‚Äôs latency without any knob-tuning, we had to bump a bit memory usage for Redwood under certain usage and workload. The following configuration has been tested on clusters from 7.1 to 7.3.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;btree-page-cache&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#btree-page-cache&quot; aria-label=&quot;Anchor link for: btree-page-cache&quot;&gt;üîó&lt;&#x2F;a&gt;BTree page cache&lt;&#x2F;h2&gt;
&lt;p&gt;We discovered the issue when we saw a performance decrease on our cluster storing time-series data. Our cluster was reporting some high disk-business, causing outages:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;10.0.3.23:4501 ( 65% cpu; 61% machine; 0.010 Gbps; 93% disk IO; 7.5 GB &#x2F; 7.4 GB RAM  )
&lt;&#x2F;span&gt;&lt;span&gt;10.0.3.24:4501 ( 61% cpu; 61% machine; 0.010 Gbps; 87% disk IO; 9.7 GB &#x2F; 7.4 GB RAM  )
&lt;&#x2F;span&gt;&lt;span&gt;10.0.3.25:4501 ( 69% cpu; 61% machine; 0.010 Gbps; 93% disk IO; 5.4 GB &#x2F; 7.4 GB RAM  )
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This was our first ¬´we need to dig into this¬ª moment with FDB. We couldn‚Äôt find the root-cause and we asked the community. Turns out we had a classic page-cache issue which was spotted by &lt;a href=&quot;https:&#x2F;&#x2F;forums.foundationdb.org&#x2F;u&#x2F;markus.pilman&#x2F;summary&quot;&gt;Markus Pilman&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;forums.foundationdb.org&#x2F;u&#x2F;wmd&#x2F;summary&quot;&gt;William Dowling&lt;&#x2F;a&gt;. While the trace files are pretty verbose, they are containing a lot of information like this one:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;&amp;quot;PagerCacheHit&amp;quot;: &amp;quot;39852&amp;quot;,
&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;PagerCacheMiss&amp;quot;: &amp;quot;25903&amp;quot;,
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Yep, that‚Äôs a 40% cache-miss ratio over 5s üò± This is why the disk was so busy, spending his time moving pages back and forth. We need to bump the memory, but how much? The general recommandation that worked for us is to target around 1-2% of the &lt;code&gt;kvstore_used_bytes&lt;&#x2F;code&gt; metrics. As we have around 1TiB of data per StorageServer, we can add the following config key:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;cache_memory = 10GiB
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Which fixed our cache-miss issue üéâ&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;&amp;quot;PagerCacheHit&amp;quot;: &amp;quot;51968&amp;quot;,
&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;PagerCacheMiss&amp;quot;: &amp;quot;432&amp;quot;,
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;¬†&lt;&#x2F;p&gt;
&lt;h2 id=&quot;byte-sample-memory-usage&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#byte-sample-memory-usage&quot; aria-label=&quot;Anchor link for: byte-sample-memory-usage&quot;&gt;üîó&lt;&#x2F;a&gt;Byte Sample memory usage&lt;&#x2F;h2&gt;
&lt;p&gt;But our problems are still unresolved, as we are still seeing some OOM üò≠ Because this cluster is storing time-series data, each StorageServers is holding around 1TiB of data. As we were holding more and more data, we saw more and more OOM errors on our &lt;code&gt;fdbmonitor&lt;&#x2F;code&gt; logs. Something was growing linearly with our usage and needed tuning. This time, we had help from &lt;a href=&quot;https:&#x2F;&#x2F;forums.foundationdb.org&#x2F;u&#x2F;SteavedHams&#x2F;summary&quot;&gt;Steve Atherton&lt;&#x2F;a&gt; which pointed us towards the direction of the &lt;a href=&quot;https:&#x2F;&#x2F;forums.foundationdb.org&#x2F;t&#x2F;foundationdb-7-1-24-the-memory-usage-after-clean-startup-of-fdbserver-process-is-too-high&#x2F;3863&#x2F;8?u=pierrez&quot;&gt;Byte Sample&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;There is a data structure that storage servers have called the Byte Sample which stores a deterministic random sample of keys. This data is persisted on disk in the storage engine and is loaded immediately upon storage server startup. Unfortunately, its size is not tracked or reported, but grows linearly with KV size and I suspect yours is somewhere around 4GB-6GB based on the memory usage I‚Äôve seen for smaller storage KV sizes.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;So, we need to add around 4GB more in the memory, but there is no config for that parameter. It needs to be embedded in the global &lt;code&gt;memory&lt;&#x2F;code&gt; parameter. Let‚Äôs compute the right value!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-global-memory-formula&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-global-memory-formula&quot; aria-label=&quot;Anchor link for: the-global-memory-formula&quot;&gt;üîó&lt;&#x2F;a&gt;The global memory formula&lt;&#x2F;h2&gt;
&lt;p&gt;By testing things on our clusters, we ended up with this formula:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;# Default is 2
&lt;&#x2F;span&gt;&lt;span&gt;cache_memory = (1-2% of kvstore_used_bytes)GiB
&lt;&#x2F;span&gt;&lt;span&gt;# Default is 8
&lt;&#x2F;span&gt;&lt;span&gt;memory = (8 + cache_memory + 4-6GB per TB of kvstore_used_bytes)GiB
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Which fixed all our memory issues with FoundationDB üéâ And to be fair, this is the only things we needed to tune on our clusters, which is quite impressive üëÄ&lt;&#x2F;p&gt;
&lt;h2 id=&quot;special-thanks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#special-thanks&quot; aria-label=&quot;Anchor link for: special-thanks&quot;&gt;üîó&lt;&#x2F;a&gt;Special thanks&lt;&#x2F;h2&gt;
&lt;p&gt;I would like to thank Markus, William and Steve from the FoundationDB community for their help ü§ù&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Thank you&lt;&#x2F;strong&gt; for reading my post! Feel free to react to this article, I am also available on &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;PierreZ&quot;&gt;Twitter&lt;&#x2F;a&gt; if needed.&lt;&#x2F;p&gt;
</description>
          <category domain="tag">foundationdb</category>
          <category domain="tag">performance</category>
          <category domain="tag">storage</category>
          <category domain="tag">database</category>
          <category domain="tag">tuning</category>
      </item>
    </channel>
</rss>
