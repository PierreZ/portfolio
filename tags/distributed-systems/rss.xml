<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
      <title>Pierre Zemb&#x27;s Blog - distributed-systems</title>
      <link>https://pierrezemb.fr</link>
      <description>Pierre Zemb personal blog</description>
      <generator>Zola</generator>
      <language>en</language>
      <atom:link href="https://pierrezemb.fr/tags/distributed-systems/rss.xml" rel="self" type="application/rss+xml"/>
      <lastBuildDate>Wed, 24 Sep 2025 00:00:00 +0000</lastBuildDate>
      <item>
          <title>A Practical Guide to Application Metrics: Where to Put Your Instrumentation</title>
          <pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate>
          <author>Pierre Zemb</author>
          <link>https://pierrezemb.fr/posts/practical-guide-to-application-metrics/</link>
          <guid>https://pierrezemb.fr/posts/practical-guide-to-application-metrics/</guid>
          <description xml:base="https://pierrezemb.fr/posts/practical-guide-to-application-metrics/">&lt;p&gt;I keep having the same conversation with junior developers. They&#x27;re building their first production service, and they ask: &quot;Where should I put metrics in my application?&quot; Then, inevitably: &quot;What should I actually measure?&quot;&lt;&#x2F;p&gt;
&lt;p&gt;After mentoring dozens of engineers and running distributed systems for years, I&#x27;ve learned these aren&#x27;t just beginner questions. Even experienced developers struggle with metrics placement because most of us learned observability as an afterthought, not as a core design principle.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve been on both sides: deploying services with no metrics and scrambling at 3 AM to understand what broke, and also building comprehensive monitoring that caught issues before users noticed. The difference isn&#x27;t just about sleep quality; it&#x27;s about building systems you can actually operate with confidence.&lt;&#x2F;p&gt;
&lt;p&gt;This post gives you a practical framework for where to instrument your applications. No theory, just patterns I&#x27;ve learned from years of production incidents.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-five-essential-metric-types&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-five-essential-metric-types&quot; aria-label=&quot;Anchor link for: the-five-essential-metric-types&quot;&gt;🔗&lt;&#x2F;a&gt;The Five Essential Metric Types&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Quick note on naming:&lt;&#x2F;strong&gt; Throughout this post, I use dots (&lt;code&gt;.&lt;&#x2F;code&gt;) as metric separators like &lt;code&gt;api.requests.total&lt;&#x2F;code&gt;. This works perfectly for us because we&#x27;re heavy &lt;a href=&quot;https:&#x2F;&#x2F;warp10.io&#x2F;&quot;&gt;Warp 10&lt;&#x2F;a&gt; users, and Warp 10 handles dots beautifully. If you&#x27;re using Prometheus or other systems that prefer underscores, just replace the dots with underscores (&lt;code&gt;api_requests_total&lt;&#x2F;code&gt;). The patterns remain the same!&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;All useful application metrics fall into five categories. Understanding these helps you decide what to instrument and where:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. Operational Counters&lt;&#x2F;strong&gt; track discrete events in your system. Every time something happens (a request arrives, a job finishes, an error occurs), you increment a counter. The most critical insight here is measuring both success and failure paths. Most developers remember to count successful operations but forget the errors, leaving them blind when things break. Examples include &lt;code&gt;api.requests.total&lt;&#x2F;code&gt;, &lt;code&gt;db.queries.executed&lt;&#x2F;code&gt;, &lt;code&gt;auth.failures.count&lt;&#x2F;code&gt;, &lt;code&gt;payments.declined.count&lt;&#x2F;code&gt;, &lt;code&gt;jobs.started&lt;&#x2F;code&gt;, and &lt;code&gt;cache.evictions&lt;&#x2F;code&gt;. Always include labels like &lt;code&gt;method&lt;&#x2F;code&gt;, &lt;code&gt;endpoint&lt;&#x2F;code&gt;, &lt;code&gt;error_type&lt;&#x2F;code&gt; to provide context.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. Resource Utilization&lt;&#x2F;strong&gt; answers &quot;how much of X am I using right now?&quot; These are your early warning system for capacity problems. Track current values with gauges, cumulative usage with counters. The key is monitoring resources before they&#x27;re completely exhausted. A connection pool might support 100 connections, but if 95 are active, you&#x27;re in trouble. Monitor &lt;code&gt;memory.used.bytes&lt;&#x2F;code&gt;, &lt;code&gt;db.connections.active&lt;&#x2F;code&gt;, &lt;code&gt;cache.size.entries&lt;&#x2F;code&gt;, &lt;code&gt;thread_pool.active_threads&lt;&#x2F;code&gt;, and &lt;code&gt;disk.space.available.bytes&lt;&#x2F;code&gt;. Watch for patterns like steadily increasing memory usage or connection counts approaching pool limits.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;3. Performance and Latency&lt;&#x2F;strong&gt; shows how fast (or slow) things are running. Users feel latency immediately, making these often your most-watched dashboards. Always include units in metric names (&lt;code&gt;.ms&lt;&#x2F;code&gt;, &lt;code&gt;.seconds&lt;&#x2F;code&gt;, &lt;code&gt;.bytes&lt;&#x2F;code&gt;) to make dashboards self-documenting. Track &lt;code&gt;api.response_time.ms&lt;&#x2F;code&gt;, &lt;code&gt;db.query.duration.ms&lt;&#x2F;code&gt;, &lt;code&gt;jobs.processing_time.seconds&lt;&#x2F;code&gt;, and &lt;code&gt;external_api.call.duration.ms&lt;&#x2F;code&gt;. Monitor percentiles (p50, p95, p99) not just averages: a 1ms average with a 5-second p99 indicates serious problems.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;4. Data Volume and Throughput&lt;&#x2F;strong&gt; tracks data flow through your system. These metrics are crucial for capacity planning and spotting bottlenecks before they cause user-visible problems. Monitor both input and output rates to understand processing efficiency. Focus on &lt;code&gt;queue.messages.consumed&lt;&#x2F;code&gt;, &lt;code&gt;network.bytes.sent&lt;&#x2F;code&gt;, &lt;code&gt;database.rows.processed&lt;&#x2F;code&gt;, &lt;code&gt;file_processor.files.completed&lt;&#x2F;code&gt;, and &lt;code&gt;batch_processor.records.per_batch&lt;&#x2F;code&gt;. Compare input vs output rates to identify accumulating backlogs.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;5. Business Logic&lt;&#x2F;strong&gt; captures domain-specific metrics that relate to your actual business value. These are often the most valuable metrics for understanding how your application is really being used and whether technical problems are affecting business outcomes. Track &lt;code&gt;orders.placed&lt;&#x2F;code&gt;, &lt;code&gt;users.registered&lt;&#x2F;code&gt;, &lt;code&gt;searches.executed&lt;&#x2F;code&gt;, &lt;code&gt;documents.uploaded&lt;&#x2F;code&gt;, and &lt;code&gt;subscriptions.activated&lt;&#x2F;code&gt;. Don&#x27;t underestimate these: they&#x27;re what your executives care about and often reveal problems that technical metrics miss.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type&lt;&#x2F;th&gt;&lt;th&gt;Examples&lt;&#x2F;th&gt;&lt;th&gt;Key Insight&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Operational&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;api.requests.total&lt;&#x2F;code&gt;, &lt;code&gt;auth.failures&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Track success AND failure paths&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Resource&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;memory.used.bytes&lt;&#x2F;code&gt;, &lt;code&gt;db.connections.active&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Early warning for capacity issues&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Performance&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;api.response_time.ms&lt;&#x2F;code&gt;, &lt;code&gt;db.query.duration.ms&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Users feel latency immediately&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Throughput&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;queue.messages.consumed&lt;&#x2F;code&gt;, &lt;code&gt;network.bytes.sent&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Understand data flow patterns&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Business&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;orders.placed&lt;&#x2F;code&gt;, &lt;code&gt;users.login&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;What executives actually care about&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;where-to-instrument-a-component-guide&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#where-to-instrument-a-component-guide&quot; aria-label=&quot;Anchor link for: where-to-instrument-a-component-guide&quot;&gt;🔗&lt;&#x2F;a&gt;Where to Instrument: A Component Guide&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;api-endpoints-and-http-requests&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#api-endpoints-and-http-requests&quot; aria-label=&quot;Anchor link for: api-endpoints-and-http-requests&quot;&gt;🔗&lt;&#x2F;a&gt;API Endpoints and HTTP Requests&lt;&#x2F;h3&gt;
&lt;p&gt;Your application&#x27;s front door deserves comprehensive monitoring. Every HTTP request tells a story from arrival to completion, and you want to capture that entire narrative, not just the happy path. Track &lt;code&gt;api.requests.total&lt;&#x2F;code&gt; with labels for &lt;code&gt;method&lt;&#x2F;code&gt;, &lt;code&gt;endpoint&lt;&#x2F;code&gt;, and &lt;code&gt;status_code&lt;&#x2F;code&gt; to understand usage patterns. Monitor &lt;code&gt;api.response_time.ms&lt;&#x2F;code&gt; to show user experience, and &lt;code&gt;api.errors.count&lt;&#x2F;code&gt; with &lt;code&gt;error_type&lt;&#x2F;code&gt; labels to reveal reliability issues. Include &lt;code&gt;auth.failures.count&lt;&#x2F;code&gt; with &lt;code&gt;failure_reason&lt;&#x2F;code&gt; to catch security problems, and &lt;code&gt;api.concurrent_requests&lt;&#x2F;code&gt; to identify when you&#x27;re approaching capacity limits. The common mistake is only instrumenting successful requests; the real value comes from measuring what happens when things go wrong: network timeouts, validation errors, service dependencies failing.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;database-layer&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#database-layer&quot; aria-label=&quot;Anchor link for: database-layer&quot;&gt;🔗&lt;&#x2F;a&gt;Database Layer&lt;&#x2F;h3&gt;
&lt;p&gt;Database calls are often your biggest bottleneck and cause more production incidents than any other component. For connection management, track &lt;code&gt;db.connections.active&lt;&#x2F;code&gt; (critical for pool management), &lt;code&gt;db.connections.idle&lt;&#x2F;code&gt; (available connections), and &lt;code&gt;db.connections.wait_time.ms&lt;&#x2F;code&gt; (time threads wait for connections). Monitor query performance with &lt;code&gt;db.queries.executed&lt;&#x2F;code&gt; (including &lt;code&gt;operation_type&lt;&#x2F;code&gt; and &lt;code&gt;table&lt;&#x2F;code&gt; labels), &lt;code&gt;db.query.duration.ms&lt;&#x2F;code&gt; (with percentile tracking), &lt;code&gt;db.slow_queries.count&lt;&#x2F;code&gt; (queries exceeding thresholds), and &lt;code&gt;db.query.rows_affected&lt;&#x2F;code&gt; (rows returned or modified). For error monitoring, track &lt;code&gt;db.errors.count&lt;&#x2F;code&gt; by &lt;code&gt;error_type&lt;&#x2F;code&gt; (timeout, deadlock, constraint violation) and &lt;code&gt;db.connection_errors.count&lt;&#x2F;code&gt; for connection failures. Connection pool exhaustion is a classic way to kill your entire application: if you support 100 connections and 95 are active, you&#x27;re in danger. Start alerting when you hit 85% utilization.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;message-queues-and-background-processing&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#message-queues-and-background-processing&quot; aria-label=&quot;Anchor link for: message-queues-and-background-processing&quot;&gt;🔗&lt;&#x2F;a&gt;Message Queues and Background Processing&lt;&#x2F;h3&gt;
&lt;p&gt;Message queues often hide subtle bugs that manifest as slowly growing delays or stuck processing. Track data flow in both directions to catch issues early. For producers, monitor &lt;code&gt;queue.messages.produced&lt;&#x2F;code&gt; (with &lt;code&gt;topic&lt;&#x2F;code&gt; and &lt;code&gt;producer_id&lt;&#x2F;code&gt; labels), &lt;code&gt;queue.messages.failed&lt;&#x2F;code&gt; (with detailed &lt;code&gt;error_type&lt;&#x2F;code&gt; labels), &lt;code&gt;queue.producer.wait_time.ms&lt;&#x2F;code&gt; (time waiting for producer availability), and &lt;code&gt;queue.batch_size&lt;&#x2F;code&gt; (messages sent per batch). For consumers, track &lt;code&gt;queue.messages.consumed&lt;&#x2F;code&gt; (successfully processed), &lt;code&gt;queue.processing.time.ms&lt;&#x2F;code&gt; (per-message duration), &lt;code&gt;queue.processing.errors&lt;&#x2F;code&gt; (failures with &lt;code&gt;error_type&lt;&#x2F;code&gt; and recovery action), &lt;code&gt;jobs.queue.depth&lt;&#x2F;code&gt; (messages waiting), and &lt;code&gt;consumer.lag.ms&lt;&#x2F;code&gt; (how far behind real-time). Background jobs need additional metrics: &lt;code&gt;jobs.started&lt;&#x2F;code&gt;, &lt;code&gt;jobs.completed&lt;&#x2F;code&gt;, &lt;code&gt;jobs.failed&lt;&#x2F;code&gt; (with failure reason), &lt;code&gt;jobs.retry.count&lt;&#x2F;code&gt;, and &lt;code&gt;jobs.execution_time.seconds&lt;&#x2F;code&gt;. Growing queue depth usually means you&#x27;re processing jobs slower than they&#x27;re being created, leading to increasing delays and eventual system overload. Consumer lag helps you understand if you&#x27;re keeping up with real-time processing needs.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;caching-and-locks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#caching-and-locks&quot; aria-label=&quot;Anchor link for: caching-and-locks&quot;&gt;🔗&lt;&#x2F;a&gt;Caching and Locks&lt;&#x2F;h3&gt;
&lt;p&gt;For cache performance, track &lt;code&gt;cache.requests.total&lt;&#x2F;code&gt; (with &lt;code&gt;operation&lt;&#x2F;code&gt; labels for get, set, delete), &lt;code&gt;cache.hits&lt;&#x2F;code&gt; and &lt;code&gt;cache.misses&lt;&#x2F;code&gt; (for calculating hit ratio), &lt;code&gt;cache.size.entries&lt;&#x2F;code&gt; (current cached items), &lt;code&gt;cache.size.bytes&lt;&#x2F;code&gt; (memory usage), &lt;code&gt;cache.evictions&lt;&#x2F;code&gt; (items removed with &lt;code&gt;eviction_reason&lt;&#x2F;code&gt;), and &lt;code&gt;cache.operation.duration.ms&lt;&#x2F;code&gt; (time for operations). Hit ratio below 80% usually indicates problems: either you&#x27;re caching the wrong things, cache TTL is too short, or your working set exceeds cache capacity.&lt;&#x2F;p&gt;
&lt;p&gt;For lock and synchronization, monitor &lt;code&gt;locks.acquire.duration.ms&lt;&#x2F;code&gt; (time from requesting to getting lock), &lt;code&gt;locks.held.duration.ms&lt;&#x2F;code&gt; (how long locks are held), &lt;code&gt;locks.contention.count&lt;&#x2F;code&gt; (threads waiting), and &lt;code&gt;locks.timeouts.count&lt;&#x2F;code&gt; (failed acquisitions within timeout). Lock contention can kill your entire application, but it stays invisible without metrics. I&#x27;ve debugged more performance issues with lock metrics than almost any other single type. High acquisition times mean contention; long hold times suggest you&#x27;re doing too much work while holding the lock.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;real-world-instrumentation-patterns&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#real-world-instrumentation-patterns&quot; aria-label=&quot;Anchor link for: real-world-instrumentation-patterns&quot;&gt;🔗&lt;&#x2F;a&gt;Real-World Instrumentation Patterns&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;the-request-lifecycle-pattern&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-request-lifecycle-pattern&quot; aria-label=&quot;Anchor link for: the-request-lifecycle-pattern&quot;&gt;🔗&lt;&#x2F;a&gt;The Request Lifecycle Pattern&lt;&#x2F;h3&gt;
&lt;p&gt;For every user-facing operation, track the complete journey from entry to exit. This means instrumenting not just the success path, but every branch your code can take. Increment &lt;code&gt;api.requests.received&lt;&#x2F;code&gt; the moment a request hits your service, track &lt;code&gt;auth.attempts.count&lt;&#x2F;code&gt; and &lt;code&gt;auth.failures.count&lt;&#x2F;code&gt; separately to show both volume and failure rate, monitor &lt;code&gt;authorization.decisions.count&lt;&#x2F;code&gt; with labels for &lt;code&gt;granted&lt;&#x2F;code&gt; vs &lt;code&gt;denied&lt;&#x2F;code&gt;, measure &lt;code&gt;business_logic.duration.ms&lt;&#x2F;code&gt; to isolate your application logic performance, and record final &lt;code&gt;response.status_code&lt;&#x2F;code&gt; distribution to understand your error patterns. Most developers instrument the happy path but forget edge cases. A request that fails authentication never reaches your business logic, but it still uses resources and affects user experience. The real value comes from measuring what happens when things go wrong: network timeouts, validation errors, service dependencies failing.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-resource-exhaustion-pattern&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-resource-exhaustion-pattern&quot; aria-label=&quot;Anchor link for: the-resource-exhaustion-pattern&quot;&gt;🔗&lt;&#x2F;a&gt;The Resource Exhaustion Pattern&lt;&#x2F;h3&gt;
&lt;p&gt;Systems fail when they run out of resources. The trick is measuring resources before they&#x27;re completely exhausted, giving you time to react. For connection pools, track &lt;code&gt;db.connections.active&lt;&#x2F;code&gt; vs &lt;code&gt;db.connections.max&lt;&#x2F;code&gt; and don&#x27;t wait until you hit 100% utilization: start alerting at 85%. Monitor &lt;code&gt;db.connections.wait_time.ms&lt;&#x2F;code&gt; because long waits indicate you&#x27;re close to exhaustion even if you haven&#x27;t hit the limit. For memory pressure, monitor both &lt;code&gt;memory.heap.used.bytes&lt;&#x2F;code&gt; and &lt;code&gt;gc.frequency.per_minute&lt;&#x2F;code&gt; since high GC frequency often predicts memory pressure before OutOfMemory errors occur. Track &lt;code&gt;memory.allocation.rate.bytes_per_second&lt;&#x2F;code&gt; to understand if your allocation rate is sustainable. For queue management, a growing &lt;code&gt;jobs.queue.depth&lt;&#x2F;code&gt; indicates you&#x27;re processing work slower than it arrives, eventually leading to timeouts and system overload. Track &lt;code&gt;queue.processing.rate.per_second&lt;&#x2F;code&gt; and &lt;code&gt;queue.arrival.rate.per_second&lt;&#x2F;code&gt;: the relationship between these rates tells you if you&#x27;re keeping up. For disk space, track &lt;code&gt;disk.available.bytes&lt;&#x2F;code&gt; and &lt;code&gt;disk.usage.rate.bytes_per_hour&lt;&#x2F;code&gt;. Linear growth can be predicted and prevented, while sudden spikes indicate immediate problems.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-business-context-pattern&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-business-context-pattern&quot; aria-label=&quot;Anchor link for: the-business-context-pattern&quot;&gt;🔗&lt;&#x2F;a&gt;The Business Context Pattern&lt;&#x2F;h3&gt;
&lt;p&gt;Technical metrics tell you &lt;em&gt;what&lt;&#x2F;em&gt; is happening; business metrics tell you &lt;em&gt;why&lt;&#x2F;em&gt; it matters. Always pair technical instrumentation with business context to understand the real impact of technical problems. Track &lt;code&gt;api.errors.count&lt;&#x2F;code&gt; alongside &lt;code&gt;orders.lost.count&lt;&#x2F;code&gt; to understand how technical problems affect sales, monitor &lt;code&gt;payment_service.response_time.ms&lt;&#x2F;code&gt; alongside &lt;code&gt;checkout.abandonment.rate&lt;&#x2F;code&gt; to see if slow payments drive users away, and measure &lt;code&gt;search.response_time.ms&lt;&#x2F;code&gt; alongside &lt;code&gt;search.result_clicks.count&lt;&#x2F;code&gt; to understand if slow search reduces engagement. For user experience correlation, pair &lt;code&gt;cache.misses.count&lt;&#x2F;code&gt; with &lt;code&gt;page.load.time.ms&lt;&#x2F;code&gt; to quantify cache performance impact, track &lt;code&gt;db.slow_queries.count&lt;&#x2F;code&gt; alongside &lt;code&gt;user.session.duration.minutes&lt;&#x2F;code&gt; to see if database performance affects user retention, and monitor &lt;code&gt;auth.failures.count&lt;&#x2F;code&gt; with &lt;code&gt;support.tickets.count&lt;&#x2F;code&gt; to predict support load from technical issues. For capacity planning, correlate &lt;code&gt;server.cpu.usage.percent&lt;&#x2F;code&gt; with &lt;code&gt;concurrent.users.count&lt;&#x2F;code&gt; to understand scaling requirements, track &lt;code&gt;memory.usage.bytes&lt;&#x2F;code&gt; alongside &lt;code&gt;active.sessions.count&lt;&#x2F;code&gt; to predict memory needs, and monitor &lt;code&gt;network.bandwidth.used.mbps&lt;&#x2F;code&gt; with &lt;code&gt;file.uploads.count&lt;&#x2F;code&gt; to plan infrastructure scaling. This pairing helps you understand the business impact of technical problems and prioritize fixes based on actual user and revenue impact.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-error-classification-pattern&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-error-classification-pattern&quot; aria-label=&quot;Anchor link for: the-error-classification-pattern&quot;&gt;🔗&lt;&#x2F;a&gt;The Error Classification Pattern&lt;&#x2F;h3&gt;
&lt;p&gt;Not all errors are created equal. Classify errors by their impact and actionability to build appropriate response strategies. User errors (4xx) like &lt;code&gt;auth.invalid_credentials&lt;&#x2F;code&gt;, &lt;code&gt;validation.missing_field&lt;&#x2F;code&gt;, or &lt;code&gt;resource.not_found&lt;&#x2F;code&gt; are usually not your fault, but track patterns to identify UX issues. High rates might indicate confusing interfaces or inadequate client-side validation; alert on unusual spikes that might indicate attacks or system confusion. System errors (5xx) like &lt;code&gt;db.connection_timeout&lt;&#x2F;code&gt;, &lt;code&gt;service.unavailable&lt;&#x2F;code&gt;, or &lt;code&gt;memory.exhausted&lt;&#x2F;code&gt; are your responsibility to fix immediately. They&#x27;re always actionable and usually indicate infrastructure or code problems that should trigger immediate alerts and investigation. External dependency errors like &lt;code&gt;payment_gateway.timeout&lt;&#x2F;code&gt;, &lt;code&gt;third_party_api.rate_limited&lt;&#x2F;code&gt;, or &lt;code&gt;cdn.unavailable&lt;&#x2F;code&gt; are outside your direct control but affect users. They require fallback strategies and user communication, and help predict when to escalate with external providers. Distinguish transient errors (&lt;code&gt;network.timeout&lt;&#x2F;code&gt;, &lt;code&gt;rate_limit.exceeded&lt;&#x2F;code&gt; that often resolve themselves) from persistent errors (&lt;code&gt;config.invalid&lt;&#x2F;code&gt;, &lt;code&gt;database.schema_mismatch&lt;&#x2F;code&gt; that require immediate intervention). Each category needs different alerting strategies, escalation procedures, and response timeframes: user errors might warrant daily review, system errors need immediate alerts, external errors require monitoring trends and fallback activation.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;best-practices-for-production-metrics&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#best-practices-for-production-metrics&quot; aria-label=&quot;Anchor link for: best-practices-for-production-metrics&quot;&gt;🔗&lt;&#x2F;a&gt;Best Practices for Production Metrics&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;naming-conventions-that-scale&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#naming-conventions-that-scale&quot; aria-label=&quot;Anchor link for: naming-conventions-that-scale&quot;&gt;🔗&lt;&#x2F;a&gt;Naming Conventions That Scale&lt;&#x2F;h3&gt;
&lt;p&gt;Consistent naming prevents the confusion that kills metrics adoption. Use a clear hierarchy: &lt;code&gt;&amp;lt;system&amp;gt;.&amp;lt;component&amp;gt;.&amp;lt;operation&amp;gt;.&amp;lt;metric_type&amp;gt;&lt;&#x2F;code&gt;. Examples include &lt;code&gt;api.auth.requests.count&lt;&#x2F;code&gt;, &lt;code&gt;db.user_queries.duration.ms&lt;&#x2F;code&gt;, &lt;code&gt;cache.metadata.hits.total&lt;&#x2F;code&gt;, and &lt;code&gt;queue.order_processing.messages.consumed&lt;&#x2F;code&gt;. Standardize your suffixes: &lt;code&gt;.count&#x2F;.total&lt;&#x2F;code&gt; for event counters, &lt;code&gt;.current&#x2F;.active&lt;&#x2F;code&gt; for current gauge values, &lt;code&gt;.duration&#x2F;.ms&#x2F;.seconds&lt;&#x2F;code&gt; for time measurements, &lt;code&gt;.bytes&#x2F;.mb&#x2F;.gb&lt;&#x2F;code&gt; for data volume, &lt;code&gt;.errors&#x2F;.failures&lt;&#x2F;code&gt; for error counters, and &lt;code&gt;.ratio&#x2F;.rate&lt;&#x2F;code&gt; for ratios and rates. Avoid mixing naming styles (&lt;code&gt;requestCount&lt;&#x2F;code&gt; vs &lt;code&gt;request_total&lt;&#x2F;code&gt;), ambiguous units (&lt;code&gt;response_time&lt;&#x2F;code&gt; without units), and inconsistent hierarchies (&lt;code&gt;api_requests&lt;&#x2F;code&gt; vs &lt;code&gt;requests.api&lt;&#x2F;code&gt;). This consistency pays off during 3 AM troubleshooting when you don&#x27;t want to waste mental energy remembering naming schemes.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;critical-mistakes-to-avoid&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#critical-mistakes-to-avoid&quot; aria-label=&quot;Anchor link for: critical-mistakes-to-avoid&quot;&gt;🔗&lt;&#x2F;a&gt;Critical Mistakes to Avoid&lt;&#x2F;h3&gt;
&lt;p&gt;The biggest mistake is using unbounded values as labels. Don&#x27;t tag metrics with user IDs, session tokens, IP addresses, or other unlimited values; your metrics system will eventually explode from too many unique series. Use &lt;code&gt;api.requests{user_type=&quot;premium&quot;, region=&quot;us-west&quot;}&lt;&#x2F;code&gt; instead of &lt;code&gt;api.requests{user_id=&quot;12345&quot;, session=&quot;abc123xyz&quot;}&lt;&#x2F;code&gt;. Always instrument failure cases, not just success paths. Track both &lt;code&gt;payments.succeeded&lt;&#x2F;code&gt; AND &lt;code&gt;payments.failed&lt;&#x2F;code&gt; with error type labels, monitor &lt;code&gt;auth.attempts&lt;&#x2F;code&gt; alongside &lt;code&gt;auth.failures&lt;&#x2F;code&gt; to understand failure rates, and count &lt;code&gt;file.uploads.completed&lt;&#x2F;code&gt; and &lt;code&gt;file.uploads.failed&lt;&#x2F;code&gt; to see processing reliability. Every metric has a cost in storage, network bandwidth, and cognitive load. If you can&#x27;t explain why a metric matters for operations or business decisions, skip it. Ask yourself: &quot;Would this metric help me during an incident?&quot; Metrics that stop updating can be worse than no metrics at all. Always include heartbeat or health check metrics to verify your instrumentation is working; track &lt;code&gt;metrics.last_updated.timestamp&lt;&#x2F;code&gt; to detect collection failures.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;a-note-on-histograms-or-why-they-re-not-here&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#a-note-on-histograms-or-why-they-re-not-here&quot; aria-label=&quot;Anchor link for: a-note-on-histograms-or-why-they-re-not-here&quot;&gt;🔗&lt;&#x2F;a&gt;A Note on Histograms (Or: Why They&#x27;re Not Here)&lt;&#x2F;h2&gt;
&lt;p&gt;I know what some of you are thinking: &quot;Where are the histograms?&quot; After all, this is a comprehensive guide to application metrics, and histograms are everywhere in monitoring discussions. Well, I deliberately left them out, and here&#x27;s why.&lt;&#x2F;p&gt;
&lt;p&gt;Prometheus histograms are fundamentally broken in ways that make them more dangerous than useful. The core problem is what I call the bucket pre-configuration paradox: you must define bucket boundaries before you know your data distribution. As LinuxCzar eloquently put it in his &lt;a href=&quot;https:&#x2F;&#x2F;linuxczar.net&#x2F;blog&#x2F;2017&#x2F;06&#x2F;15&#x2F;prometheus-histogram-2&#x2F;&quot;&gt;&quot;tale of woe&quot;&lt;&#x2F;a&gt;, this creates an impossible choice between accuracy (many buckets) and operability (few buckets). Get it wrong, and you either lose precision or crash your Prometheus server with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;discussions&#x2F;10598&quot;&gt;cardinality explosion&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;But the problems run deeper. You &lt;a href=&quot;https:&#x2F;&#x2F;www.solarwinds.com&#x2F;blog&#x2F;why-percentiles-dont-work-the-way-you-think&quot;&gt;mathematically cannot aggregate percentiles&lt;&#x2F;a&gt; across instances because the underlying event data is lost. The linear interpolation algorithm produces &lt;a href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;docs&#x2F;practices&#x2F;histograms&#x2F;&quot;&gt;significant estimation errors&lt;&#x2F;a&gt;, and Prometheus&#x27;s scraping architecture introduces &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;issues&#x2F;1887&quot;&gt;data corruption&lt;&#x2F;a&gt; where histogram buckets update inconsistently. The &lt;a href=&quot;https:&#x2F;&#x2F;chronosphere.io&#x2F;learn&#x2F;histograms-for-complex-systems&#x2F;&quot;&gt;operational burden&lt;&#x2F;a&gt; never ends: every performance improvement potentially invalidates your bucket choices, forcing constant manual reconfiguration.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;catwell.info&quot;&gt;Pierre Chapuis&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;catwell.info&#x2F;post&#x2F;3lzxliivegs2k&quot;&gt;pointed out&lt;&#x2F;a&gt; the root cause I missed: Prometheus implements an outdated 2005 algorithm from Cormode et al. for histogram summaries and quantiles. There are much better algorithms available now, including improved versions from the same authors. Check out &lt;a href=&quot;https:&#x2F;&#x2F;cs.uwaterloo.ca&#x2F;~kdaudjee&#x2F;Daudjee_Sketches.pdf&quot;&gt;this paper&lt;&#x2F;a&gt; for a good overview of modern sketch algorithms. The estimation errors and operational problems I described are symptoms of using this old algorithm.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Instead, I prefer the combination of simple counters and gauges paired with distributed tracing. Trace-derived global metrics give you actual data distributions without guessing bucket boundaries, eliminate the aggregation problem by preserving request context, and adapt automatically as your system evolves. You get better insights with less operational overhead, which seems like a better deal to me.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;quick-reference-metrics-by-component&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#quick-reference-metrics-by-component&quot; aria-label=&quot;Anchor link for: quick-reference-metrics-by-component&quot;&gt;🔗&lt;&#x2F;a&gt;Quick Reference: Metrics by Component&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Component&lt;&#x2F;th&gt;&lt;th&gt;Essential Metrics&lt;&#x2F;th&gt;&lt;th&gt;Purpose&lt;&#x2F;th&gt;&lt;th&gt;Key Labels&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;API Endpoints&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;api.requests.total&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;api.response_time.ms&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;api.errors.count&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;auth.failures.count&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Track every request lifecycle&lt;br&gt;Monitor user-facing performance&lt;br&gt;Catch errors before users complain&lt;br&gt;Security monitoring&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;method&lt;&#x2F;code&gt;, &lt;code&gt;endpoint&lt;&#x2F;code&gt;, &lt;code&gt;status_code&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;endpoint&lt;&#x2F;code&gt;, &lt;code&gt;user_type&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;error_type&lt;&#x2F;code&gt;, &lt;code&gt;endpoint&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;failure_reason&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Database&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;db.connections.active&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;db.queries.executed&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;db.query.duration.ms&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;db.errors.count&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;db.slow_queries.count&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Prevent connection exhaustion&lt;br&gt;Track database usage patterns&lt;br&gt;Identify performance bottlenecks&lt;br&gt;Monitor database health&lt;br&gt;Catch expensive queries&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;pool_name&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;operation_type&lt;&#x2F;code&gt;, &lt;code&gt;table&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;operation_type&lt;&#x2F;code&gt;, &lt;code&gt;table&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;error_type&lt;&#x2F;code&gt;, &lt;code&gt;operation&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;table&lt;&#x2F;code&gt;, &lt;code&gt;query_type&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Message Queues&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;queue.messages.produced&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;queue.messages.consumed&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;queue.processing.time.ms&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;queue.processing.errors&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;jobs.queue.depth&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Track producer health&lt;br&gt;Monitor consumer throughput&lt;br&gt;Identify processing bottlenecks&lt;br&gt;Catch processing failures&lt;br&gt;Detect backlog buildup&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;topic&lt;&#x2F;code&gt;, &lt;code&gt;producer_id&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;topic&lt;&#x2F;code&gt;, &lt;code&gt;consumer_group&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;topic&lt;&#x2F;code&gt;, &lt;code&gt;message_type&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;error_type&lt;&#x2F;code&gt;, &lt;code&gt;topic&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;queue_name&lt;&#x2F;code&gt;, &lt;code&gt;priority&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Cache&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;cache.requests.total&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;cache.hits&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;cache.misses&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;cache.size.entries&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;cache.evictions&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Monitor cache usage&lt;br&gt;Track cache effectiveness&lt;br&gt;Identify cache problems&lt;br&gt;Monitor memory usage&lt;br&gt;Understand eviction patterns&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;cache_type&lt;&#x2F;code&gt;, &lt;code&gt;operation&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;cache_type&lt;&#x2F;code&gt;, &lt;code&gt;key_prefix&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;cache_type&lt;&#x2F;code&gt;, &lt;code&gt;miss_reason&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;cache_type&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;cache_type&lt;&#x2F;code&gt;, &lt;code&gt;eviction_reason&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Locks&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;locks.acquire.duration.ms&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;locks.held.duration.ms&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;locks.contention.count&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Detect lock contention&lt;br&gt;Find locks held too long&lt;br&gt;Monitor thread blocking&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;lock_name&lt;&#x2F;code&gt;, &lt;code&gt;thread_type&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;lock_name&lt;&#x2F;code&gt;, &lt;code&gt;operation&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;lock_name&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Business&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;orders.placed&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;users.login&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;payments.processed&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;feature.usage.count&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;workflow.state_changes&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;Business KPI tracking&lt;br&gt;User activity monitoring&lt;br&gt;Revenue stream health&lt;br&gt;Feature adoption metrics&lt;br&gt;Process flow monitoring&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;user_type&lt;&#x2F;code&gt;, &lt;code&gt;order_value_range&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;user_type&lt;&#x2F;code&gt;, &lt;code&gt;login_method&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;payment_method&lt;&#x2F;code&gt;, &lt;code&gt;amount_range&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;feature_name&lt;&#x2F;code&gt;, &lt;code&gt;user_segment&lt;&#x2F;code&gt;&lt;br&gt;&lt;code&gt;workflow_name&lt;&#x2F;code&gt;, &lt;code&gt;from_state&lt;&#x2F;code&gt;, &lt;code&gt;to_state&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#conclusion&quot; aria-label=&quot;Anchor link for: conclusion&quot;&gt;🔗&lt;&#x2F;a&gt;Conclusion&lt;&#x2F;h2&gt;
&lt;p&gt;Effective metrics instrumentation isn&#x27;t about collecting everything; it&#x27;s about collecting the right things in the right places. Start with the five essential metric types, instrument your critical components, and build from there.&lt;&#x2F;p&gt;
&lt;p&gt;Think about metrics as part of your application design, not an afterthought. When writing code, ask yourself: &quot;How will I know if this is working correctly in production?&quot; The answer guides your instrumentation decisions.&lt;&#x2F;p&gt;
&lt;p&gt;The best observability system helps you sleep better at night. If your metrics aren&#x27;t giving you confidence in your system&#x27;s health, you&#x27;re measuring the wrong things.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Feel free to reach out with any questions or to share your experiences with application metrics. You can find me on &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;PierreZ&quot;&gt;Twitter&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;pierrezemb.fr&quot;&gt;Bluesky&lt;&#x2F;a&gt; or through my &lt;a href=&quot;https:&#x2F;&#x2F;pierrezemb.fr&quot;&gt;website&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</description>
          <category domain="tag">observability</category>
          <category domain="tag">metrics</category>
          <category domain="tag">monitoring</category>
          <category domain="tag">distributed-systems</category>
      </item>
      <item>
          <title>Shipped vs. Operated, or How Many Bash Scripts Does It Take?</title>
          <pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate>
          <author>Pierre Zemb</author>
          <link>https://pierrezemb.fr/posts/shipped-vs-operated/</link>
          <guid>https://pierrezemb.fr/posts/shipped-vs-operated/</guid>
          <description xml:base="https://pierrezemb.fr/posts/shipped-vs-operated/">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;&#x2F;strong&gt; The difference between shipped and operated software is the difference between something you can run and forget, and something that demands ongoing, hands-on care. Choosing the former protects your team’s focus and sanity.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;the-shipped-vs-operated-spectrum&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-shipped-vs-operated-spectrum&quot; aria-label=&quot;Anchor link for: the-shipped-vs-operated-spectrum&quot;&gt;🔗&lt;&#x2F;a&gt;The Shipped vs. Operated Spectrum&lt;&#x2F;h2&gt;
&lt;p&gt;Some technologies arrive as complete systems: you deploy them, give them minimal care, and they quietly do their job. Others arrive like complex machines: powerful, but demanding regular attention and maintenance. That’s the difference between &lt;em&gt;shipped&lt;&#x2F;em&gt; and &lt;em&gt;operated&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The distinction isn’t just about features; it’s about the level of operational effort the system will demand over its lifetime. &lt;strong&gt;Operated&lt;&#x2F;strong&gt; technologies require continuous human care to stay healthy. They age, drift, and accumulate operational quirks. They often have sharp edges you only discover at 2 a.m., and when something goes wrong, you need people who already know the failure modes by heart. Think of a self-managed &lt;strong&gt;HBase&lt;&#x2F;strong&gt; or a ZooKeeper ensemble that you &lt;em&gt;really&lt;&#x2F;em&gt; hope never splits brain.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Shipped&lt;&#x2F;strong&gt; technologies are built to reduce that constant overhead. They can still fail, but they tend to fail in ways that are predictable, recoverable, and not existential. You can learn them as you go. Your outages will be frustrating, but they won’t demand a dedicated handler on payroll. &lt;strong&gt;FoundationDB&lt;&#x2F;strong&gt; is a good example: it’s not magic, but its operational surface area is small enough to fit in a single human brain.&lt;&#x2F;p&gt;
&lt;p&gt;For contrast, I’ve also spent years with the other kind: &lt;strong&gt;HBase&lt;&#x2F;strong&gt; clusters spread over 250+ nodes, &lt;strong&gt;Ceph&lt;&#x2F;strong&gt;, &lt;strong&gt;Kafka&lt;&#x2F;strong&gt; and &lt;strong&gt;ZooKeeper&lt;&#x2F;strong&gt; in various configurations, &lt;strong&gt;Pulsar&lt;&#x2F;strong&gt;, &lt;strong&gt;Warp10&lt;&#x2F;strong&gt;, &lt;strong&gt;etcd&lt;&#x2F;strong&gt;, &lt;strong&gt;Kubernetes&lt;&#x2F;strong&gt;, &lt;strong&gt;Flink&lt;&#x2F;strong&gt;, and &lt;strong&gt;RabbitMQ&lt;&#x2F;strong&gt;, each with its own set of operational “adventures.”&lt;&#x2F;p&gt;
&lt;h2 id=&quot;identifying-operated-systems&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#identifying-operated-systems&quot; aria-label=&quot;Anchor link for: identifying-operated-systems&quot;&gt;🔗&lt;&#x2F;a&gt;Identifying Operated Systems&lt;&#x2F;h2&gt;
&lt;p&gt;Some systems live in both worlds depending on how you use them. &lt;strong&gt;PostgreSQL&lt;&#x2F;strong&gt; in standalone mode is usually shipped: it’s simple to run, predictable, and rarely causes surprises. But under certain conditions, like fighting vacuum performance at scale or running it in HA mode under sustained heavy load, it shifts into operated territory. The difference isn’t in the codebase, but in the demands your use case puts on it.&lt;&#x2F;p&gt;
&lt;p&gt;A quick way to tell which camp your system belongs to is the &lt;strong&gt;Bash Script Test&lt;&#x2F;strong&gt;: ask how many bash scripts or home-grown tools are required to survive an on-call shift. If the answer includes a collection of automation to clean up data, shuffle it between nodes, or probe the cluster’s health, you’re probably in operated territory. I’ve been there: running &lt;code&gt;hbck&lt;&#x2F;code&gt; and manually moving regions in &lt;strong&gt;HBase&lt;&#x2F;strong&gt;, shuffling partitions around in &lt;strong&gt;Kafka&lt;&#x2F;strong&gt; to balance load, or triggering repairs in &lt;strong&gt;Ceph&lt;&#x2F;strong&gt; after failed scrub errors. Many distributed systems quietly rely on these manual interventions, often run weekly, to stay healthy, and that’s an operational cost you can’t ignore.&lt;&#x2F;p&gt;
&lt;p&gt;By contrast, we have &lt;strong&gt;no&lt;&#x2F;strong&gt; such scripts for &lt;strong&gt;FoundationDB&lt;&#x2F;strong&gt;, and that’s exactly why it feels shipped.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-strategic-cost-of-operations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-strategic-cost-of-operations&quot; aria-label=&quot;Anchor link for: the-strategic-cost-of-operations&quot;&gt;🔗&lt;&#x2F;a&gt;The Strategic Cost of Operations&lt;&#x2F;h2&gt;
&lt;p&gt;Each operated system consumes a slice of your team’s focus. Add too many, and you’ll spend more time keeping the lights on than moving forward. The more you can choose robust, low-maintenance software, the more space you keep for actually building new things.&lt;&#x2F;p&gt;
&lt;p&gt;I’m not a fan of Kubernetes from an operational perspective. But it does something important for end users: it gives them a standard way to write software that reacts to the state of the infrastructure through &lt;a href=&quot;https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;extend-kubernetes&#x2F;operator&#x2F;&quot;&gt;Operators&lt;&#x2F;a&gt;. Operators turn that into continuous automation, with a reconciliation loop that keeps drifting systems aligned with the desired state. It’s a way to bake SRE knowledge into code, so even complex systems can be run and handed over without months of hand-holding.&lt;&#x2F;p&gt;
&lt;p&gt;The stakes are only going to get higher as LLMs become a common tool for software engineers. We’ll inevitably build more advanced and complex systems, but that complexity doesn’t disappear; it gets pushed to the people on call. LLMs are good at fixing failures that are reproducible and deterministic, because they can alter the system freely, but most on-call incidents aren’t like that. The only way to keep operational load sustainable is to change how we design and test: building for robustness from the start, and using techniques like &lt;a href=&quot;&#x2F;posts&#x2F;simulation-driven-development&#x2F;&quot;&gt;simulation-driven development&lt;&#x2F;a&gt; to expose failure modes before they reach production.&lt;&#x2F;p&gt;
&lt;p&gt;If you can, choose the system you can deploy and leave alone, not the complex machine that demands your weekends.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Feel free to reach out with any questions or to share your experiences with shipped&#x2F;operated software. You can find me on &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;PierreZ&quot;&gt;Twitter&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;pierrezemb.fr&quot;&gt;Bluesky&lt;&#x2F;a&gt; or through my &lt;a href=&quot;https:&#x2F;&#x2F;pierrezemb.fr&quot;&gt;website&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</description>
          <category domain="tag">distributed-systems</category>
          <category domain="tag">operation</category>
      </item>
      <item>
          <title>Two Podcast Episodes on Topics Developers Rarely Talk About</title>
          <pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate>
          <author>Pierre Zemb</author>
          <link>https://pierrezemb.fr/posts/debugging-and-correctness-podcasts/</link>
          <guid>https://pierrezemb.fr/posts/debugging-and-correctness-podcasts/</guid>
          <description xml:base="https://pierrezemb.fr/posts/debugging-and-correctness-podcasts/">&lt;p&gt;I was listening to a couple of podcasts the other day and stumbled across two episodes that were so compelling I had to stop my chores and listen. They dive into corners of software engineering that most developers barely think about; not because they’re unimportant, but because they appear in the hard corners of engineering:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;catastrophic data corruption,&lt;&#x2F;li&gt;
&lt;li&gt;correctness work done before a single line is shipped.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The first is &lt;a href=&quot;https:&#x2F;&#x2F;oxide-and-friends.transistor.fm&#x2F;episodes&#x2F;adventures-in-data-corruption&quot;&gt;Adventures in Data Corruption&lt;&#x2F;a&gt; from &lt;em&gt;Oxide and Friends&lt;&#x2F;em&gt;. Two years ago, the Oxide team ran into data corruption during what should have been a routine network transfer. The debugging journey that followed went from packet traces to CPU speculation quirks, peeling back the stack layer by layer, hardware, kernel, network, application, asking hard questions at each step. What I love here is the combination of clear storytelling and the rapid-fire hypotheses: they make an assumption, test it, discard it, and immediately move to the next, pulling you along in the investigation until the root cause finally clicks into place.&lt;&#x2F;p&gt;
&lt;p&gt;The second is &lt;a href=&quot;https:&#x2F;&#x2F;x.com&#x2F;AntithesisHQ&#x2F;status&#x2F;1953097721205710918&quot;&gt;Scaling Correctness: Marc Brooker on a Decade of Formal Methods at AWS&lt;&#x2F;a&gt; of &lt;em&gt;The BugBash Podcast&lt;&#x2F;em&gt; by Antithesis. Marc Brooker, who has spent nearly 17 years building core AWS services like S3 and Lambda, shares the company’s decade-long journey with formal methods, from heavyweight tools like TLA+ to the &lt;em&gt;lightweight&lt;&#x2F;em&gt; approaches that any team can adopt like &lt;a href=&quot;&#x2F;tags&#x2F;simulation&quot;&gt;simulation-based testing&lt;&#x2F;a&gt;. At AWS, they’ve learned that investing in correctness up front not only improves reliability but actually speeds up delivery. They also touch on deterministic simulation testing, the challenge of verifying UIs and control planes, and the role AI might play in the future of verification.&lt;&#x2F;p&gt;
&lt;p&gt;I’ve been paged way too many times for metastable failures, data corruption, network meltdowns, or NTP drift in production. These days, I’d rather tackle the correctness part &lt;em&gt;before&lt;&#x2F;em&gt; those alarms go off. Every new layer I build is designed to be simulated to explore failure modes in a controlled environment before they can hurt real users.&lt;&#x2F;p&gt;
&lt;p&gt;But when things fall apart anyway, and spoilers &lt;strong&gt;they will&lt;&#x2F;strong&gt;, developers have the opportunity to truly understand their software. Being responsible for the systems you build means you’re the one getting paged, and it’s in those moments of crisis that the sharpest debugging skills are forged.&lt;&#x2F;p&gt;
&lt;p&gt;So don’t just bookmark them. Put them at the top of your queue. Listen. And maybe, the next time your system misbehaves, you’ll be ready.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Feel free to reach out with any questions or to share your experiences with debugging and correctness. You can find me on &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;PierreZ&quot;&gt;Twitter&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;pierrezemb.fr&quot;&gt;Bluesky&lt;&#x2F;a&gt; or through my &lt;a href=&quot;https:&#x2F;&#x2F;pierrezemb.fr&quot;&gt;website&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</description>
          <category domain="tag">distributed-systems</category>
          <category domain="tag">debugging</category>
          <category domain="tag">correctness</category>
          <category domain="tag">podcasts</category>
          <category domain="tag">simulation</category>
      </item>
      <item>
          <title>Bypassing FoundationDB&#x27;s Transaction Limits with Record Layer Continuations</title>
          <pubDate>Tue, 03 Jun 2025 00:30:00 +0200</pubDate>
          <author>Pierre Zemb</author>
          <link>https://pierrezemb.fr/posts/understanding-fdb-record-layer-continuations/</link>
          <guid>https://pierrezemb.fr/posts/understanding-fdb-record-layer-continuations/</guid>
          <description xml:base="https://pierrezemb.fr/posts/understanding-fdb-record-layer-continuations/">&lt;h2 id=&quot;introducing-the-foundationdb-record-layer&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#introducing-the-foundationdb-record-layer&quot; aria-label=&quot;Anchor link for: introducing-the-foundationdb-record-layer&quot;&gt;🔗&lt;&#x2F;a&gt;Introducing the FoundationDB Record Layer&lt;&#x2F;h2&gt;
&lt;p&gt;Before we dive into the specifics of handling large operations with continuations (the main topic of this post), let&#x27;s briefly introduce the &lt;a href=&quot;https:&#x2F;&#x2F;foundationdb.github.io&#x2F;fdb-record-layer&#x2F;index.html&quot;&gt;&lt;strong&gt;FoundationDB Record Layer&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;. It&#x27;s a powerful open-source library built atop FoundationDB that brings a structured, record-oriented data model to FDB&#x27;s highly scalable key-value store. Think of it as adding schema management, rich indexing capabilities, and a sophisticated query engine, making it easier to build complex applications.&lt;&#x2F;p&gt;
&lt;p&gt;The Record Layer is versatile and has been adopted for demanding use-cases, most notably by Apple as the core of CloudKit, powering services for millions of users. It allows developers to define their data models using Protocol Buffers and then query them in a flexible manner.&lt;&#x2F;p&gt;
&lt;p&gt;For instance, you can express queries like finding all &#x27;Order&#x27; records for roses costing less than $50 with a declarative API (example in Java):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#2b303b;color:#c0c5ce;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span style=&quot;color:#ebcb8b;&quot;&gt;RecordQuery&lt;&#x2F;span&gt;&lt;span&gt; query = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ebcb8b;&quot;&gt;RecordQuery&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;newBuilder&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;setRecordType&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;Order&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;)
&lt;&#x2F;span&gt;&lt;span&gt;        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;setFilter&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ebcb8b;&quot;&gt;Query&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;and&lt;&#x2F;span&gt;&lt;span&gt;(
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ebcb8b;&quot;&gt;Query&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;field&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;price&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;lessThan&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;50&lt;&#x2F;span&gt;&lt;span&gt;),
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ebcb8b;&quot;&gt;Query&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;field&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;flower&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;matches&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ebcb8b;&quot;&gt;Query&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;field&lt;&#x2F;span&gt;&lt;span&gt;(&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#a3be8c;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;equalsValue&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ebcb8b;&quot;&gt;FlowerType&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d08770;&quot;&gt;ROSE&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;()))))
&lt;&#x2F;span&gt;&lt;span&gt;        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bf616a;&quot;&gt;build&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To get started and explore its capabilities further, the official &lt;a href=&quot;https:&#x2F;&#x2F;foundationdb.github.io&#x2F;fdb-record-layer&#x2F;GettingStarted.html&quot;&gt;Getting Started Guide&lt;&#x2F;a&gt; is an excellent resource. You can also watch these talks for a deeper understanding:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SvoUHHM9IKU&quot;&gt;Using FoundationDB and the FDB Record Layer to Build CloudKit - Scott Gray, Apple&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=HLE8chgw6LI&quot;&gt;FoundationDB Record Layer: Open Source Structured Storage on FoundationDB - Nicholas Schiefer, Apple&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;blockquote&gt;
&lt;p&gt;For a detailed academic perspective on its design and how CloudKit uses it, refer to the &lt;a href=&quot;https:&#x2F;&#x2F;www.foundationdb.org&#x2F;files&#x2F;record-layer-paper.pdf&quot;&gt;SIGMOD&#x27;19 paper: FoundationDB Record Layer: A Multi-Tenant Structured Datastore&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;the-challenge-fdb-s-transaction-constraints&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-challenge-fdb-s-transaction-constraints&quot; aria-label=&quot;Anchor link for: the-challenge-fdb-s-transaction-constraints&quot;&gt;🔗&lt;&#x2F;a&gt;The Challenge: FDB&#x27;s Transaction Constraints&lt;&#x2F;h2&gt;
&lt;p&gt;FoundationDB (FDB) imposes strict constraints on its transactions: they must complete within 5 seconds and are limited to 10MB of manipulated data, either writes or reads. These constraints are fundamental to FDB&#x27;s design, ensuring high performance and serializable isolation. However, they pose a significant challenge for operations that inherently require processing large datasets or executing complex queries that cannot complete within these tight boundaries, such as full table scans, large analytical queries, or bulk data exports.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;FoundationDB Record Layer&lt;&#x2F;strong&gt; addresses this challenge through a mechanism known as &lt;strong&gt;continuations&lt;&#x2F;strong&gt;. Continuations allow a single logical operation to be broken down into a sequence of smaller, independent FDB transactions. Each transaction processes a segment of the total workload and, if more work remains, yields a &lt;strong&gt;continuation token&lt;&#x2F;strong&gt;. This opaque token encapsulates the state required to resume the operation precisely where the previous transaction left off.&lt;&#x2F;p&gt;
&lt;p&gt;This article delves into the technical details of Record Layer continuations, exploring how they function and how to leverage them effectively to build robust, scalable applications on FDB.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bridging-transactions-the-role-of-continuations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#bridging-transactions-the-role-of-continuations&quot; aria-label=&quot;Anchor link for: bridging-transactions-the-role-of-continuations&quot;&gt;🔗&lt;&#x2F;a&gt;Bridging Transactions: The Role of Continuations&lt;&#x2F;h2&gt;
&lt;p&gt;Consider a query to retrieve all records matching a specific filter from a large dataset. Executing this as a single FDB transaction would likely violate the 5-second or 10MB limit. The Record Layer employs continuations to serialize this operation across multiple transactions:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initial Request:&lt;&#x2F;strong&gt; The application initiates a query against the Record Layer.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Segmented Execution:&lt;&#x2F;strong&gt; The Record Layer&#x27;s query planner executes the query, but with built-in scan limiters. It processes records until a predefined limit (e.g., row count, time duration, or byte size) is approached, or it nears FDB&#x27;s intrinsic transaction limits.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;State Serialization:&lt;&#x2F;strong&gt; Before the current FDB transaction commits, if the logical operation is incomplete, the Record Layer serializes the execution state of the query plan into a continuation token.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Partial Result &amp;amp; Token:&lt;&#x2F;strong&gt; The application receives the processed segment of data and the continuation token. The FDB transaction for this segment commits successfully.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Resumption:&lt;&#x2F;strong&gt; To fetch the next segment, the application submits a new request, providing the previously received continuation token.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;State Deserialization &amp;amp; Continued Execution:&lt;&#x2F;strong&gt; The Record Layer deserializes the token, restores the query plan&#x27;s state, and resumes execution from the exact point it paused. This typically involves adjusting scan boundaries (e.g., starting a key-range scan from the key after the last one processed).&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;This cycle repeats until the entire logical operation is complete. The continuation token acts as the critical link, enabling a series of short, FDB-compliant transactions to collectively achieve the effect of a single, long-running operation without violating FDB&#x27;s core constraints.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dissecting-the-continuation-token&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#dissecting-the-continuation-token&quot; aria-label=&quot;Anchor link for: dissecting-the-continuation-token&quot;&gt;🔗&lt;&#x2F;a&gt;Dissecting the Continuation Token&lt;&#x2F;h2&gt;
&lt;p&gt;While the continuation token is &lt;strong&gt;opaque&lt;&#x2F;strong&gt; to the application (it&#x27;s a &lt;code&gt;byte[]&lt;&#x2F;code&gt; that should not be introspected or modified), it internally contains structured information vital for resuming query execution. The exact format is an implementation detail of the Record Layer and can evolve, but conceptually, it must capture:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scan Boundaries:&lt;&#x2F;strong&gt; The key (or keys, for multi-dimensional indexes) where the next scan segment should begin. This ensures no data is missed or re-processed unnecessarily.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Query Plan State:&lt;&#x2F;strong&gt; For complex query plans involving joins, filters, aggregations, or in-memory sorting, the token may need to store intermediate state specific to those operators. For instance, a &lt;code&gt;UnionPlan&lt;&#x2F;code&gt; or &lt;code&gt;IntersectionPlan&lt;&#x2F;code&gt; might need to remember which child plan was active and its respective continuation.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Scan Limiter State:&lt;&#x2F;strong&gt; Information about accumulated counts or sizes if the scan was paused due to application-defined limits rather than FDB limits.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Version Information:&lt;&#x2F;strong&gt; To ensure compatibility if the token format changes across Record Layer versions.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The opacity of the token is a deliberate design choice. It decouples the application from the internal mechanics of the Record Layer, allowing the latter to evolve its continuation strategies (e.g., for efficiency or new features) without breaking client applications. The application&#x27;s responsibility is solely to store and return this token verbatim.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;resuming-query-execution-via-continuations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#resuming-query-execution-via-continuations&quot; aria-label=&quot;Anchor link for: resuming-query-execution-via-continuations&quot;&gt;🔗&lt;&#x2F;a&gt;Resuming Query Execution via Continuations&lt;&#x2F;h2&gt;
&lt;p&gt;When a continuation token is provided to a &lt;code&gt;RecordCursor&lt;&#x2F;code&gt; (the Record Layer&#x27;s abstraction for iterating over query results), the underlying &lt;code&gt;RecordQueryPlan&lt;&#x2F;code&gt; uses it to reconstruct its state.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Plan Identification:&lt;&#x2F;strong&gt; The token typically identifies the specific query plan or sub-plan it pertains to.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;State Restoration:&lt;&#x2F;strong&gt; Each operator in the query plan (e.g., &lt;code&gt;IndexScanPlan&lt;&#x2F;code&gt;, &lt;code&gt;FilterPlan&lt;&#x2F;code&gt;, &lt;code&gt;SortPlan&lt;&#x2F;code&gt;) that can be stateful across transaction boundaries implements logic to initialize itself from the continuation. For an &lt;code&gt;IndexScanPlan&lt;&#x2F;code&gt;, this primarily means setting the &lt;code&gt;ScanComparisons&lt;&#x2F;code&gt; for the next range read. For a &lt;code&gt;UnionPlan&lt;&#x2F;code&gt;, it might mean restoring the continuation for one of its child plans and indicating which child to resume.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Execution Resumption:&lt;&#x2F;strong&gt; Once the plan&#x27;s state is restored, the &lt;code&gt;RecordCursor&lt;&#x2F;code&gt; can proceed to fetch the next batch of records. The execution effectively &quot;jumps&quot; to the point encoded in the continuation.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;This mechanism allows the Record Layer to transparently manage the complexities of distributed, stateful iteration over potentially vast datasets, all while adhering to FDB&#x27;s transactional model.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;implications-of-non-atomicity&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#implications-of-non-atomicity&quot; aria-label=&quot;Anchor link for: implications-of-non-atomicity&quot;&gt;🔗&lt;&#x2F;a&gt;Implications of Non-Atomicity&lt;&#x2F;h2&gt;
&lt;p&gt;It&#x27;s important to understand a key implication of this multi-transaction approach: while each individual FDB transaction executed as part of a continued operation is atomic and isolated (typically providing serializable isolation), the overall logical operation spanning multiple continuations is &lt;strong&gt;not atomic&lt;&#x2F;strong&gt; in the same way. Mutations to the data by other concurrent transactions can occur &lt;em&gt;between&lt;&#x2F;em&gt; the FDB transactions of a continued scan. As a result, a long-running operation that uses continuations doesn&#x27;t see the entire dataset at a single, frozen moment in time. Instead, it might see some data that was present or changed &lt;em&gt;after&lt;&#x2F;em&gt; the operation began but &lt;em&gt;before&lt;&#x2F;em&gt; it completed. This is a natural consequence of breaking the work into smaller pieces to fit within FDB&#x27;s transaction limits. Applications should be aware of this behavior, particularly if they need all the data to reflect its state from one specific instant.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#conclusion&quot; aria-label=&quot;Anchor link for: conclusion&quot;&gt;🔗&lt;&#x2F;a&gt;Conclusion&lt;&#x2F;h2&gt;
&lt;p&gt;The Record Layer&#x27;s continuation feature is a powerful tool for handling large datasets and complex queries in FoundationDB, but it&#x27;s important to understand the implications of non-atomicity. By breaking operations into smaller, FDB-compliant transactions, the Record Layer provides a flexible and scalable solution while maintaining the core principles of FDB&#x27;s transactional model.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Feel free to reach out with any questions or to share your thoughts. You can find me on &lt;a href=&quot;https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;pierrezemb.fr&quot;&gt;Bluesky&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;PierreZ&quot;&gt;Twitter&lt;&#x2F;a&gt; or through my &lt;a href=&quot;https:&#x2F;&#x2F;pierrezemb.fr&quot;&gt;website&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</description>
          <category domain="tag">foundationdb</category>
          <category domain="tag">record-layer</category>
          <category domain="tag">java</category>
          <category domain="tag">database</category>
          <category domain="tag">continuation</category>
          <category domain="tag">pagination</category>
          <category domain="tag">distributed-systems</category>
      </item>
    </channel>
</rss>
