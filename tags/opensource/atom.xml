<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Pierre Zemb&#x27;s Blog - opensource</title>
    <subtitle>Pierre Zemb personal blog</subtitle>
    <link rel="self" type="application/atom+xml" href="https://pierrezemb.fr/tags/opensource/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://pierrezemb.fr"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2020-09-23T10:24:27+01:00</updated>
    <id>https://pierrezemb.fr/tags/opensource/atom.xml</id>
    <entry xml:lang="en">
        <title>Announcing Record-Store, a new (experimental) place for your data</title>
        <published>2020-09-23T10:24:27+01:00</published>
        <updated>2020-09-23T10:24:27+01:00</updated>
        
        <author>
          <name>
            
              Pierre Zemb
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pierrezemb.fr/posts/announcing-record-store/"/>
        <id>https://pierrezemb.fr/posts/announcing-record-store/</id>
        
        <category term="database" schema="tag" label="database"/>
        <category term="storage" schema="tag" label="storage"/>
        <category term="distributed" schema="tag" label="distributed"/>
        <category term="opensource" schema="tag" label="opensource"/>
        <category term="foundationdb" schema="tag" label="foundationdb"/>
        <content type="html" xml:base="https://pierrezemb.fr/posts/announcing-record-store/">&lt;p&gt;TL;DR: I&#x27;m really happy to announce my latest open-source project called Record-Store ðŸš€ Please check it out on &lt;a href=&quot;https:&#x2F;&#x2F;pierrez.github.io&#x2F;record-store&quot;&gt;https:&#x2F;&#x2F;pierrez.github.io&#x2F;record-store&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#what&quot; aria-label=&quot;Anchor link for: what&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;What?&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;code&gt;Record-Store&lt;&#x2F;code&gt; is a &lt;a href=&quot;https:&#x2F;&#x2F;apple.github.io&#x2F;foundationdb&#x2F;layer-concept.html&quot;&gt;layer&lt;&#x2F;a&gt; running on top of &lt;a href=&quot;https:&#x2F;&#x2F;foundationdb.org&quot;&gt;FoundationDB&lt;&#x2F;a&gt;. It provides abstractions to create, load and deletes customer-defined data called &lt;code&gt;records&lt;&#x2F;code&gt;, which are hold into a &lt;code&gt;RecordSpace&lt;&#x2F;code&gt;. We would like to have this kind of flow for developers:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Opening RecordSpace, for example &lt;code&gt;prod&#x2F;users&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Create a protobuf definition which will be used as schema&lt;&#x2F;li&gt;
&lt;li&gt;Upsert schema&lt;&#x2F;li&gt;
&lt;li&gt;Push records&lt;&#x2F;li&gt;
&lt;li&gt;Query records&lt;&#x2F;li&gt;
&lt;li&gt;delete records&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;You need another &lt;code&gt;KeySpace&lt;&#x2F;code&gt; to store another type of data, or maybe a &lt;code&gt;KeySpace&lt;&#x2F;code&gt; dedicated to production env? Juste create it and you are good to go!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;features&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#features&quot; aria-label=&quot;Anchor link for: features&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Features&lt;&#x2F;h2&gt;
&lt;p&gt;It is currently an experiment, but it already has some strong features:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-tenant&lt;&#x2F;strong&gt; A &lt;code&gt;tenant&lt;&#x2F;code&gt; can create as many &lt;code&gt;RecordSpace&lt;&#x2F;code&gt; as we want, and we can have many &lt;code&gt;tenants&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Standard API&lt;&#x2F;strong&gt; We are exposing the record-store with standard technologies:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;grpc.io&quot;&gt;gRPC&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;em&gt;very experimental&lt;&#x2F;em&gt; &lt;a href=&quot;https:&#x2F;&#x2F;graphql.org&quot;&gt;GraphQL&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable&lt;&#x2F;strong&gt; We are based on the same tech behind &lt;a href=&quot;https:&#x2F;&#x2F;www.foundationdb.org&#x2F;files&#x2F;record-layer-paper.pdf&quot;&gt;CloudKit&lt;&#x2F;a&gt; called the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;foundationdb&#x2F;fdb-record-layer&#x2F;&quot;&gt;Record Layer&lt;&#x2F;a&gt;,&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transactional&lt;&#x2F;strong&gt; We are running on top of &lt;a href=&quot;https:&#x2F;&#x2F;www.foundationdb.org&#x2F;&quot;&gt;FoundationDB&lt;&#x2F;a&gt;. FoundationDB gives you the power of ACID transactions in a distributed database.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Encrypted&lt;&#x2F;strong&gt; Data are encrypted by default.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-model&lt;&#x2F;strong&gt; For each &lt;code&gt;RecordSpace&lt;&#x2F;code&gt;, you can define a &lt;code&gt;schema&lt;&#x2F;code&gt;, which is in-fact only a &lt;code&gt;Protobuf&lt;&#x2F;code&gt; definition. You need to store some &lt;code&gt;users&lt;&#x2F;code&gt;, or a more complicated structure? If you can represent it as &lt;a href=&quot;https:&#x2F;&#x2F;developers.google.com&#x2F;protocol-buffers&quot;&gt;Protobuf&lt;&#x2F;a&gt;, you are good to go!&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Index-defined queries&lt;&#x2F;strong&gt; Your queries&#x27;s capabilities are defined by the indexes you put on your schema.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Secured&lt;&#x2F;strong&gt; We are using &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;CleverCloud&#x2F;biscuit&quot;&gt;Biscuit&lt;&#x2F;a&gt;, a mix of &lt;code&gt;JWT&lt;&#x2F;code&gt; and &lt;code&gt;Macaroons&lt;&#x2F;code&gt; to ensure auth{entication, orization}.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;why&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#why&quot; aria-label=&quot;Anchor link for: why&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Why?&lt;&#x2F;h2&gt;
&lt;p&gt;Lately, I have been playing a lot with my &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;PierreZ&#x2F;fdb-etcd&quot;&gt;ETCD-Layer&lt;&#x2F;a&gt; that is using the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;foundationdb&#x2F;fdb-record-layer&#x2F;&quot;&gt;Record-Layer&lt;&#x2F;a&gt;. Thanks to it, I was able to bootstrap my ETCD-layer very quickly, but I was not using a tenth of the capacities of this library. So I decided to go deeper. &lt;strong&gt;What would a gRPC abstraction of the Record-Layer look like?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The name of this project itself is a tribute to the Record Layer as we are exposing the layer within a gRPC interface.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;try-it-out&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#try-it-out&quot; aria-label=&quot;Anchor link for: try-it-out&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Try it out&lt;&#x2F;h2&gt;
&lt;p&gt;Record-Store is open sourced under Apache License V2 in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;PierreZ&#x2F;record-store&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;PierreZ&#x2F;record-store&lt;&#x2F;a&gt; and the documentation can be found &lt;a href=&quot;https:&#x2F;&#x2F;pierrez.github.io&#x2F;record-store&quot;&gt;https:&#x2F;&#x2F;pierrez.github.io&#x2F;record-store&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Thank you&lt;&#x2F;strong&gt; for reading my post! Feel free to react to this article, I am also available on &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;PierreZ&quot;&gt;Twitter&lt;&#x2F;a&gt; if needed.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Announcing Kafka-on-Pulsar: bring native Kafka protocol support to Apache Pulsar</title>
        <published>2020-03-24T10:24:27+01:00</published>
        <updated>2020-03-24T10:24:27+01:00</updated>
        
        <author>
          <name>
            
              Pierre Zemb
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pierrezemb.fr/posts/announcing-kop/"/>
        <id>https://pierrezemb.fr/posts/announcing-kop/</id>
        
        <category term="messaging" schema="tag" label="messaging"/>
        <category term="distributed" schema="tag" label="distributed"/>
        <category term="kafka" schema="tag" label="kafka"/>
        <category term="pulsar" schema="tag" label="pulsar"/>
        <category term="opensource" schema="tag" label="opensource"/>
        <content type="html" xml:base="https://pierrezemb.fr/posts/announcing-kop/">&lt;blockquote&gt;
&lt;p&gt;This is a repost from &lt;a href=&quot;https:&#x2F;&#x2F;www.ovh.com&#x2F;blog&#x2F;announcing-kafka-on-pulsar-bring-native-kafka-protocol-support-to-apache-pulsar&#x2F;&quot; title=&quot;Permalink to announcing KoP&quot;&gt;OVHcloud&#x27;s official blogpost.&lt;&#x2F;a&gt;, please read it there to support my company. Thanks &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;LostInBrittany&#x2F;&quot;&gt;Horacio Gonzalez&lt;&#x2F;a&gt; for the awesome drawings!&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;This post has been published on both the StreamNative and OVHcloud blogs and was co-authored by &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;sijieg&quot;&gt;Sijie Guo&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;Jia_Zhai&quot;&gt;Jia Zhai&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;PierreZ&quot;&gt;Pierre Zemb&lt;&#x2F;a&gt;. Thanks &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;LostInBrittany&quot;&gt;Horacio Gonzalez&lt;&#x2F;a&gt; for the illustrations!&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;announcing-kop&#x2F;kop-1.png&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We are excited to announce that StreamNative and OVHcloud are open-sourcing &quot;Kafka on Pulsar&quot; (KoP). KoP brings the native Apache Kafka protocol support to Apache Pulsar by introducing a Kafka protocol handler on Pulsar brokers. By adding the KoP protocol handler to your existing Pulsar cluster, you can now migrate your existing Kafka applications and services to Pulsar without modifying the code. This enables Kafka applications to leverage Pulsar&#x27;s powerful features, such as:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Streamlined operations with enterprise-grade multi-tenancy&lt;&#x2F;li&gt;
&lt;li&gt;Simplified operations with a rebalance-free architecture&lt;&#x2F;li&gt;
&lt;li&gt;Infinite event stream retention with Apache BookKeeper and tiered storage&lt;&#x2F;li&gt;
&lt;li&gt;Serverless event processing with Pulsar Functions&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;what-is-apache-pulsar&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-is-apache-pulsar&quot; aria-label=&quot;Anchor link for: what-is-apache-pulsar&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;What is Apache Pulsar?&lt;&#x2F;h2&gt;
&lt;p&gt;Apache Pulsar is an event streaming platform designed from the ground up to be cloud-native- deploying a multi-layer and segment-centric architecture. The architecture separates serving and storage into different layers, making the system container-friendly. The cloud-native architecture provides scalability, availability and resiliency and enables companies to expand their offerings with real-time data-enabled solutions. Pulsar has gained wide adoption since it was open-sourced in 2016 and was designated an Apache Top-Level project in 2018.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-need-behind-kop&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-need-behind-kop&quot; aria-label=&quot;Anchor link for: the-need-behind-kop&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;The need behind KoP&lt;&#x2F;h2&gt;
&lt;p&gt;Pulsar provides a unified messaging model for both queueing and streaming workloads. Pulsar implemented its own protobuf-based binary protocol to provide high performance and low latency. This choice of protobuf makes it convenient to implement Pulsar &lt;a href=&quot;https:&#x2F;&#x2F;pulsar.apache.org&#x2F;docs&#x2F;en&#x2F;client-libraries&#x2F;&quot;&gt;clients&lt;&#x2F;a&gt; and the project already supports Java, Go, Python and C++ languages alongside &lt;a href=&quot;https:&#x2F;&#x2F;pulsar.apache.org&#x2F;docs&#x2F;en&#x2F;client-libraries&#x2F;#thirdparty-clients&quot;&gt;thirdparty clients&lt;&#x2F;a&gt; provided by the community. However, existing applications written using other messaging protocols had to be rewritten to adopt Pulsar&#x27;s new unified messaging protocol.&lt;&#x2F;p&gt;
&lt;p&gt;To address this, the Pulsar community developed applications to facilitate the migration to Pulsar from other messaging systems. For example, Pulsar provides a &lt;a href=&quot;http:&#x2F;&#x2F;(https:&#x2F;&#x2F;pulsar.apache.org&#x2F;docs&#x2F;en&#x2F;adaptors-kafka&quot;&gt;Kafka wrapper&lt;&#x2F;a&gt; on Kafka Java API, which allows existing applications that already use Kafka Java client switching from Kafka to Pulsar &lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Cy9ev9nAZpI&quot;&gt;without code change&lt;&#x2F;a&gt;. Pulsar also has a rich connector ecosystem, connecting Pulsar with other data systems. Yet, there was still a strong demand from those looking to switch from other Kafka applications to Pulsar.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;streamnative-and-ovhcloud-s-collaboration&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#streamnative-and-ovhcloud-s-collaboration&quot; aria-label=&quot;Anchor link for: streamnative-and-ovhcloud-s-collaboration&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;StreamNative and OVHcloud&#x27;s collaboration&lt;&#x2F;h2&gt;
&lt;p&gt;StreamNative was receiving a lot of inbound requests for help migrating from other messaging systems to Pulsar and recognized the need to support other messaging protocols (such as AMQP and Kafka) natively on Pulsar. StreamNative began working on introducing a general protocol handler framework in Pulsar that would allow developers using other messaging protocols to use Pulsar.&lt;&#x2F;p&gt;
&lt;p&gt;Internally, OVHcloud had been running Apache Kafka for years, but despite their experience operating multiple clusters with millions of messages per second on Kafka, there were painful operational challenges. For example, putting thousands of topics from thousands of users into a single cluster was difficult without multi-tenancy.&lt;&#x2F;p&gt;
&lt;p&gt;As a result, OVHcloud decided to shift and build the foundation of their topic-as-a-service product, called ioStream, on Pulsar instead of Kafka. Pulsar&#x27;s multi-tenancy and the overall architecture with Apache Bookkeeper simplified operations compared to Kafka.&lt;&#x2F;p&gt;
&lt;p&gt;After spawning the first region, OVHcloud decided to implement it as a proof-of-concept proxy capable of transforming the Kafka protocol to Pulsar on the fly. During this process, OVHcloud discovered that StreamNative was working on bringing the Kafka protocol natively to Pulsar, and they joined forces to develop KoP.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;announcing-kop&#x2F;kop-2.png&quot; alt=&quot;kop image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;KoP was developed to provide a streamlined and comprehensive solution leveraging Pulsar and BookKeeper&#x27;s event stream storage infrastructure and Pulsar&#x27;s pluggable protocol handler framework. KoP is implemented as a protocol handler plugin with protocol name &quot;kafka&quot;. It can be installed and configured to run as part of Pulsar brokers.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-distributed-log&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-distributed-log&quot; aria-label=&quot;Anchor link for: the-distributed-log&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;The distributed log&lt;&#x2F;h2&gt;
&lt;p&gt;Both Pulsar and Kafka share a very similar data model around &lt;strong&gt;log&lt;&#x2F;strong&gt; for both pub&#x2F;sub messaging and event streaming. For example, both are built on top of a distributed log. Kafka implements the distributed log in a partition-basis architecture, where a distributed log (a partition in Kafka) is designated to store in a set of brokers, while Pulsar deploys a &lt;strong&gt;segment&lt;&#x2F;strong&gt;-based architecture to implement its distributed log by leveraging Apache BookKeeper as its scale-out segment storage layer. Pulsar&#x27;s &lt;em&gt;segment&lt;&#x2F;em&gt; based architecture provides benefits such as rebalance-free, instant scalability, and infinite event stream storage. You can learn more about the key differences between Pulsar and Kafka in &lt;a href=&quot;https:&#x2F;&#x2F;www.splunk.com&#x2F;en_us&#x2F;blog&#x2F;it&#x2F;comparing-pulsar-and-kafka-how-a-segment-based-architecture-delivers-better-performance-scalability-and-resilience.html&quot;&gt;this Splunk blog&lt;&#x2F;a&gt; and in &lt;a href=&quot;http:&#x2F;&#x2F;bookkeeper.apache.org&#x2F;distributedlog&#x2F;technical-review&#x2F;2016&#x2F;09&#x2F;19&#x2F;kafka-vs-distributedlog.html&quot;&gt;this blog from the Bookkeeper project&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Since both of the systems are built on a similar data model, a distributed log, it is very simple to implement a Kafka-compatible protocol handler by leveraging Pulsar&#x27;s distributed log storage and its pluggable protocol handler framework (introduced in the 2.5.0 release).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;implementations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#implementations&quot; aria-label=&quot;Anchor link for: implementations&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Implementations&lt;&#x2F;h2&gt;
&lt;p&gt;The implementation is done by comparing the protocols between Pulsar and Kafka. We found that there are a lot of similarities between these two protocols. Both protocols are comprised of the following operations:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Topic Lookup&lt;&#x2F;strong&gt;: All the clients connect to any broker to lookup the metadata (i.e. the owner broker) of the topics. After fetching the metadata, the clients establish persistent TCP connections to the owner brokers.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Produce&lt;&#x2F;strong&gt;: The clients talk to the &lt;strong&gt;owner&lt;&#x2F;strong&gt; broker of a topic partition to append the messages to a distributed log.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Consume&lt;&#x2F;strong&gt;: The clients talk to the &lt;strong&gt;owner&lt;&#x2F;strong&gt; broker of a topic partition to read the messages from a distributed log.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Offset&lt;&#x2F;strong&gt;: The messages produced to a topic partition are assigned with an offset. The offset in Pulsar is called MessageId. Consumers can use &lt;strong&gt;offsets&lt;&#x2F;strong&gt; to seek to a given position within the log to read messages.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Consumption State&lt;&#x2F;strong&gt;: Both systems maintain the consumption state for consumers within a subscription (or a consumer group in Kafka). The consumption state is stored in __offsets topic in Kafka, while the consumption state is stored as cursors in Pulsar.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;As you can see, these are all the primitive operations provided by a scale-out distributed log storage such as Apache BookKeeper. The core capabilities of Pulsar are implemented on top of Apache BookKeeper. Thus it is pretty easy and straightforward to implement the Kafka concepts by using the existing components that Pulsar has developed on BookKeeper.&lt;br&gt;
The following figure illustrates how we add the Kafka protocol support within Pulsar. We are introducing a new &lt;strong&gt;Protocol Handler&lt;&#x2F;strong&gt;which implements the Kafka wire protocol by leveraging the existing components (such as topic discovery, the distributed log library â€“ ManagedLedger, cursors and etc) that Pulsar already has.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;announcing-kop&#x2F;kop-3.png&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;topics&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#topics&quot; aria-label=&quot;Anchor link for: topics&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Topics&lt;&#x2F;h3&gt;
&lt;p&gt;In Kafka, all the topics are stored in one flat namespace. But in Pulsar, topics are organized in hierarchical multi-tenant namespaces. We introduce a setting &lt;em&gt;kafkaNamespace&lt;&#x2F;em&gt; in broker configuration to allow the administrator configuring to map Kafka topics to Pulsar topics.&lt;&#x2F;p&gt;
&lt;p&gt;In order to let Kafka users leverage the multi-tenancy feature of Apache Pulsar, a Kafka user can specify a Pulsar tenant and namespace as its SASL username when it uses SASL authentication mechanism to authenticate a Kafka client.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;message-id-and-offset&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#message-id-and-offset&quot; aria-label=&quot;Anchor link for: message-id-and-offset&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Message ID and offset&lt;&#x2F;h3&gt;
&lt;p&gt;In Kafka, each message is assigned with an offset once it is successfully produced to a topic partition. In Pulsar, each message is assigned with a &lt;code&gt;MessageID&lt;&#x2F;code&gt;. The message id consists of 3 components, &lt;em&gt;ledger-id&lt;&#x2F;em&gt;, &lt;em&gt;entry-id&lt;&#x2F;em&gt;, and &lt;em&gt;batch-index&lt;&#x2F;em&gt;. We are using the same approach in Pulsar-Kafka wrapper to convert a Pulsar MessageID to an offset and vice versa.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;messages&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#messages&quot; aria-label=&quot;Anchor link for: messages&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Messages&lt;&#x2F;h3&gt;
&lt;p&gt;Both a Kafka message and a Pulsar message have key, value, timestamp, and headers (note: this is called &#x27;properties&#x27; in Pulsar). We convert these fields automatically between Kafka messages and Pulsar messages.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;topic-lookup&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#topic-lookup&quot; aria-label=&quot;Anchor link for: topic-lookup&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Topic lookup&lt;&#x2F;h3&gt;
&lt;p&gt;We use the same topic lookup approach for the Kafka request handler as the Pulsar request handler. The request handler does topic discovery to lookup all the ownerships for the requested topic partitions and responds with the ownership information as part of Kafka TopicMetadata back to Kafka clients.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;produce-messages&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#produce-messages&quot; aria-label=&quot;Anchor link for: produce-messages&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Produce Messages&lt;&#x2F;h3&gt;
&lt;p&gt;When the Kafka request handler receives produced messages from a Kafka client, it converts Kafka messages to Pulsar messages by mapping the fields (i.e. key, value, timestamp and headers) one by one, and uses the ManagedLedger append API to append those converted Pulsar messages to BookKeeper. Converting Kafka messages to Pulsar messages allows existing Pulsar applications to consume messages produced by Kafka clients.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;consume-messages&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#consume-messages&quot; aria-label=&quot;Anchor link for: consume-messages&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Consume Messages&lt;&#x2F;h3&gt;
&lt;p&gt;When the Kafka request handler receives a consumer request from a Kafka client, it opens a non-durable cursor to read the entries starting from the requested offset. The Kafka request handler converts the Pulsar messages back to Kafka messages to allow existing Kafka applications to consume the messages produced by Pulsar clients.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;group-coordinator-offsets-management&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#group-coordinator-offsets-management&quot; aria-label=&quot;Anchor link for: group-coordinator-offsets-management&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Group coordinator &amp;amp; offsets management&lt;&#x2F;h3&gt;
&lt;p&gt;The most challenging part is to implement the group coordinator and offsets management. Because Pulsar doesn&#x27;t have a centralized group coordinator for assigning partitions to consumers of a consumer group and managing offsets for each consumer group. In Pulsar, the partition assignment is managed by broker on a per-partition basis, and the offset management is done by storing the acknowledgements in cursors by the owner broker of that partition.&lt;&#x2F;p&gt;
&lt;p&gt;It is difficult to align the Pulsar model with the Kafka model. Hence, for the sake of providing full compatibility with Kafka clients, we implemented the Kafka group coordinator by storing the coordinator group changes and offsets in a system topic called *public&#x2F;kafka&#x2F;*&lt;em&gt;offsets&lt;&#x2F;em&gt; in Pulsar.&lt;&#x2F;p&gt;
&lt;p&gt;This allows us to bridge the gap between Pulsar and Kafka and allows people to use existing Pulsar tools and policies to manage subscriptions and monitor Kafka consumers. We add a background thread in the implemented group coordinator to periodically sync offset updates from the system topic to Pulsar cursors. Hence a Kafka consumer group is effectively treated as a Pulsar subscription. All the existing Pulsar toolings can be used for managing Kafka consumer groups as well.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bridge-two-popular-messaging-ecosystems&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#bridge-two-popular-messaging-ecosystems&quot; aria-label=&quot;Anchor link for: bridge-two-popular-messaging-ecosystems&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Bridge two popular messaging ecosystems&lt;&#x2F;h2&gt;
&lt;p&gt;At both companies, we value customer success. We believe that providing a native Kafka protocol on Apache Pulsar will reduce the barriers for people adopting Pulsar to achieve their business success. By integrating two popular event streaming ecosystems, KoP unlocks new use cases. Customers can leverage advantages from each ecosystem and build a truly unified event streaming platform with Apache Pulsar to accelerate the development of real-time applications and services.&lt;&#x2F;p&gt;
&lt;p&gt;With KoP, a log collector can continue collecting log data from its sources and producing messages to Apache Pulsar using existing Kafka integrations. The downstream applications can use Pulsar Functions to process the events arriving in the system to do serverless event streaming.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;try-it-out&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#try-it-out&quot; aria-label=&quot;Anchor link for: try-it-out&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Try it out&lt;&#x2F;h2&gt;
&lt;p&gt;KoP is open sourced under Apache License V2 in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;streamnative&#x2F;kop&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;streamnative&#x2F;kop&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We are looking forward to your issues, and PRs. You can also &lt;a href=&quot;https:&#x2F;&#x2F;apache-pulsar.herokuapp.com&#x2F;&quot;&gt;join #kop channel in Pulsar Slack&lt;&#x2F;a&gt; to discuss all things about Kafka-on-Pulsar.&lt;&#x2F;p&gt;
&lt;p&gt;StreamNative and OVHcloud are also hosting a webinar about KoP on March 31. If you are interested in learning more details about KoP,&lt;a href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;webinar&#x2F;register&#x2F;6515842602644&#x2F;WN_l_i-3ekDSg6PwPFn7tqRvA&quot;&gt;please sign up&lt;&#x2F;a&gt;. Looking forward to meeting you online.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;announcing-kop&#x2F;kop-4.png&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;thanks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#thanks&quot; aria-label=&quot;Anchor link for: thanks&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Thanks&lt;&#x2F;h2&gt;
&lt;p&gt;The KoP project was originally initiated by StreamNative. The OVHcloud team joined the project to collaborate on the development of the KoP project. Many thanks to Pierre Zemb and Steven Le Roux from OVHcloud for their contributions to this project!&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Contributing to Apache HBase: custom data balancing</title>
        <published>2020-02-14T10:24:27+01:00</published>
        <updated>2020-02-14T10:24:27+01:00</updated>
        
        <author>
          <name>
            
              Pierre Zemb
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pierrezemb.fr/posts/hbase-custom-data-balancing/"/>
        <id>https://pierrezemb.fr/posts/hbase-custom-data-balancing/</id>
        
        <category term="database" schema="tag" label="database"/>
        <category term="distributed" schema="tag" label="distributed"/>
        <category term="hbase" schema="tag" label="hbase"/>
        <category term="performance" schema="tag" label="performance"/>
        <category term="opensource" schema="tag" label="opensource"/>
        <content type="html" xml:base="https://pierrezemb.fr/posts/hbase-custom-data-balancing/">&lt;blockquote&gt;
&lt;p&gt;This is a repost from &lt;a href=&quot;https:&#x2F;&#x2F;www.ovh.com&#x2F;blog&#x2F;contributing-to-apache-hbase-custom-data-balancing&#x2F;&quot; title=&quot;Permalink to Contributing to Apache HBase: custom data balancing&quot;&gt;OVHcloud&#x27;s official blogpost.&lt;&#x2F;a&gt;, please read it there to support my company. Thanks &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;LostInBrittany&#x2F;&quot;&gt;Horacio Gonzalez&lt;&#x2F;a&gt; for the awesome drawings!&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;In today&#x27;s blogpost, we&#x27;re going to take a look at our upstream
contribution to Apache HBase&#x27;s stochastic load balancer, based on our
experience of running HBase clusters to support OVHcloud&#x27;s monitoring.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;hbase-custom-data-balancing&#x2F;hbase-ovh-1.jpeg&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-context&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-context&quot; aria-label=&quot;Anchor link for: the-context&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;The context&lt;&#x2F;h2&gt;
&lt;p&gt;Have you ever wondered how:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;we generate the graphs for your OVHcloud server or web hosting package?&lt;&#x2F;li&gt;
&lt;li&gt;our internal teams monitor their own servers and applications?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;All internal teams are constantly gathering telemetry and monitoring data&lt;&#x2F;strong&gt; and sending them to a &lt;strong&gt;dedicated team,&lt;&#x2F;strong&gt; who are responsible for &lt;strong&gt;handling all the metrics and logs generated by OVHcloud&#x27;s infrastructure&lt;&#x2F;strong&gt;: the Observability team.&lt;&#x2F;p&gt;
&lt;p&gt;We tried a lot of different &lt;strong&gt;Time Series databases&lt;&#x2F;strong&gt;, and eventually chose &lt;a href=&quot;https:&#x2F;&#x2F;warp10.io&#x2F;&quot;&gt;Warp10&lt;&#x2F;a&gt; to handle our workloads. &lt;strong&gt;Warp10&lt;&#x2F;strong&gt; can be integrated with the various &lt;strong&gt;big-data solutions&lt;&#x2F;strong&gt; provided by the &lt;a href=&quot;https:&#x2F;&#x2F;www.apache.org&#x2F;&quot;&gt;Apache Foundation.&lt;&#x2F;a&gt; In our case, we use &lt;a href=&quot;http:&#x2F;&#x2F;hbase.apache.org&#x2F;&quot;&gt;Apache HBase&lt;&#x2F;a&gt; as the long-term storage datastore for our metrics.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;http:&#x2F;&#x2F;hbase.apache.org&#x2F;&quot;&gt;Apache HBase&lt;&#x2F;a&gt;, a datastore built on top of &lt;a href=&quot;http:&#x2F;&#x2F;hadoop.apache.org&#x2F;&quot;&gt;Apache Hadoop&lt;&#x2F;a&gt;, provides &lt;strong&gt;an elastic, distributed, key-ordered map.&lt;&#x2F;strong&gt; As such, one of the key features of Apache HBase for us is the ability to &lt;strong&gt;scan&lt;&#x2F;strong&gt;, i.e. retrieve a range of keys. Thanks to this feature, we can fetch &lt;strong&gt;thousands of datapoints in an optimised way&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We have our own dedicated clusters, the biggest of which has more than 270 nodes to spread our workloads:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;between 1.6 and 2 million writes per second, 24&#x2F;7&lt;&#x2F;li&gt;
&lt;li&gt;between 4 and 6 million reads per second&lt;&#x2F;li&gt;
&lt;li&gt;around 300TB of telemetry, stored within Apache HBase&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;As you can probably imagine, storing 300TB of data in 270 nodes comes with some challenges regarding repartition, as &lt;strong&gt;every&lt;&#x2F;strong&gt; &lt;strong&gt;bit is hot data, and should be accessible at any time&lt;&#x2F;strong&gt;. Let&#x27;s dive in!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-does-balancing-work-in-apache-hbase&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-does-balancing-work-in-apache-hbase&quot; aria-label=&quot;Anchor link for: how-does-balancing-work-in-apache-hbase&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;How does balancing work in Apache HBase?&lt;&#x2F;h2&gt;
&lt;p&gt;Before diving into the balancer, let&#x27;s take a look at how it works. In Apache HBase, data is split into shards called &lt;code&gt;Regions&lt;&#x2F;code&gt;, and distributed through &lt;code&gt;RegionServers&lt;&#x2F;code&gt;. The number of regions will increase as the data is coming in, and regions will be split as a result. This is where the &lt;code&gt;Balancer&lt;&#x2F;code&gt; comes in. It will &lt;strong&gt;move regions&lt;&#x2F;strong&gt; to avoid hotspotting a single &lt;code&gt;RegionServer&lt;&#x2F;code&gt; and effectively distribute the load.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;hbase-custom-data-balancing&#x2F;hbase-ovh-2.jpeg&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The actual implementation, called &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;blob&#x2F;master&#x2F;hbase-server&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;master&#x2F;balancer&#x2F;StochasticLoadBalancer.java&quot;&gt;StochasticBalancer&lt;&#x2F;a&gt;, uses &lt;strong&gt;a cost-based approach:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;It first computes the &lt;strong&gt;overall cost&lt;&#x2F;strong&gt; of the cluster, by looping through &lt;code&gt;cost functions&lt;&#x2F;code&gt;. Every cost function &lt;strong&gt;returns a number between 0 and 1 inclusive&lt;&#x2F;strong&gt;, where 0 is the lowest cost-best solution, and 1 is the highest possible cost and worst solution. Apache Hbase is coming with several cost functions, which are measuring things like region load, table load, data locality, number of regions per RegionServers... The computed costs are &lt;strong&gt;scaled by their respective coefficients, defined in the configuration&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Now that the initial cost is computed, we can try to &lt;code&gt;Mutate&lt;&#x2F;code&gt; our cluster. For this, the Balancer creates a random &lt;code&gt;nextAction&lt;&#x2F;code&gt;, which could be something like &lt;strong&gt;swapping two regions&lt;&#x2F;strong&gt;, or &lt;strong&gt;moving one region to another RegionServer&lt;&#x2F;strong&gt;. The action is &lt;strong&gt;applied&lt;&#x2F;strong&gt; &lt;strong&gt;virtually&lt;&#x2F;strong&gt; , and then the &lt;strong&gt;new cost is calculated&lt;&#x2F;strong&gt;. If the new cost is lower than our previous one, the action is stored. If not, it is skipped. This operation is repeated &lt;code&gt;thousands of times&lt;&#x2F;code&gt;, hence the &lt;code&gt;Stochastic&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;At the end, &lt;strong&gt;the list of valid actions is applied to the actual cluster.&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;what-was-not-working-for-us&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-was-not-working-for-us&quot; aria-label=&quot;Anchor link for: what-was-not-working-for-us&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;What was not working for us?&lt;&#x2F;h2&gt;
&lt;p&gt;We found out that &lt;strong&gt;for our specific use case&lt;&#x2F;strong&gt;, which involved:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Single table&lt;&#x2F;li&gt;
&lt;li&gt;Dedicated Apache HBase and Apache Hadoop, &lt;strong&gt;tailored for our requirements&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Good key distribution&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;the number of regions per RegionServer was the real limit for us&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Even if the balancing strategy seems simple, &lt;strong&gt;we do think that being able to run an Apache HBase cluster on heterogeneous hardware is vital&lt;&#x2F;strong&gt;, especially in cloud environments, because you &lt;strong&gt;may not be able to buy the same server specs again in the future.&lt;&#x2F;strong&gt;
In our earlier example, our cluster grew from 80 to ~250 machines in
four years. Throughout that time, we bought new dedicated server
references, and even tested some special internal references.&lt;&#x2F;p&gt;
&lt;p&gt;We ended-up with differents groups of hardware: &lt;strong&gt;some servers can handle only 180 regions, whereas the biggest can handle more than 900&lt;&#x2F;strong&gt;. Because of this disparity, we had to disable the Load Balancer to avoid the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;blob&#x2F;master&#x2F;hbase-server&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;master&#x2F;balancer&#x2F;StochasticLoadBalancer.java#L1194&quot;&gt;RegionCountSkewCostFunction&lt;&#x2F;a&gt;, which would try to bring all RegionServers to the same number of regions.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;hbase-custom-data-balancing&#x2F;hbase-ovh-3.jpeg&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Two years ago we developed some internal tools, which are responsible
for load balancing regions across RegionServers. The tooling worked
really good for our use case, simplifying the day-to-day operation of
our cluster.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Open source is at the DNA of OVHcloud&lt;&#x2F;strong&gt;, and that means that we build our tools on open source software, but also that we &lt;strong&gt;contribute&lt;&#x2F;strong&gt;
and give it back to the community. When we talked around, we saw that
we weren&#x27;t the only one concerned by the heterogenous cluster problem.
We decided to rewrite our tooling to make it more general, and to &lt;strong&gt;contribute&lt;&#x2F;strong&gt; it &lt;strong&gt;directly upstream&lt;&#x2F;strong&gt; to the HBase project &lt;strong&gt;.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;our-contributions&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#our-contributions&quot; aria-label=&quot;Anchor link for: our-contributions&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Our contributions&lt;&#x2F;h2&gt;
&lt;p&gt;The first contribution was pretty simple, the cost function list was a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;blob&#x2F;8cb531f207b9f9f51ab1509655ae59701b66ac37&#x2F;hbase-server&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;master&#x2F;balancer&#x2F;StochasticLoadBalancer.java#L199-L213&quot;&gt;constant&lt;&#x2F;a&gt;. We &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;commit&#x2F;836f26976e1ad8b35d778c563067ed0614c026e9&quot;&gt;added the possibility to load custom cost functions&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The second contribution was about &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;commit&#x2F;42d535a57a75b58f585b48df9af9c966e6c7e46a&quot;&gt;adding an optional costFunction to balance regions according to a capacity rule&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-does-it-works&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-does-it-works&quot; aria-label=&quot;Anchor link for: how-does-it-works&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;How does it works?&lt;&#x2F;h2&gt;
&lt;p&gt;The balancer will load a file containing lines of rules. &lt;strong&gt;A rule is composed of a regexp for hostname, and a limit.&lt;&#x2F;strong&gt; For example, we could have:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;rs[0-9] 200
&lt;&#x2F;span&gt;&lt;span&gt;rs1[0-9] 50
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;RegionServers with &lt;strong&gt;hostnames matching the first rules will have a limit of 200&lt;&#x2F;strong&gt;, and &lt;strong&gt;the others 50&lt;&#x2F;strong&gt;. If there&#x27;s no match, a default is set.&lt;&#x2F;p&gt;
&lt;p&gt;Thanks to these rule, we have two key pieces of information:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;max number of regions for this cluster&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;the *&lt;em&gt;rules for each servers&lt;&#x2F;em&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The &lt;code&gt;HeterogeneousRegionCountCostFunction&lt;&#x2F;code&gt; will try to &lt;strong&gt;balance regions, according to their capacity.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s take an example... Imagine that we have 20 RS:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;10 RS, named &lt;code&gt;rs0&lt;&#x2F;code&gt; to &lt;code&gt;rs9&lt;&#x2F;code&gt;, loaded with 60 regions each, which can each handle 200 regions.&lt;&#x2F;li&gt;
&lt;li&gt;10 RS, named &lt;code&gt;rs10&lt;&#x2F;code&gt; to &lt;code&gt;rs19&lt;&#x2F;code&gt;, loaded with 60 regions each, which can each handle 50 regions.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;So, based on the following rules:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;rs[0-9] 200
&lt;&#x2F;span&gt;&lt;span&gt;rs1[0-9] 50
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;... we can see that the &lt;strong&gt;second group is overloaded&lt;&#x2F;strong&gt;, whereas the first group has plenty of space.&lt;&#x2F;p&gt;
&lt;p&gt;We know that we can handle a maximum of &lt;strong&gt;2,500 regions&lt;&#x2F;strong&gt; (200Ã—10 + 50Ã—10), and we have currently &lt;strong&gt;1,200 regions&lt;&#x2F;strong&gt; (60Ã—20). As such, the &lt;code&gt;HeterogeneousRegionCountCostFunction&lt;&#x2F;code&gt; will understand that the cluster is &lt;strong&gt;full at 48.0%&lt;&#x2F;strong&gt; (1200&#x2F;2500). Based on this information, we will then &lt;strong&gt;try to put all the RegionServers at ~48% of the load, according to the rules.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;hbase-custom-data-balancing&#x2F;hbase-ovh-4.jpeg&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;where-to-next&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#where-to-next&quot; aria-label=&quot;Anchor link for: where-to-next&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Where to next?&lt;&#x2F;h2&gt;
&lt;p&gt;Thanks to Apache HBase&#x27;s contributors, our patches are now &lt;strong&gt;merged&lt;&#x2F;strong&gt; into the master branch. As soon as Apache HBase maintainers publish a new release, we will deploy and use it at scale. This &lt;strong&gt;will allow more automation on our side, and ease operations for the Observability Team.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Contributing was an awesome journey. What I love most about open
source is the opportunity ability to contribute back, and build stronger
software. We &lt;strong&gt;had an opinion&lt;&#x2F;strong&gt; about how a particular issue should addressed, but &lt;strong&gt;the discussions with the community helped us to refine it&lt;&#x2F;strong&gt;. We spoke with e &lt;strong&gt;ngineers from other companies, who were struggling with Apache HBase&#x27;s cloud deployments, just as we were&lt;&#x2F;strong&gt;, and thanks to those exchanges, &lt;strong&gt;our contribution became more and more relevant.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
