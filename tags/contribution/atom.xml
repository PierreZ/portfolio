<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Pierre Zemb&#x27;s Blog - contribution</title>
    <subtitle>Pierre Zemb personal blog</subtitle>
    <link rel="self" type="application/atom+xml" href="https://pierrezemb.fr/tags/contribution/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://pierrezemb.fr"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2020-02-14T10:24:27+01:00</updated>
    <id>https://pierrezemb.fr/tags/contribution/atom.xml</id>
    <entry xml:lang="en">
        <title>Contributing to Apache HBase: custom data balancing</title>
        <published>2020-02-14T10:24:27+01:00</published>
        <updated>2020-02-14T10:24:27+01:00</updated>
        
        <author>
          <name>
            
              Pierre Zemb
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pierrezemb.fr/posts/hbase-custom-data-balancing/"/>
        <id>https://pierrezemb.fr/posts/hbase-custom-data-balancing/</id>
        
        <content type="html" xml:base="https://pierrezemb.fr/posts/hbase-custom-data-balancing/">&lt;blockquote&gt;
&lt;p&gt;This is a repost from &lt;a href=&quot;https:&#x2F;&#x2F;www.ovh.com&#x2F;blog&#x2F;contributing-to-apache-hbase-custom-data-balancing&#x2F;&quot; title=&quot;Permalink to Contributing to Apache HBase: custom data balancing&quot;&gt;OVHcloud&#x27;s official blogpost.&lt;&#x2F;a&gt;, please read it there to support my company. Thanks &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;LostInBrittany&#x2F;&quot;&gt;Horacio Gonzalez&lt;&#x2F;a&gt; for the awesome drawings!&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;In today&#x27;s blogpost, we&#x27;re going to take a look at our upstream
contribution to Apache HBase&#x27;s stochastic load balancer, based on our
experience of running HBase clusters to support OVHcloud&#x27;s monitoring.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;hbase-custom-data-balancing&#x2F;hbase-ovh-1.jpeg&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-context&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-context&quot; aria-label=&quot;Anchor link for: the-context&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;The context&lt;&#x2F;h2&gt;
&lt;p&gt;Have you ever wondered how:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;we generate the graphs for your OVHcloud server or web hosting package?&lt;&#x2F;li&gt;
&lt;li&gt;our internal teams monitor their own servers and applications?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;All internal teams are constantly gathering telemetry and monitoring data&lt;&#x2F;strong&gt; and sending them to a &lt;strong&gt;dedicated team,&lt;&#x2F;strong&gt; who are responsible for &lt;strong&gt;handling all the metrics and logs generated by OVHcloud&#x27;s infrastructure&lt;&#x2F;strong&gt;: the Observability team.&lt;&#x2F;p&gt;
&lt;p&gt;We tried a lot of different &lt;strong&gt;Time Series databases&lt;&#x2F;strong&gt;, and eventually chose &lt;a href=&quot;https:&#x2F;&#x2F;warp10.io&#x2F;&quot;&gt;Warp10&lt;&#x2F;a&gt; to handle our workloads. &lt;strong&gt;Warp10&lt;&#x2F;strong&gt; can be integrated with the various &lt;strong&gt;big-data solutions&lt;&#x2F;strong&gt; provided by the &lt;a href=&quot;https:&#x2F;&#x2F;www.apache.org&#x2F;&quot;&gt;Apache Foundation.&lt;&#x2F;a&gt; In our case, we use &lt;a href=&quot;http:&#x2F;&#x2F;hbase.apache.org&#x2F;&quot;&gt;Apache HBase&lt;&#x2F;a&gt; as the long-term storage datastore for our metrics.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;http:&#x2F;&#x2F;hbase.apache.org&#x2F;&quot;&gt;Apache HBase&lt;&#x2F;a&gt;, a datastore built on top of &lt;a href=&quot;http:&#x2F;&#x2F;hadoop.apache.org&#x2F;&quot;&gt;Apache Hadoop&lt;&#x2F;a&gt;, provides &lt;strong&gt;an elastic, distributed, key-ordered map.&lt;&#x2F;strong&gt; As such, one of the key features of Apache HBase for us is the ability to &lt;strong&gt;scan&lt;&#x2F;strong&gt;, i.e. retrieve a range of keys. Thanks to this feature, we can fetch &lt;strong&gt;thousands of datapoints in an optimised way&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We have our own dedicated clusters, the biggest of which has more than 270 nodes to spread our workloads:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;between 1.6 and 2 million writes per second, 24&#x2F;7&lt;&#x2F;li&gt;
&lt;li&gt;between 4 and 6 million reads per second&lt;&#x2F;li&gt;
&lt;li&gt;around 300TB of telemetry, stored within Apache HBase&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;As you can probably imagine, storing 300TB of data in 270 nodes comes with some challenges regarding repartition, as &lt;strong&gt;every&lt;&#x2F;strong&gt; &lt;strong&gt;bit is hot data, and should be accessible at any time&lt;&#x2F;strong&gt;. Let&#x27;s dive in!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-does-balancing-work-in-apache-hbase&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-does-balancing-work-in-apache-hbase&quot; aria-label=&quot;Anchor link for: how-does-balancing-work-in-apache-hbase&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;How does balancing work in Apache HBase?&lt;&#x2F;h2&gt;
&lt;p&gt;Before diving into the balancer, let&#x27;s take a look at how it works. In Apache HBase, data is split into shards called &lt;code&gt;Regions&lt;&#x2F;code&gt;, and distributed through &lt;code&gt;RegionServers&lt;&#x2F;code&gt;. The number of regions will increase as the data is coming in, and regions will be split as a result. This is where the &lt;code&gt;Balancer&lt;&#x2F;code&gt; comes in. It will &lt;strong&gt;move regions&lt;&#x2F;strong&gt; to avoid hotspotting a single &lt;code&gt;RegionServer&lt;&#x2F;code&gt; and effectively distribute the load.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;hbase-custom-data-balancing&#x2F;hbase-ovh-2.jpeg&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The actual implementation, called &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;blob&#x2F;master&#x2F;hbase-server&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;master&#x2F;balancer&#x2F;StochasticLoadBalancer.java&quot;&gt;StochasticBalancer&lt;&#x2F;a&gt;, uses &lt;strong&gt;a cost-based approach:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;It first computes the &lt;strong&gt;overall cost&lt;&#x2F;strong&gt; of the cluster, by looping through &lt;code&gt;cost functions&lt;&#x2F;code&gt;. Every cost function &lt;strong&gt;returns a number between 0 and 1 inclusive&lt;&#x2F;strong&gt;, where 0 is the lowest cost-best solution, and 1 is the highest possible cost and worst solution. Apache Hbase is coming with several cost functions, which are measuring things like region load, table load, data locality, number of regions per RegionServers... The computed costs are &lt;strong&gt;scaled by their respective coefficients, defined in the configuration&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Now that the initial cost is computed, we can try to &lt;code&gt;Mutate&lt;&#x2F;code&gt; our cluster. For this, the Balancer creates a random &lt;code&gt;nextAction&lt;&#x2F;code&gt;, which could be something like &lt;strong&gt;swapping two regions&lt;&#x2F;strong&gt;, or &lt;strong&gt;moving one region to another RegionServer&lt;&#x2F;strong&gt;. The action is &lt;strong&gt;applied&lt;&#x2F;strong&gt; &lt;strong&gt;virtually&lt;&#x2F;strong&gt; , and then the &lt;strong&gt;new cost is calculated&lt;&#x2F;strong&gt;. If the new cost is lower than our previous one, the action is stored. If not, it is skipped. This operation is repeated &lt;code&gt;thousands of times&lt;&#x2F;code&gt;, hence the &lt;code&gt;Stochastic&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;At the end, &lt;strong&gt;the list of valid actions is applied to the actual cluster.&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;what-was-not-working-for-us&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-was-not-working-for-us&quot; aria-label=&quot;Anchor link for: what-was-not-working-for-us&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;What was not working for us?&lt;&#x2F;h2&gt;
&lt;p&gt;We found out that &lt;strong&gt;for our specific use case&lt;&#x2F;strong&gt;, which involved:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Single table&lt;&#x2F;li&gt;
&lt;li&gt;Dedicated Apache HBase and Apache Hadoop, &lt;strong&gt;tailored for our requirements&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Good key distribution&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;the number of regions per RegionServer was the real limit for us&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Even if the balancing strategy seems simple, &lt;strong&gt;we do think that being able to run an Apache HBase cluster on heterogeneous hardware is vital&lt;&#x2F;strong&gt;, especially in cloud environments, because you &lt;strong&gt;may not be able to buy the same server specs again in the future.&lt;&#x2F;strong&gt;
In our earlier example, our cluster grew from 80 to ~250 machines in
four years. Throughout that time, we bought new dedicated server
references, and even tested some special internal references.&lt;&#x2F;p&gt;
&lt;p&gt;We ended-up with differents groups of hardware: &lt;strong&gt;some servers can handle only 180 regions, whereas the biggest can handle more than 900&lt;&#x2F;strong&gt;. Because of this disparity, we had to disable the Load Balancer to avoid the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;blob&#x2F;master&#x2F;hbase-server&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;master&#x2F;balancer&#x2F;StochasticLoadBalancer.java#L1194&quot;&gt;RegionCountSkewCostFunction&lt;&#x2F;a&gt;, which would try to bring all RegionServers to the same number of regions.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;hbase-custom-data-balancing&#x2F;hbase-ovh-3.jpeg&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Two years ago we developed some internal tools, which are responsible
for load balancing regions across RegionServers. The tooling worked
really good for our use case, simplifying the day-to-day operation of
our cluster.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Open source is at the DNA of OVHcloud&lt;&#x2F;strong&gt;, and that means that we build our tools on open source software, but also that we &lt;strong&gt;contribute&lt;&#x2F;strong&gt;
and give it back to the community. When we talked around, we saw that
we weren&#x27;t the only one concerned by the heterogenous cluster problem.
We decided to rewrite our tooling to make it more general, and to &lt;strong&gt;contribute&lt;&#x2F;strong&gt; it &lt;strong&gt;directly upstream&lt;&#x2F;strong&gt; to the HBase project &lt;strong&gt;.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;our-contributions&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#our-contributions&quot; aria-label=&quot;Anchor link for: our-contributions&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Our contributions&lt;&#x2F;h2&gt;
&lt;p&gt;The first contribution was pretty simple, the cost function list was a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;blob&#x2F;8cb531f207b9f9f51ab1509655ae59701b66ac37&#x2F;hbase-server&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;master&#x2F;balancer&#x2F;StochasticLoadBalancer.java#L199-L213&quot;&gt;constant&lt;&#x2F;a&gt;. We &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;commit&#x2F;836f26976e1ad8b35d778c563067ed0614c026e9&quot;&gt;added the possibility to load custom cost functions&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The second contribution was about &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hbase&#x2F;commit&#x2F;42d535a57a75b58f585b48df9af9c966e6c7e46a&quot;&gt;adding an optional costFunction to balance regions according to a capacity rule&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-does-it-works&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-does-it-works&quot; aria-label=&quot;Anchor link for: how-does-it-works&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;How does it works?&lt;&#x2F;h2&gt;
&lt;p&gt;The balancer will load a file containing lines of rules. &lt;strong&gt;A rule is composed of a regexp for hostname, and a limit.&lt;&#x2F;strong&gt; For example, we could have:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;rs[0-9] 200
&lt;&#x2F;span&gt;&lt;span&gt;rs1[0-9] 50
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;RegionServers with &lt;strong&gt;hostnames matching the first rules will have a limit of 200&lt;&#x2F;strong&gt;, and &lt;strong&gt;the others 50&lt;&#x2F;strong&gt;. If there&#x27;s no match, a default is set.&lt;&#x2F;p&gt;
&lt;p&gt;Thanks to these rule, we have two key pieces of information:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;max number of regions for this cluster&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;the *&lt;em&gt;rules for each servers&lt;&#x2F;em&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The &lt;code&gt;HeterogeneousRegionCountCostFunction&lt;&#x2F;code&gt; will try to &lt;strong&gt;balance regions, according to their capacity.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s take an example... Imagine that we have 20 RS:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;10 RS, named &lt;code&gt;rs0&lt;&#x2F;code&gt; to &lt;code&gt;rs9&lt;&#x2F;code&gt;, loaded with 60 regions each, which can each handle 200 regions.&lt;&#x2F;li&gt;
&lt;li&gt;10 RS, named &lt;code&gt;rs10&lt;&#x2F;code&gt; to &lt;code&gt;rs19&lt;&#x2F;code&gt;, loaded with 60 regions each, which can each handle 50 regions.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;So, based on the following rules:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;rs[0-9] 200
&lt;&#x2F;span&gt;&lt;span&gt;rs1[0-9] 50
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;... we can see that the &lt;strong&gt;second group is overloaded&lt;&#x2F;strong&gt;, whereas the first group has plenty of space.&lt;&#x2F;p&gt;
&lt;p&gt;We know that we can handle a maximum of &lt;strong&gt;2,500 regions&lt;&#x2F;strong&gt; (200Ã—10 + 50Ã—10), and we have currently &lt;strong&gt;1,200 regions&lt;&#x2F;strong&gt; (60Ã—20). As such, the &lt;code&gt;HeterogeneousRegionCountCostFunction&lt;&#x2F;code&gt; will understand that the cluster is &lt;strong&gt;full at 48.0%&lt;&#x2F;strong&gt; (1200&#x2F;2500). Based on this information, we will then &lt;strong&gt;try to put all the RegionServers at ~48% of the load, according to the rules.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;hbase-custom-data-balancing&#x2F;hbase-ovh-4.jpeg&quot; alt=&quot;hbase image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;where-to-next&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#where-to-next&quot; aria-label=&quot;Anchor link for: where-to-next&quot;&gt;ðŸ”—&lt;&#x2F;a&gt;Where to next?&lt;&#x2F;h2&gt;
&lt;p&gt;Thanks to Apache HBase&#x27;s contributors, our patches are now &lt;strong&gt;merged&lt;&#x2F;strong&gt; into the master branch. As soon as Apache HBase maintainers publish a new release, we will deploy and use it at scale. This &lt;strong&gt;will allow more automation on our side, and ease operations for the Observability Team.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Contributing was an awesome journey. What I love most about open
source is the opportunity ability to contribute back, and build stronger
software. We &lt;strong&gt;had an opinion&lt;&#x2F;strong&gt; about how a particular issue should addressed, but &lt;strong&gt;the discussions with the community helped us to refine it&lt;&#x2F;strong&gt;. We spoke with e &lt;strong&gt;ngineers from other companies, who were struggling with Apache HBase&#x27;s cloud deployments, just as we were&lt;&#x2F;strong&gt;, and thanks to those exchanges, &lt;strong&gt;our contribution became more and more relevant.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
