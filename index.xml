<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pierre Zemb</title>
    <link>https://pierrezemb.fr/</link>
    <description>Recent content on Pierre Zemb</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&lt;a href=&#34;http://creativecommons.org/licenses/by/3.0/&#34;&gt;Some Rights Reserved&lt;/a&gt;</copyright>
    <lastBuildDate>Sun, 04 Aug 2019 15:07:11 +0200</lastBuildDate>
    
	<atom:link href="https://pierrezemb.fr/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What can be gleaned about GFS successor codenamed Colossus?</title>
      <link>https://pierrezemb.fr/posts/colossus-google/</link>
      <pubDate>Sun, 04 Aug 2019 15:07:11 +0200</pubDate>
      
      <guid>https://pierrezemb.fr/posts/colossus-google/</guid>
      <description>In the last few months, there has been numerous blogposts about the end of the Hadoop-era. It is true that:
 Health of Hadoop-based companies are publicly bad Hadoop has a bad publicity with headlines like &amp;lsquo;What does the death of Hadoop mean for big data?&amp;rsquo;  Hadoop, as a distributed-system, is hard to operate, but can be essential for some type of workload. As Hadoop is based on GFS, we can wonder how GFS evolved inside Google.</description>
    </item>
    
    <item>
      <title>Playing with TTL in HBase</title>
      <link>https://pierrezemb.fr/posts/ttl-hbase/</link>
      <pubDate>Mon, 27 May 2019 22:07:11 +0200</pubDate>
      
      <guid>https://pierrezemb.fr/posts/ttl-hbase/</guid>
      <description>Among all features provided by HBase, there is one that is pretty handy to deal with your data&amp;rsquo;s lifecyle: the fact that every cell version can have Time to Live or TTL. Let&amp;rsquo;s dive into the feature!
Time To Live (TTL) Let&amp;rsquo;s read the doc first!
 ColumnFamilies can set a TTL length in seconds, and HBase will automatically delete rows once the expiration time is reached.
 HBase Book: Time To Live (TTL)</description>
    </item>
    
    <item>
      <title>Monitoring OVH: 300k servers, 27 DCs and one metrics platform</title>
      <link>https://pierrezemb.fr/talks/one-monitoring-platform/</link>
      <pubDate>Sun, 19 May 2019 17:24:34 +0200</pubDate>
      
      <guid>https://pierrezemb.fr/talks/one-monitoring-platform/</guid>
      <description>Abstract What to do when you must monitor the whole infrastructure of the biggest European hosting and cloud provider? How to choose a tool when the most used ones fail to scale to your needs? How to build an Metrics platform to unify, conciliate and replace years of fragmented legacy partial solutions?
In this talk we will relate our experience building and maintaining OVH Metrics, the platform used to monitor all OVH infrastructure.</description>
    </item>
    
    <item>
      <title>Operate JVM at Scale</title>
      <link>https://pierrezemb.fr/talks/handling-jvm-scale/</link>
      <pubDate>Mon, 06 May 2019 22:18:13 +0200</pubDate>
      
      <guid>https://pierrezemb.fr/talks/handling-jvm-scale/</guid>
      <description>Abstract OVH has 27 data centers, more than 300 000 servers and 1.3 million customers. We operate one of the world’s largest cloud infrastructures on a daily basis, running millions of applications. Those softwares manage millions of transactions every second. To monitor such an architecture, OVH has chosen to build a unified monitoring platform: OVH Metrics Data Platform. Such a platform uses complex distributed systems to operate, based mainly on open source tools running on the JVM (Apache Kafka, Apache HBase, Warp 10, …).</description>
    </item>
    
    <item>
      <title>Handling alerts at OVH-Scale with Apache Flink</title>
      <link>https://pierrezemb.fr/talks/ovh-alerts-flink/</link>
      <pubDate>Thu, 18 Apr 2019 22:22:36 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/talks/ovh-alerts-flink/</guid>
      <description>Abstract OVH relies heavily on metrics to effectively monitor its entire infrastructure. Offering a low-level vision and business, these allow teams to better operate the daily operation of our services. After managing more than 300 TB of telemetry, we started working on an alerting solution over this huge datalake. For that, we decided to use Apache Flink to manage all these large scale alerts. Today, this project manages the alerting of flagship OVH products such as Public Cloud Instances and Kubernetes.</description>
    </item>
    
    <item>
      <title>What we learned as DevOps @OVH</title>
      <link>https://pierrezemb.fr/talks/devops-ovh/</link>
      <pubDate>Wed, 06 Mar 2019 22:22:36 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/talks/devops-ovh/</guid>
      <description>Abstract Have you been wondered what is like to be working at the largest European Cloud Provider as a DevOps ? Despite both working on differents teams and products, we have a lot in common, such as good practices, organizations, technologies and even our good mood ! Time to lift the curtain! We will share with you perspectives from two DevOps: one working on OVH managed Kubernetes product and one working on Metrics, OVH distributed monitoring platform.</description>
    </item>
    
    <item>
      <title>Handling OVH&#39;s alerts with Apache Flink</title>
      <link>https://pierrezemb.fr/posts/ovh-alerts-flink/</link>
      <pubDate>Sun, 03 Feb 2019 15:37:27 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/posts/ovh-alerts-flink/</guid>
      <description>This is a repost from OVH&amp;rsquo;s official blogpost.. Thanks Horacio Gonzalez for the awesome drawings!
Handling OVH&amp;rsquo;s alerts with Apache Flink OVH relies extensively on metrics to effectively monitor its entire stack. Whether they are low-level or business centric, they allow teams to gain insight into how our services are operating on a daily basis. The need to store millions of datapoints per second has produced the need to create a dedicated team to build a operate a product to handle that load: **Metrics Data Platform.</description>
    </item>
    
    <item>
      <title>What are ACID transactions?</title>
      <link>https://pierrezemb.fr/posts/acid-transactions/</link>
      <pubDate>Sun, 03 Feb 2019 15:37:27 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/posts/acid-transactions/</guid>
      <description>Transaction? &amp;quot;Programming should be about transforming data&amp;quot;  &amp;mdash; Programming Elixir 1.3 by Dave Thomas
As developers, we are interacting oftenly with data, whenever handling it from an API or a messaging consumer. To store it, we started to create softwares called relational database management system or RDBMS. Thanks to them, we, as developers, can develop applications pretty easily, without the need to implement our own storage solution. Interacting with mySQL or PostgreSQL have now become a commodity.</description>
    </item>
    
    <item>
      <title>Hbase Data Model</title>
      <link>https://pierrezemb.fr/posts/hbase-data-model/</link>
      <pubDate>Sun, 27 Jan 2019 20:24:27 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/posts/hbase-data-model/</guid>
      <description>HBase?  Apache HBase™ is a type of &amp;ldquo;NoSQL&amp;rdquo; database. &amp;ldquo;NoSQL&amp;rdquo; is a general term meaning that the database isn’t an RDBMS which supports SQL as its primary access language. Technically speaking, HBase is really more a &amp;ldquo;Data Store&amp;rdquo; than &amp;ldquo;Data Base&amp;rdquo; because it lacks many of the features you find in an RDBMS, such as typed columns, secondary indexes, triggers, and advanced query languages, etc.
 &amp;ndash; Hbase architecture overview</description>
    </item>
    
    <item>
      <title>Rediscover the known data universe with NASA dataset</title>
      <link>https://pierrezemb.fr/talks/nasa-datasets/</link>
      <pubDate>Sat, 15 Dec 2018 19:11:11 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/talks/nasa-datasets/</guid>
      <description>Abstract For many years humanity has been exploring the skies, dreaming of interstellar voyages and new planetary colonies. And you, do you want to go 3h with us to discover the universe?
It turns out that NASA has a great public dataset, especially one that is used to search for exoplanets, that is, planets outside our solar system.
During this Hands-on, we will guide you through the different stages of rediscovering exoplanets using Warp10, an open-source time series processing platform.</description>
    </item>
    
  </channel>
</rss>