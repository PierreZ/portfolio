<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pierre Zemb</title>
    <link>https://pierrezemb.fr/</link>
    <description>Recent content on Pierre Zemb</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&lt;a href=&#34;http://creativecommons.org/licenses/by/3.0/&#34;&gt;Some Rights Reserved&lt;/a&gt;</copyright>
    <lastBuildDate>Sun, 08 Dec 2019 15:00:00 +0100</lastBuildDate>
    
	<atom:link href="https://pierrezemb.fr/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Diving into Kafka&#39;s Protocol</title>
      <link>https://pierrezemb.fr/posts/diving-into-kafka-protocol/</link>
      <pubDate>Sun, 08 Dec 2019 15:00:00 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/posts/diving-into-kafka-protocol/</guid>
      <description>Diving Into is a blogpost serie where we are digging a specific part of of the project&#39;s basecode. In this episode, we will digg into Kafka&#39;s protocol.
 The protocol reference For the last few months, I worked a lot around Kafka&#39;s protocols, first by creating a fully async Kafka to Pulsar Proxy in Rust, and now by contributing directly to KoP (Kafka On Pulsar). The full Kafka Protocol documentation is available here, but it does not offer a global view of what is happening for a classic Producer and Consumer exchange.</description>
    </item>
    
    <item>
      <title>Diving into Hbase&#39;s MemStore</title>
      <link>https://pierrezemb.fr/posts/diving-into-hbase-memstore/</link>
      <pubDate>Sun, 17 Nov 2019 10:24:27 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/posts/diving-into-hbase-memstore/</guid>
      <description>Diving Into is a blogpost serie where we are digging a specific part of of the project&#39;s basecode. In this episode, we will digg into the implementation behind Hbase&#39;s MemStore.
 tl;dr: Hbase is using the ConcurrentSkipListMap.
What is the MemStore?  The memtable from the official BigTable paper is the equivalent of the MemStore in Hbase.
 As rows are sorted lexicographically in Hbase, when data comes in, you need to have some kind of a in-memory buffer to order those keys.</description>
    </item>
    
    <item>
      <title>Bookkeeper 101</title>
      <link>https://pierrezemb.fr/talks/bookkeeper-101/</link>
      <pubDate>Tue, 08 Oct 2019 12:22:36 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/talks/bookkeeper-101/</guid>
      <description>Abstract Apache Bookkeeper is the main datastore used by Apache Pulsar. This Top-Level Apache project is defined as « A scalable, fault-tolerant, and low-latency storage service optimized for real-time workloads ». BookKeeper is suitable for a wide variety of use cases, from WAL to storage. In this talk, we will dive into Bookkeeper’s concepts, the public API, deployments and how it is used within Pulsar.
Occurences  HUG France: Special Apache Pulsar Meetup  Ressources  slides  Photos and tweets Si vous cherchez à organiser les MeetUP, nos bureaux à Paris, Nantes, Rennes, Lyon, Bordeaux, Brest, Wroclaw, Madrid, Milan, London, Montreal, Dallas, Reston sont dispo gratos.</description>
    </item>
    
    <item>
      <title>What can be gleaned about GFS successor codenamed Colossus?</title>
      <link>https://pierrezemb.fr/posts/colossus-google/</link>
      <pubDate>Sun, 04 Aug 2019 15:07:11 +0200</pubDate>
      
      <guid>https://pierrezemb.fr/posts/colossus-google/</guid>
      <description>In the last few months, there has been numerous blogposts about the end of the Hadoop-era. It is true that:
 Health of Hadoop-based companies are publicly bad Hadoop has a bad publicity with headlines like &amp;lsquo;What does the death of Hadoop mean for big data?&#39;  Hadoop, as a distributed-system, is hard to operate, but can be essential for some type of workload. As Hadoop is based on GFS, we can wonder how GFS evolved inside Google.</description>
    </item>
    
    <item>
      <title>Playing with TTL in HBase</title>
      <link>https://pierrezemb.fr/posts/ttl-hbase/</link>
      <pubDate>Mon, 27 May 2019 22:07:11 +0200</pubDate>
      
      <guid>https://pierrezemb.fr/posts/ttl-hbase/</guid>
      <description>Among all features provided by HBase, there is one that is pretty handy to deal with your data&#39;s lifecyle: the fact that every cell version can have Time to Live or TTL. Let&#39;s dive into the feature!
Time To Live (TTL) Let&#39;s read the doc first!
 ColumnFamilies can set a TTL length in seconds, and HBase will automatically delete rows once the expiration time is reached.
 HBase Book: Time To Live (TTL)</description>
    </item>
    
    <item>
      <title>Monitoring OVH: 300k servers, 27 DCs and one metrics platform</title>
      <link>https://pierrezemb.fr/talks/one-monitoring-platform/</link>
      <pubDate>Sun, 19 May 2019 17:24:34 +0200</pubDate>
      
      <guid>https://pierrezemb.fr/talks/one-monitoring-platform/</guid>
      <description>Abstract What to do when you must monitor the whole infrastructure of the biggest European hosting and cloud provider? How to choose a tool when the most used ones fail to scale to your needs? How to build an Metrics platform to unify, conciliate and replace years of fragmented legacy partial solutions?
In this talk we will relate our experience building and maintaining OVH Metrics, the platform used to monitor all OVH infrastructure.</description>
    </item>
    
    <item>
      <title>Operate JVM at Scale</title>
      <link>https://pierrezemb.fr/talks/handling-jvm-scale/</link>
      <pubDate>Mon, 06 May 2019 22:18:13 +0200</pubDate>
      
      <guid>https://pierrezemb.fr/talks/handling-jvm-scale/</guid>
      <description>Abstract OVH has 27 data centers, more than 300 000 servers and 1.3 million customers. We operate one of the world’s largest cloud infrastructures on a daily basis, running millions of applications. Those softwares manage millions of transactions every second. To monitor such an architecture, OVH has chosen to build a unified monitoring platform: OVH Metrics Data Platform. Such a platform uses complex distributed systems to operate, based mainly on open source tools running on the JVM (Apache Kafka, Apache HBase, Warp 10, …).</description>
    </item>
    
    <item>
      <title>Handling alerts at OVH-Scale with Apache Flink</title>
      <link>https://pierrezemb.fr/talks/ovh-alerts-flink/</link>
      <pubDate>Thu, 18 Apr 2019 22:22:36 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/talks/ovh-alerts-flink/</guid>
      <description>Abstract OVH relies heavily on metrics to effectively monitor its entire infrastructure. Offering a low-level vision and business, these allow teams to better operate the daily operation of our services. After managing more than 300 TB of telemetry, we started working on an alerting solution over this huge datalake. For that, we decided to use Apache Flink to manage all these large scale alerts. Today, this project manages the alerting of flagship OVH products such as Public Cloud Instances and Kubernetes.</description>
    </item>
    
    <item>
      <title>What we learned as DevOps @OVH</title>
      <link>https://pierrezemb.fr/talks/devops-ovh/</link>
      <pubDate>Wed, 06 Mar 2019 22:22:36 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/talks/devops-ovh/</guid>
      <description>Abstract Have you been wondered what is like to be working at the largest European Cloud Provider as a DevOps ? Despite both working on differents teams and products, we have a lot in common, such as good practices, organizations, technologies and even our good mood ! Time to lift the curtain! We will share with you perspectives from two DevOps: one working on OVH managed Kubernetes product and one working on Metrics, OVH distributed monitoring platform.</description>
    </item>
    
    <item>
      <title>Handling OVH&#39;s alerts with Apache Flink</title>
      <link>https://pierrezemb.fr/posts/ovh-alerts-flink/</link>
      <pubDate>Sun, 03 Feb 2019 15:37:27 +0100</pubDate>
      
      <guid>https://pierrezemb.fr/posts/ovh-alerts-flink/</guid>
      <description>This is a repost from OVH&#39;s official blogpost.. Thanks Horacio Gonzalez for the awesome drawings!
Handling OVH&#39;s alerts with Apache Flink OVH relies extensively on metrics to effectively monitor its entire stack. Whether they are low-level or business centric, they allow teams to gain insight into how our services are operating on a daily basis. The need to store millions of datapoints per second has produced the need to create a dedicated team to build a operate a product to handle that load: **Metrics Data Platform.</description>
    </item>
    
  </channel>
</rss>